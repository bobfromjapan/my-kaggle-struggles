{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"sz = 384\nbs = 4\nnfolds = 5\nSEED = 2022\nN = 16 #number of tiles per image\nEPOCHS = 10\nN_tile = 64","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-18T07:04:57.656917Z","iopub.execute_input":"2022-09-18T07:04:57.657613Z","iopub.status.idle":"2022-09-18T07:04:57.668920Z","shell.execute_reply.started":"2022-09-18T07:04:57.657523Z","shell.execute_reply":"2022-09-18T07:04:57.667993Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:57.675972Z","iopub.execute_input":"2022-09-18T07:04:57.676871Z","iopub.status.idle":"2022-09-18T07:04:57.683190Z","shell.execute_reply.started":"2022-09-18T07:04:57.676813Z","shell.execute_reply":"2022-09-18T07:04:57.682119Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport zipfile\nimport torch\n\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nimport cv2\n# from efficientnet_pytorch import EfficientNet\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom torchvision import models\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom fastai.vision import *\nfrom fastai.layers import AdaptiveConcatPool2d, Flatten, Mish\nimport timm","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:57.684594Z","iopub.execute_input":"2022-09-18T07:04:57.684994Z","iopub.status.idle":"2022-09-18T07:04:59.723546Z","shell.execute_reply.started":"2022-09-18T07:04:57.684957Z","shell.execute_reply":"2022-09-18T07:04:59.722288Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nscaler = torch.cuda.amp.GradScaler()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:59.727869Z","iopub.execute_input":"2022-09-18T07:04:59.728663Z","iopub.status.idle":"2022-09-18T07:04:59.768643Z","shell.execute_reply.started":"2022-09-18T07:04:59.728631Z","shell.execute_reply":"2022-09-18T07:04:59.767687Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, arch='tf_efficientnetv2_s', n=2, pre=True):\n        super().__init__()\n#         m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', arch)\n        m = timm.create_model(arch, pretrained=pre)\n        self.enc = nn.Sequential(*list(m.children())[:-2])       \n        nc = list(m.children())[-1].in_features\n        self.head = nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*nc,512),\n                            Mish(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,n))\n        \n    def forward(self, x):\n        x = [x for x in x]\n        shape = x[0].shape\n        n = N\n        x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n        #x: bs*N x 3 x 128 x 128\n        x = self.enc(x)\n        #x: bs*N x C x 4 x 4\n        shape = x.shape\n#         print(x.shape)\n        #concatenate the output for tiles into a single map\n        x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n          .view(-1,shape[1],shape[2]*n,shape[3])\n        #x: bs x C x N*4 x 4\n        x = self.head(x)\n        #x: bs x n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:59.770684Z","iopub.execute_input":"2022-09-18T07:04:59.771633Z","iopub.status.idle":"2022-09-18T07:04:59.783734Z","shell.execute_reply.started":"2022-09-18T07:04:59.771595Z","shell.execute_reply":"2022-09-18T07:04:59.782774Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# #PyTorch\n# class DiceBCELoss(nn.Module):\n#     def __init__(self, weight=None, size_average=True):\n#         super(DiceBCELoss, self).__init__()\n        \n#     def onehot_onezero(y_true):\n#         res = []\n#         for i in y_true:\n#             if i == 0:\n#                 res.append(torch.tensor([1., 0.]))\n#             else:\n#                 res.append(torch.tensor([0., 1.]))\n\n#         return torch.stack(res)\n\n#     def forward(self, inputs, targets, smooth=1):\n        \n#         #comment out if your model contains a sigmoid or equivalent activation layer\n#         inputs = F.sigmoid(inputs)  \n        \n#         targets = DiceBCELoss.onehot_onezero(targets).to(device)\n#         #flatten label and prediction tensors\n#         inputs = inputs.view(-1)\n#         targets = targets.view(-1)\n        \n#         intersection = (inputs * targets).sum()                            \n#         dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n#         BCE = torch.nn.BCELoss()(inputs, targets)\n#         Dice_BCE = BCE + dice_loss\n        \n#         return Dice_BCE","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:59.785320Z","iopub.execute_input":"2022-09-18T07:04:59.785701Z","iopub.status.idle":"2022-09-18T07:04:59.794332Z","shell.execute_reply.started":"2022-09-18T07:04:59.785667Z","shell.execute_reply":"2022-09-18T07:04:59.793252Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class FocalLossWithOneHot(nn.Module):\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLossWithOneHot, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, input, target):\n        y = torch.nn.functional.one_hot(target.to(torch.int64), num_classes=2)\n\n        logit = torch.nn.functional.softmax(input, dim=-1)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n\n        return loss.sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:59.795875Z","iopub.execute_input":"2022-09-18T07:04:59.796258Z","iopub.status.idle":"2022-09-18T07:04:59.808429Z","shell.execute_reply.started":"2022-09-18T07:04:59.796225Z","shell.execute_reply":"2022-09-18T07:04:59.807585Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = Model()\nmodel = model.to(device)\n\n# クロスエントロピー損失関数使用\n# loss_fn = nn.BCEWithLogitsLoss().cuda()\nloss_fn = nn.CrossEntropyLoss().cuda()\n# loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n\n# loss_fn = FocalLossWithOneHot(gamma=2).cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n\n# optimizer = SGD(model, 0.1)\nscheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n# 前処理\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToPILImage(),\n    torchvision.transforms.Resize((sz, sz)),\n#     torchvision.transforms.RandAugment(),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.RandomVerticalFlip(),\n    torchvision.transforms.RandomRotation(45),\n#     torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\ntransform_val = torchvision.transforms.Compose([\n    torchvision.transforms.ToPILImage(),\n    torchvision.transforms.Resize((sz, sz)),\n#     torchvision.transforms.RandAugment(),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:04:59.810020Z","iopub.execute_input":"2022-09-18T07:04:59.810351Z","iopub.status.idle":"2022-09-18T07:05:01.989553Z","shell.execute_reply.started":"2022-09-18T07:04:59.810318Z","shell.execute_reply":"2022-09-18T07:05:01.988545Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# !pip install torchinfo","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:01.991274Z","iopub.execute_input":"2022-09-18T07:05:01.991662Z","iopub.status.idle":"2022-09-18T07:05:01.997023Z","shell.execute_reply.started":"2022-09-18T07:05:01.991624Z","shell.execute_reply":"2022-09-18T07:05:01.995915Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# from torchinfo import summary\n# summary(model=model, input_size=(bs, N, 3, sz, sz))","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:01.998920Z","iopub.execute_input":"2022-09-18T07:05:01.999328Z","iopub.status.idle":"2022-09-18T07:05:02.007724Z","shell.execute_reply.started":"2022-09-18T07:05:01.999290Z","shell.execute_reply":"2022-09-18T07:05:02.006600Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# This block is reffered from https://www.kaggle.com/code/yasufuminakama/mayo-train-images-size-1024-n-16-1/notebook\ntrain = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/train.csv')\n# train = train[train[\"image_id\"] != \"2c3c06_0\"]\n\ntrain['image_dir'] = ''\n\ntrain.loc[:100,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_1/'\ntrain.loc[100:200,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_2/'\ntrain.loc[200:300,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_3/'\ntrain.loc[300:400,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_4/'\ntrain.loc[400:500,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_5/'\ntrain.loc[500:600,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_6/'\ntrain.loc[600:700,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_7/'\ntrain.loc[700:,'image_dir'] = '/kaggle/input/mayotiled64384x384/train_images/train_images_8/'\n# train.loc[:100,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_1/'\n# train.loc[100:200,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_2/'\n# train.loc[200:300,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_3/'\n# train.loc[300:400,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_4/'\n# train.loc[400:500,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_5/'\n# train.loc[500:600,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_6/'\n# train.loc[600:700,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_7/'\n# train.loc[700:,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_8/'\n\ntarget_mapper = {\"CE\": 0, \"LAA\": 1}\n\ntrain[\"target\"] = train[\"label\"].map(lambda x: target_mapper[x])\n\nsplits = StratifiedKFold(n_splits=nfolds, random_state=SEED, shuffle=True)\nsplits = list(splits.split(train,train.center_id))\nfolds_splits = np.zeros(len(train)).astype(np.int)\nfor i in range(nfolds): folds_splits[splits[i][1]] = i\ntrain['split'] = folds_splits\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df, transform=None, aug=True):\n        self.cfg = cfg\n        self.image_ids = df['image_id'].values\n        self.image_dirs = df['image_dir'].values\n#         self.image_path = df[\"path\"].values\n        self.labels = df['target'].values\n        self.transform = transform\n        self.aug = aug\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_dir = self.image_dirs[idx]\n        images = []\n        img_indexes = random.sample(list(range(0, N_tile)), N)\n        for i in img_indexes:\n            path = image_dir + image_id + f'_{i}.jpg'\n            image = cv2.imread(path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = self.transform(image)\n            images.append(image)\n        images = torch.stack(images, dim=0)\n        \n#         if self.aug:\n#             images = torchvision.transforms.RandAugment()(images)\n            \n        label = torch.tensor(self.labels[idx]).long()\n        return images, label","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:02.010539Z","iopub.execute_input":"2022-09-18T07:05:02.010906Z","iopub.status.idle":"2022-09-18T07:05:02.039191Z","shell.execute_reply.started":"2022-09-18T07:05:02.010873Z","shell.execute_reply":"2022-09-18T07:05:02.038210Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_one_epoch(model, device, train_loader, val_loader, optimizer, scheduler, epoch, loss_fn):\n    running_loss = 0.\n    last_loss = 0.\n    val_loss = 0.\n    model.train(True)\n\n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, (data, target) in enumerate(train_loader):\n        # Every data instance is an input + label pair\n        inputs, labels = data.to(device), target.to(device)\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n#             loss = loss_fn(outputs, labels)\n\n#             loss = loss_fn(torch.squeeze(outputs), labels.float())\n            loss = loss_fn(outputs, labels)\n\n#         if len(outputs) != bs:\n#             print(outputs)\n\n        # Compute the loss and its gradients\n        \n        scaler.scale(loss).backward()\n        \n        scaler.step(optimizer)\n        scaler.update()\n\n        # Gather data and report\n        running_loss += loss.item()\n        if i % 10 == 9:\n            last_loss = running_loss / 10 # loss per batch\n            print('  batch {} loss: {}'.format(i + 1, last_loss))\n            tb_x = epoch * len(train_loader) + i + 1\n            running_loss = 0.\n#     scheduler.step()\n    \n#     model.requires_grad_(False)\n    model.eval()\n    with torch.no_grad():\n        for j, (data, target) in enumerate(val_loader):\n            inputs, labels = data.to(device), target.to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n#                 loss = loss_fn(torch.squeeze(outputs), labels.float())\n                loss = loss_fn(outputs, labels)\n\n            val_loss += loss.item()\n    \n    return val_loss/j","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:02.040789Z","iopub.execute_input":"2022-09-18T07:05:02.041408Z","iopub.status.idle":"2022-09-18T07:05:02.051716Z","shell.execute_reply.started":"2022-09-18T07:05:02.041372Z","shell.execute_reply":"2022-09-18T07:05:02.050868Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# for i, one_batch in enumerate(training_loader):\n#     x = one_batch[0]\n#     print(x.shape)\n#     y = one_batch[1]\n#     x = [x for x in x]\n#     shape = x[0].shape\n#     print(torch.stack(x,1).view(-1,shape[1],shape[2],shape[3]).shape)\n#     if i==5:\n#         break","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:02.057244Z","iopub.execute_input":"2022-09-18T07:05:02.057716Z","iopub.status.idle":"2022-09-18T07:05:02.064813Z","shell.execute_reply.started":"2022-09-18T07:05:02.057688Z","shell.execute_reply":"2022-09-18T07:05:02.063881Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# y","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:02.066321Z","iopub.execute_input":"2022-09-18T07:05:02.066945Z","iopub.status.idle":"2022-09-18T07:05:02.078453Z","shell.execute_reply.started":"2022-09-18T07:05:02.066906Z","shell.execute_reply":"2022-09-18T07:05:02.077361Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for fold in range(nfolds):\n    training_set = TrainDataset(None,train[train[\"split\"]!=fold], transform=transform, aug=True)\n    training_loader = torch.utils.data.DataLoader(training_set, shuffle=True, num_workers=2, batch_size=bs, drop_last=True)\n    val_set = TrainDataset(None,train[train[\"split\"]==fold], transform=transform_val, aug=False)\n    val_loader = torch.utils.data.DataLoader(val_set, shuffle=True, num_workers=2, batch_size=bs, drop_last=True)\n\n    epoch_number=0\n    avg_loss = [999,]\n    for epoch in range(EPOCHS):\n        print('EPOCH {}:'.format(epoch_number + 1))\n\n        epoch_loss = train_one_epoch(model=model, device=device, train_loader=training_loader, val_loader=val_loader, optimizer=optimizer, scheduler=scheduler, epoch=epoch_number, loss_fn=loss_fn)\n        epoch_number += 1\n        print(\"EPOCH \", str(epoch+1), \"val loss: \", epoch_loss)\n        \n        if min(avg_loss) > epoch_loss:\n            print(\"save model concat-tile-pooling-384-effv2-fold{}.pth\".format(str(fold)))\n            torch.save(model.state_dict(), \"concat-tile-pooling-384-effv2-fold{}.pth\".format(str(fold)))\n        \n        avg_loss.append(epoch_loss)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T07:05:02.079813Z","iopub.execute_input":"2022-09-18T07:05:02.080291Z","iopub.status.idle":"2022-09-18T09:50:42.723532Z","shell.execute_reply.started":"2022-09-18T07:05:02.080251Z","shell.execute_reply":"2022-09-18T09:50:42.722122Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"EPOCH 1:\n  batch 10 loss: 1.3066925823688507\n  batch 20 loss: 1.8727423429489136\n  batch 30 loss: 0.8587545201182365\n  batch 40 loss: 1.1802908092737199\n  batch 50 loss: 0.9048580080270767\n  batch 60 loss: 0.9885639548301697\n  batch 70 loss: 0.8081225037574769\n  batch 80 loss: 0.7727133899927139\n  batch 90 loss: 0.7118479162454605\n  batch 100 loss: 0.9034016370773316\n  batch 110 loss: 1.087413950264454\n  batch 120 loss: 0.7902820318937301\n  batch 130 loss: 0.7376235663890839\n  batch 140 loss: 0.6717137843370438\n  batch 150 loss: 0.7587411075830459\nEPOCH  1 val loss:  3.7815375816490917\nsave model concat-tile-pooling-384-effv2-fold0.pth\nEPOCH 2:\n  batch 10 loss: 0.5877505868673325\n  batch 20 loss: 0.8544884026050568\n  batch 30 loss: 1.1354012608528137\n  batch 40 loss: 0.879451933503151\n  batch 50 loss: 0.628269599378109\n  batch 60 loss: 0.8919453918933868\n  batch 70 loss: 0.7769123703241348\n  batch 80 loss: 1.1661761380732059\n  batch 90 loss: 0.9975090593099594\n  batch 100 loss: 0.47421872019767763\n  batch 110 loss: 0.5638541027903556\n  batch 120 loss: 0.5943273037672043\n  batch 130 loss: 0.4919838160276413\n  batch 140 loss: 0.9078116953372956\n  batch 150 loss: 0.7604789882898331\nEPOCH  2 val loss:  0.649579629715946\nsave model concat-tile-pooling-384-effv2-fold0.pth\nEPOCH 3:\n  batch 10 loss: 0.5571022376418113\n  batch 20 loss: 0.8190272182226181\n  batch 30 loss: 0.7355071991682053\n  batch 40 loss: 1.0167687863111496\n  batch 50 loss: 0.5728528186678886\n  batch 60 loss: 0.6271035924553872\n  batch 70 loss: 0.798626109957695\n  batch 80 loss: 0.4611453413963318\n  batch 90 loss: 0.746256260573864\n  batch 100 loss: 0.9383333295583725\n  batch 110 loss: 0.9243004053831101\n  batch 120 loss: 1.118674200773239\n  batch 130 loss: 0.8559355407953262\n  batch 140 loss: 0.7278521537780762\n  batch 150 loss: 0.6388553708791733\nEPOCH  3 val loss:  2.1484386811595564\nEPOCH 4:\n  batch 10 loss: 0.8343833446502685\n  batch 20 loss: 0.7826333165168762\n  batch 30 loss: 0.7890866488218308\n  batch 40 loss: 0.8698287963867187\n  batch 50 loss: 0.824943083524704\n  batch 60 loss: 0.7906041949987411\n  batch 70 loss: 0.5895465403795243\n  batch 80 loss: 0.7490647554397583\n  batch 90 loss: 0.7005917832255364\n  batch 100 loss: 0.5795875936746597\n  batch 110 loss: 0.7056331917643547\n  batch 120 loss: 0.6757527410984039\n  batch 130 loss: 0.8360889211297036\n  batch 140 loss: 0.6108489722013474\n  batch 150 loss: 0.7340474128723145\nEPOCH  4 val loss:  3.1973557811644344\nEPOCH 5:\n  batch 10 loss: 0.8395010858774186\n  batch 20 loss: 0.8098956674337388\n  batch 30 loss: 0.7529621839523315\n  batch 40 loss: 0.9248908996582031\n  batch 50 loss: 1.0926427468657494\n  batch 60 loss: 0.6424082234501839\n  batch 70 loss: 0.7834579408168793\n  batch 80 loss: 0.6877944529056549\n  batch 90 loss: 0.5851576447486877\n  batch 100 loss: 0.6917591869831086\n  batch 110 loss: 1.13930803835392\n  batch 120 loss: 1.1294088929891586\n  batch 130 loss: 0.7456250622868538\n  batch 140 loss: 0.6841646745800972\n  batch 150 loss: 0.667234230041504\nEPOCH  5 val loss:  0.7461056568556361\nEPOCH 6:\n  batch 10 loss: 0.5708862125873566\n  batch 20 loss: 0.7222358614206315\n  batch 30 loss: 0.9822601974010468\n  batch 40 loss: 0.8486167907714843\n  batch 50 loss: 0.7052135765552521\n  batch 60 loss: 0.911959308385849\n  batch 70 loss: 0.8336536437273026\n  batch 80 loss: 0.5948137938976288\n  batch 90 loss: 0.6862793207168579\n  batch 100 loss: 0.7366690665483475\n  batch 110 loss: 0.5262987732887268\n  batch 120 loss: 0.7385151535272598\n  batch 130 loss: 0.6217576086521148\n  batch 140 loss: 0.6568469285964966\n  batch 150 loss: 0.8120758146047592\nEPOCH  6 val loss:  6.568993947572178\nEPOCH 7:\n  batch 10 loss: 0.6509568572044373\n  batch 20 loss: 0.6902457118034363\n  batch 30 loss: 0.7628664433956146\n  batch 40 loss: 0.6099828898906707\n  batch 50 loss: 0.6316351562738418\n  batch 60 loss: 0.4675223916769028\n  batch 70 loss: 0.7151572406291962\n  batch 80 loss: 0.8480534255504608\n  batch 90 loss: 0.5900273203849793\n  batch 100 loss: 0.6433322548866272\n  batch 110 loss: 0.7296343386173249\n  batch 120 loss: 0.6604014307260513\n  batch 130 loss: 0.9252746075391769\n  batch 140 loss: 0.7377793893218041\n  batch 150 loss: 0.6595027148723602\nEPOCH  7 val loss:  0.7624729461967945\nEPOCH 8:\n  batch 10 loss: 0.49129463732242584\n  batch 20 loss: 0.9579855144023895\n  batch 30 loss: 0.9205956101417542\n  batch 40 loss: 0.9250917673110962\n  batch 50 loss: 0.8160962909460068\n  batch 60 loss: 0.7577752530574798\n  batch 70 loss: 0.7772458374500275\n  batch 80 loss: 0.626713091135025\n  batch 90 loss: 0.9147648066282272\n  batch 100 loss: 0.5865636050701142\n  batch 110 loss: 0.6224832981824875\n  batch 120 loss: 0.7274832516908646\n  batch 130 loss: 0.6892517417669296\n  batch 140 loss: 0.645624703168869\n  batch 150 loss: 0.6905632704496384\nEPOCH  8 val loss:  0.974778314638469\nEPOCH 9:\n  batch 10 loss: 0.7041524261236191\n  batch 20 loss: 0.5056594580411911\n  batch 30 loss: 0.710655590891838\n  batch 40 loss: 0.7785704225301743\n  batch 50 loss: 0.7383645117282868\n  batch 60 loss: 0.6565711587667465\n  batch 70 loss: 0.7185320213437081\n  batch 80 loss: 0.7515961855649949\n  batch 90 loss: 0.6614045947790146\n  batch 100 loss: 0.7095126330852508\n  batch 110 loss: 0.4463418275117874\n  batch 120 loss: 0.8662333190441132\n  batch 130 loss: 0.668131622672081\n  batch 140 loss: 0.7539704531431198\n  batch 150 loss: 0.6032467484474182\nEPOCH  9 val loss:  0.6774404413170285\nEPOCH 10:\n  batch 10 loss: 0.7037163466215134\n  batch 20 loss: 0.6500529766082763\n  batch 30 loss: 0.6462799906730652\n  batch 40 loss: 0.6746961116790772\n  batch 50 loss: 0.7246576547622681\n  batch 60 loss: 0.6470598459243775\n  batch 70 loss: 0.6576199352741241\n  batch 80 loss: 0.7504226058721543\n  batch 90 loss: 0.7464848011732101\n  batch 100 loss: 0.8649673491716385\n  batch 110 loss: 0.6718056485056877\n  batch 120 loss: 0.6946084350347519\n  batch 130 loss: 0.8986423373222351\n  batch 140 loss: 0.7320615887641907\n  batch 150 loss: 0.5975865423679352\nEPOCH  10 val loss:  0.6618040071593391\nEPOCH 1:\n  batch 10 loss: 0.686143571138382\n  batch 20 loss: 0.7024990007281303\n  batch 30 loss: 0.6554387450218201\n  batch 40 loss: 0.9296733945608139\n  batch 50 loss: 0.7770517870783806\n  batch 60 loss: 0.6684717059135437\n  batch 70 loss: 0.6154716610908508\n  batch 80 loss: 0.6863979250192642\n  batch 90 loss: 0.9198785424232483\n  batch 100 loss: 1.0006649672985077\n  batch 110 loss: 0.8072498202323913\n  batch 120 loss: 0.8173502326011658\n  batch 130 loss: 0.7188842058181762\n  batch 140 loss: 0.75554940700531\n  batch 150 loss: 0.7416782289743423\nEPOCH  1 val loss:  0.8263205320884784\nsave model concat-tile-pooling-384-effv2-fold1.pth\nEPOCH 2:\n  batch 10 loss: 1.0633553862571716\n  batch 20 loss: 0.5827633440494537\n  batch 30 loss: 0.7479681730270386\n  batch 40 loss: 0.6060211718082428\n  batch 50 loss: 0.8203572183847427\n  batch 60 loss: 0.9036603391170501\n  batch 70 loss: 0.7139050245285035\n  batch 80 loss: 0.8112673863768578\n  batch 90 loss: 0.9498857140541077\n  batch 100 loss: 0.586460816860199\n  batch 110 loss: 0.6077997356653213\n  batch 120 loss: 0.6581423282623291\n  batch 130 loss: 0.7703237920999527\n  batch 140 loss: 0.6050189077854157\n  batch 150 loss: 0.6175141870975495\nEPOCH  2 val loss:  0.8222277201712132\nsave model concat-tile-pooling-384-effv2-fold1.pth\nEPOCH 3:\n  batch 10 loss: 0.9304358214139938\n  batch 20 loss: 0.7722251698374748\n  batch 30 loss: 0.9815505087375641\n  batch 40 loss: 0.6892977625131607\n  batch 50 loss: 0.8103411853313446\n  batch 60 loss: 1.0114707589149474\n  batch 70 loss: 0.7382628619670868\n  batch 80 loss: 0.7187990427017212\n  batch 90 loss: 0.7880990684032441\n  batch 100 loss: 0.6365070313215255\n  batch 110 loss: 0.6786739975214005\n  batch 120 loss: 0.6865529850125313\n  batch 130 loss: 0.54008269906044\n  batch 140 loss: 0.7552886694669724\n  batch 150 loss: 0.6226290971040725\nEPOCH  3 val loss:  0.8381572100851271\nEPOCH 4:\n  batch 10 loss: 0.6553658872842789\n  batch 20 loss: 0.8395632356405258\n  batch 30 loss: 0.7748935699462891\n  batch 40 loss: 0.6294827848672867\n  batch 50 loss: 0.6698521837592125\n  batch 60 loss: 0.598287396132946\n  batch 70 loss: 0.5761734783649445\n  batch 80 loss: 0.7693152248859405\n  batch 90 loss: 0.8552927076816559\n  batch 100 loss: 0.4827264815568924\n  batch 110 loss: 0.7396162718534469\n  batch 120 loss: 0.8022633552551269\n  batch 130 loss: 0.7949005991220475\n  batch 140 loss: 0.7743554934859276\n  batch 150 loss: 0.7679534375667572\nEPOCH  4 val loss:  0.761473134987884\nsave model concat-tile-pooling-384-effv2-fold1.pth\nEPOCH 5:\n  batch 10 loss: 0.552000093460083\n  batch 20 loss: 0.6692287892103195\n  batch 30 loss: 0.6204822659492493\n  batch 40 loss: 0.7969254434108735\n  batch 50 loss: 0.6871577560901642\n  batch 60 loss: 0.842857551574707\n  batch 70 loss: 0.6448241949081421\n  batch 80 loss: 0.9571339190006256\n  batch 90 loss: 0.7867632627487182\n  batch 100 loss: 0.8662846893072128\n  batch 110 loss: 0.7635967627167701\n  batch 120 loss: 0.7569702088832855\n  batch 130 loss: 0.5794638156890869\n  batch 140 loss: 0.8040994286537171\n  batch 150 loss: 0.5308205723762512\nEPOCH  5 val loss:  1.044387205917802\nEPOCH 6:\n  batch 10 loss: 0.8867094367742538\n  batch 20 loss: 0.7608069211244584\n  batch 30 loss: 0.6443341255187989\n  batch 40 loss: 0.7499828189611435\n  batch 50 loss: 0.8042371898889542\n  batch 60 loss: 0.7409036755561829\n  batch 70 loss: 0.7532677263021469\n  batch 80 loss: 0.5617006957530976\n  batch 90 loss: 0.713755190372467\n  batch 100 loss: 0.5696687161922455\n  batch 110 loss: 0.7764072328805923\n  batch 120 loss: 0.8908886730670929\n  batch 130 loss: 0.7830059379339218\n  batch 140 loss: 0.5944281697273255\n  batch 150 loss: 0.6394920527935029\nEPOCH  6 val loss:  0.6205521639850404\nsave model concat-tile-pooling-384-effv2-fold1.pth\nEPOCH 7:\n  batch 10 loss: 0.5621720910072326\n  batch 20 loss: 0.7509784519672393\n  batch 30 loss: 0.5839499980211258\n  batch 40 loss: 0.6668820083141327\n  batch 50 loss: 0.5389072567224502\n  batch 60 loss: 0.6402505993843078\n  batch 70 loss: 0.6585660725831985\n  batch 80 loss: 0.6262982219457627\n  batch 90 loss: 0.9930772334337234\n  batch 100 loss: 0.8808383256196975\n  batch 110 loss: 0.8052864789962768\n  batch 120 loss: 0.756700524687767\n  batch 130 loss: 0.8571247220039367\n  batch 140 loss: 0.7638000011444092\n  batch 150 loss: 0.6634548962116241\nEPOCH  7 val loss:  0.6174856854809655\nsave model concat-tile-pooling-384-effv2-fold1.pth\nEPOCH 8:\n  batch 10 loss: 0.6744704663753509\n  batch 20 loss: 0.7444742143154144\n  batch 30 loss: 0.9694119602441787\n  batch 40 loss: 0.7963908076286316\n  batch 50 loss: 0.6749409735202789\n  batch 60 loss: 0.6545493364334106\n  batch 70 loss: 0.7969405889511109\n  batch 80 loss: 0.7845400601625443\n  batch 90 loss: 0.7415612310171127\n  batch 100 loss: 0.6993826448917388\n  batch 110 loss: 0.7793610602617264\n  batch 120 loss: 0.505518102645874\n  batch 130 loss: 0.7283563524484634\n  batch 140 loss: 0.7568307459354401\n  batch 150 loss: 0.7981957137584687\nEPOCH  8 val loss:  0.6265551464425193\nEPOCH 9:\n  batch 10 loss: 0.8550223022699356\n  batch 20 loss: 0.6986349582672119\n  batch 30 loss: 0.8476840168237686\n  batch 40 loss: 0.6795424193143844\n  batch 50 loss: 0.5911699146032333\n  batch 60 loss: 0.9378557980060578\n  batch 70 loss: 0.5282624006271363\n  batch 80 loss: 0.6830586612224578\n  batch 90 loss: 0.7158513903617859\n  batch 100 loss: 0.9027490317821503\n  batch 110 loss: 0.6583587974309921\n  batch 120 loss: 0.5393918499350547\n  batch 130 loss: 0.620610237121582\n  batch 140 loss: 0.6478079468011856\n  batch 150 loss: 0.6190094769001007\nEPOCH  9 val loss:  0.659598088512818\nEPOCH 10:\n  batch 10 loss: 0.7576624900102615\n  batch 20 loss: 0.7096345454454422\n  batch 30 loss: 0.6014850735664368\n  batch 40 loss: 0.7139168441295624\n  batch 50 loss: 0.7437368512153626\n  batch 60 loss: 0.6458549827337265\n  batch 70 loss: 0.8067694246768952\n  batch 80 loss: 0.944982898235321\n  batch 90 loss: 0.515623939037323\n  batch 100 loss: 0.7183172509074212\n  batch 110 loss: 0.7123339504003525\n  batch 120 loss: 0.8147643879055977\n  batch 130 loss: 0.6360913664102554\n  batch 140 loss: 0.6780257403850556\n  batch 150 loss: 0.6623043864965439\nEPOCH  10 val loss:  0.6868625096976757\nEPOCH 1:\n  batch 10 loss: 0.6659413374960422\n  batch 20 loss: 0.73131422996521\n  batch 30 loss: 0.6264211311936378\n  batch 40 loss: 0.7955146074295044\n  batch 50 loss: 0.6494223058223725\n  batch 60 loss: 0.737630732357502\n  batch 70 loss: 0.6686020523309708\n  batch 80 loss: 0.5242655336856842\n  batch 90 loss: 0.6561805903911591\n  batch 100 loss: 0.7343621969223022\n  batch 110 loss: 0.8938550099730491\n  batch 120 loss: 0.7729214936494827\n  batch 130 loss: 0.8085805833339691\n  batch 140 loss: 0.6623429834842682\n  batch 150 loss: 0.7596982479095459\nEPOCH  1 val loss:  17.72084371248881\nsave model concat-tile-pooling-384-effv2-fold2.pth\nEPOCH 2:\n  batch 10 loss: 0.6261602073907853\n  batch 20 loss: 0.8308483004570008\n  batch 30 loss: 0.6090333193540574\n  batch 40 loss: 0.6639873951673507\n  batch 50 loss: 0.7940438210964202\n  batch 60 loss: 0.6921632528305054\n  batch 70 loss: 1.0356516301631928\n  batch 80 loss: 0.5210411339998245\n  batch 90 loss: 0.8191281467676162\n  batch 100 loss: 0.6697151303291321\n  batch 110 loss: 0.7404697254300118\n  batch 120 loss: 0.8175506323575974\n  batch 130 loss: 0.7724146455526352\n  batch 140 loss: 1.0494226664304733\n  batch 150 loss: 0.7362283200025559\nEPOCH  2 val loss:  0.6162614838944541\nsave model concat-tile-pooling-384-effv2-fold2.pth\nEPOCH 3:\n  batch 10 loss: 0.5819712609052659\n  batch 20 loss: 0.679413092136383\n  batch 30 loss: 0.562804901599884\n  batch 40 loss: 0.7619894236326218\n  batch 50 loss: 0.7544820129871368\n  batch 60 loss: 0.7566804528236389\n  batch 70 loss: 0.5828459411859512\n  batch 80 loss: 0.5696290522813797\n  batch 90 loss: 0.7618456423282624\n  batch 100 loss: 0.7372355356812477\n  batch 110 loss: 0.6487738281488419\n  batch 120 loss: 0.621339425444603\n  batch 130 loss: 0.7535141110420227\n  batch 140 loss: 0.6471953749656677\n  batch 150 loss: 0.9058575868606568\nEPOCH  3 val loss:  0.9796023844844766\nEPOCH 4:\n  batch 10 loss: 0.7601857006549835\n  batch 20 loss: 0.6536377131938934\n  batch 30 loss: 0.6495103687047958\n  batch 40 loss: 0.7945251941680909\n  batch 50 loss: 0.6942849516868591\n  batch 60 loss: 0.6619138300418854\n  batch 70 loss: 0.7178552269935607\n  batch 80 loss: 0.5917257964611053\n  batch 90 loss: 0.6978689104318618\n  batch 100 loss: 0.7006823807954788\n  batch 110 loss: 0.6947844594717025\n  batch 120 loss: 0.5864888295531273\n  batch 130 loss: 0.9169228553771973\n  batch 140 loss: 0.6991556078195572\n  batch 150 loss: 0.7760600179433823\nEPOCH  4 val loss:  0.8048286856048636\nEPOCH 5:\n  batch 10 loss: 0.5950092822313309\n  batch 20 loss: 0.8856397479772568\n  batch 30 loss: 0.5908290326595307\n  batch 40 loss: 0.7988881587982177\n  batch 50 loss: 0.6780868411064148\n  batch 60 loss: 0.6281631037592887\n  batch 70 loss: 0.8944205999374389\n  batch 80 loss: 0.6372128576040268\n  batch 90 loss: 0.6052716493606567\n  batch 100 loss: 0.5143479108810425\n  batch 110 loss: 0.5869055792689324\n  batch 120 loss: 0.7772427305579186\n  batch 130 loss: 0.9263731658458709\n  batch 140 loss: 0.6128632307052613\n  batch 150 loss: 0.7329023361206055\nEPOCH  5 val loss:  0.6739343098468251\nEPOCH 6:\n  batch 10 loss: 0.6916990548372268\n  batch 20 loss: 0.6955688148736954\n  batch 30 loss: 0.638505819439888\n  batch 40 loss: 0.7120433807373047\n  batch 50 loss: 0.746338552236557\n  batch 60 loss: 0.5458368420600891\n  batch 70 loss: 0.6584208935499192\n  batch 80 loss: 0.5280353486537933\n  batch 90 loss: 0.7777001783251762\n  batch 100 loss: 0.8049001604318619\n  batch 110 loss: 0.657055988907814\n  batch 120 loss: 0.7317752242088318\n  batch 130 loss: 0.6387471914291382\n  batch 140 loss: 0.5605533748865128\n  batch 150 loss: 0.8173042297363281\nEPOCH  6 val loss:  0.8500992390844557\nEPOCH 7:\n  batch 10 loss: 0.6236460626125335\n  batch 20 loss: 0.7169271528720855\n  batch 30 loss: 0.8024096071720124\n  batch 40 loss: 0.6471492230892182\n  batch 50 loss: 0.7391627281904221\n  batch 60 loss: 0.6533243656158447\n  batch 70 loss: 0.8131332665681839\n  batch 80 loss: 0.6081457018852234\n  batch 90 loss: 0.5110990941524506\n  batch 100 loss: 0.5577352315187454\n  batch 110 loss: 0.6767786532640457\n  batch 120 loss: 0.6483229279518128\n  batch 130 loss: 0.6942535042762756\n  batch 140 loss: 0.7175654619932175\n  batch 150 loss: 0.6617228865623475\nEPOCH  7 val loss:  0.6562382115258111\nEPOCH 8:\n  batch 10 loss: 0.6749885082244873\n  batch 20 loss: 0.5800357103347779\n  batch 30 loss: 0.8352508008480072\n  batch 40 loss: 0.801428210735321\n  batch 50 loss: 0.7021101176738739\n  batch 60 loss: 0.7408425390720368\n  batch 70 loss: 0.7071706473827362\n  batch 80 loss: 0.6545809090137482\n  batch 90 loss: 0.799583426117897\n  batch 100 loss: 0.6208691239356995\n  batch 110 loss: 0.8611889570951462\n  batch 120 loss: 0.6985033839941025\n  batch 130 loss: 0.6899105906486511\n  batch 140 loss: 0.7047662258148193\n  batch 150 loss: 0.5885827809572219\nEPOCH  8 val loss:  0.7786967953046163\nEPOCH 9:\n  batch 10 loss: 0.9530464261770248\n  batch 20 loss: 0.824978718161583\n  batch 30 loss: 0.7386329978704452\n  batch 40 loss: 0.6370668888092041\n  batch 50 loss: 0.6409810468554497\n  batch 60 loss: 0.655044287443161\n  batch 70 loss: 0.4770259842276573\n  batch 80 loss: 0.4999164044857025\n  batch 90 loss: 0.7121921092271805\n  batch 100 loss: 1.0058595299720765\n  batch 110 loss: 1.1555287718772889\n  batch 120 loss: 0.6236608564853668\n  batch 130 loss: 0.9454914957284928\n  batch 140 loss: 0.6440433889627457\n  batch 150 loss: 0.6627587825059891\nEPOCH  9 val loss:  0.7403069639371501\nEPOCH 10:\n  batch 10 loss: 0.7969909280538559\n  batch 20 loss: 0.7748272091150283\n  batch 30 loss: 0.8829649895429611\n  batch 40 loss: 0.6820052653551102\n  batch 50 loss: 0.5871953517198563\n  batch 60 loss: 0.7879475563764572\n  batch 70 loss: 0.7090074181556701\n  batch 80 loss: 0.5418204337358474\n  batch 90 loss: 0.767698860168457\n  batch 100 loss: 0.5513522505760193\n  batch 110 loss: 0.5789962410926819\n  batch 120 loss: 0.8212676584720612\n  batch 130 loss: 0.5088702782988548\n  batch 140 loss: 0.966576772928238\n  batch 150 loss: 0.7622269228100776\nEPOCH  10 val loss:  0.6680343548456827\nEPOCH 1:\n  batch 10 loss: 0.9255710452795028\n  batch 20 loss: 0.7927352175116539\n  batch 30 loss: 1.0320683479309083\n  batch 40 loss: 0.7002314895391464\n  batch 50 loss: 0.6606248825788498\n  batch 60 loss: 0.6469813197851181\n  batch 70 loss: 0.6059067085385322\n  batch 80 loss: 0.8415212482213974\n  batch 90 loss: 0.7199586272239685\n  batch 100 loss: 0.8963717639446258\n  batch 110 loss: 0.7898512810468674\n  batch 120 loss: 0.5843619197607041\n  batch 130 loss: 0.9122708231210709\n  batch 140 loss: 0.6634324580430985\n  batch 150 loss: 0.8340278118848801\nEPOCH  1 val loss:  0.563561375770304\nsave model concat-tile-pooling-384-effv2-fold3.pth\nEPOCH 2:\n  batch 10 loss: 0.45815565884113313\n  batch 20 loss: 1.043325385451317\n  batch 30 loss: 0.7605568319559097\n  batch 40 loss: 0.8872323781251907\n  batch 50 loss: 0.7813958197832107\n  batch 60 loss: 0.8629086047410965\n  batch 70 loss: 0.6997072279453278\n  batch 80 loss: 0.7677791804075241\n  batch 90 loss: 0.7232326567173004\n  batch 100 loss: 0.5069267868995666\n  batch 110 loss: 0.8220123440027237\n  batch 120 loss: 0.7880161523818969\n  batch 130 loss: 0.7543804407119751\n  batch 140 loss: 0.6217669695615768\n  batch 150 loss: 0.5956860601902008\nEPOCH  2 val loss:  0.6814431188007196\nEPOCH 3:\n  batch 10 loss: 1.0736672103404998\n  batch 20 loss: 0.7391830950975418\n  batch 30 loss: 0.6950601756572723\n  batch 40 loss: 0.7754738837480545\n  batch 50 loss: 0.769516184926033\n  batch 60 loss: 0.657590651512146\n  batch 70 loss: 0.6121491253376007\n  batch 80 loss: 0.669683426618576\n  batch 90 loss: 0.7159555196762085\n  batch 100 loss: 0.7140751361846924\n  batch 110 loss: 0.7973940715193748\n  batch 120 loss: 0.6282373368740082\n  batch 130 loss: 0.6494720339775085\n  batch 140 loss: 0.5650256663560868\n  batch 150 loss: 0.6884720385074615\nEPOCH  3 val loss:  0.5545127027564578\nsave model concat-tile-pooling-384-effv2-fold3.pth\nEPOCH 4:\n  batch 10 loss: 0.6676031798124313\n  batch 20 loss: 0.6323182255029678\n  batch 30 loss: 0.7681610822677613\n  batch 40 loss: 0.6998470544815063\n  batch 50 loss: 0.7003130227327347\n  batch 60 loss: 0.5540659725666046\n  batch 70 loss: 0.5291364908218383\n  batch 80 loss: 0.7875710129737854\n  batch 90 loss: 0.7363894611597062\n  batch 100 loss: 1.026977077126503\n  batch 110 loss: 0.9077585875988007\n  batch 120 loss: 0.6379145324230194\n  batch 130 loss: 0.8074126660823822\n  batch 140 loss: 0.6827750325202941\n  batch 150 loss: 0.6165072083473205\nEPOCH  4 val loss:  0.6758086633765035\nEPOCH 5:\n  batch 10 loss: 0.6256803393363952\n  batch 20 loss: 0.7238971889019012\n  batch 30 loss: 0.7545076370239258\n  batch 40 loss: 0.8028747886419296\n  batch 50 loss: 0.6078091472387314\n  batch 60 loss: 0.676113224029541\n  batch 70 loss: 0.827466681599617\n  batch 80 loss: 0.9716129899024963\n  batch 90 loss: 0.8014264613389969\n  batch 100 loss: 0.6288082748651505\n  batch 110 loss: 0.8619989335536957\n  batch 120 loss: 0.6442682445049286\n  batch 130 loss: 0.7686485290527344\n  batch 140 loss: 0.73706713616848\n  batch 150 loss: 0.6924692839384079\nEPOCH  5 val loss:  0.5597405855854353\nEPOCH 6:\n  batch 10 loss: 0.7974353045225143\n  batch 20 loss: 0.7042471975088119\n  batch 30 loss: 0.649059334397316\n  batch 40 loss: 0.6903018981218338\n  batch 50 loss: 0.6584999471902847\n  batch 60 loss: 0.7235890090465545\n  batch 70 loss: 0.6895844131708145\n  batch 80 loss: 0.5977744221687317\n  batch 90 loss: 0.6585787743330002\n  batch 100 loss: 0.6910262495279312\n  batch 110 loss: 0.5174312800168991\n  batch 120 loss: 0.6515140861272812\n  batch 130 loss: 0.7037164092063903\n  batch 140 loss: 0.9884787738323212\n  batch 150 loss: 0.6566943615674973\nEPOCH  6 val loss:  0.5803997595277097\nEPOCH 7:\n  batch 10 loss: 0.8121766984462738\n  batch 20 loss: 0.8780531227588654\n  batch 30 loss: 0.6798476964235306\n  batch 40 loss: 0.7873955965042114\n  batch 50 loss: 0.62524194419384\n  batch 60 loss: 0.6512772023677826\n  batch 70 loss: 0.492404206097126\n  batch 80 loss: 0.6739421427249909\n  batch 90 loss: 0.7833551436662674\n  batch 100 loss: 0.6764329493045806\n  batch 110 loss: 0.6760090559720993\n  batch 120 loss: 0.7287981539964676\n  batch 130 loss: 0.5853753089904785\n  batch 140 loss: 0.7000898152589798\n  batch 150 loss: 0.8257735520601273\nEPOCH  7 val loss:  0.608020983196588\nEPOCH 8:\n  batch 10 loss: 1.041166466474533\n  batch 20 loss: 0.7705763876438141\n  batch 30 loss: 0.7050760686397552\n  batch 40 loss: 0.6839487999677658\n  batch 50 loss: 0.8011901170015335\n  batch 60 loss: 0.6848270833492279\n  batch 70 loss: 0.7969098448753357\n  batch 80 loss: 0.6270235687494278\n  batch 90 loss: 0.7132527709007264\n  batch 100 loss: 0.7231645882129669\n  batch 110 loss: 1.0299331486225127\n  batch 120 loss: 0.6980263084173203\n  batch 130 loss: 0.6798085600137711\n  batch 140 loss: 0.6310001850128174\n  batch 150 loss: 0.7149897992610932\nEPOCH  8 val loss:  0.5860961082701882\nEPOCH 9:\n  batch 10 loss: 0.9342061340808868\n  batch 20 loss: 0.7241296380758285\n  batch 30 loss: 0.6979448050260544\n  batch 40 loss: 0.6635891318321228\n  batch 50 loss: 0.7401251569390297\n  batch 60 loss: 0.8002538830041885\n  batch 70 loss: 0.78533234000206\n  batch 80 loss: 0.8493855595588684\n  batch 90 loss: 0.6810222923755646\n  batch 100 loss: 0.5928848028182984\n  batch 110 loss: 0.5828700006008148\n  batch 120 loss: 0.6319014638662338\n  batch 130 loss: 0.6728403508663178\n  batch 140 loss: 0.8139804065227508\n  batch 150 loss: 0.5922665774822236\nEPOCH  9 val loss:  1.2791552548308875\nEPOCH 10:\n  batch 10 loss: 0.686438313126564\n  batch 20 loss: 0.801764652132988\n  batch 30 loss: 0.860846047103405\n  batch 40 loss: 0.8392992913722992\n  batch 50 loss: 0.5346535265445709\n  batch 60 loss: 0.812969633936882\n  batch 70 loss: 0.8458769798278809\n  batch 80 loss: 0.6361249566078186\n  batch 90 loss: 0.6218688011169433\n  batch 100 loss: 0.6338996231555939\n  batch 110 loss: 0.7007209599018097\n  batch 120 loss: 0.825796140730381\n  batch 130 loss: 0.673661956191063\n  batch 140 loss: 0.7060933977365493\n  batch 150 loss: 0.6721660226583481\nEPOCH  10 val loss:  0.871757249865267\nEPOCH 1:\n  batch 10 loss: 0.8558400720357895\n  batch 20 loss: 0.5698531746864319\n  batch 30 loss: 0.7877098321914673\n  batch 40 loss: 0.619164326786995\n  batch 50 loss: 0.6081663101911545\n  batch 60 loss: 0.586561007797718\n  batch 70 loss: 0.602107758820057\n  batch 80 loss: 0.7465002045035363\n  batch 90 loss: 0.9031222462654114\n  batch 100 loss: 0.6311996191740036\n  batch 110 loss: 0.7332111656665802\n  batch 120 loss: 0.8187797635793685\n  batch 130 loss: 0.8377183705568314\n  batch 140 loss: 0.7384698003530502\n  batch 150 loss: 0.7087710797786713\nEPOCH  1 val loss:  1.8754392084148195\nsave model concat-tile-pooling-384-effv2-fold4.pth\nEPOCH 2:\n  batch 10 loss: 0.6502691030502319\n  batch 20 loss: 0.7965086251497269\n  batch 30 loss: 0.9213071674108505\n  batch 40 loss: 0.6888089656829834\n  batch 50 loss: 0.6535237714648247\n  batch 60 loss: 0.9692241817712783\n  batch 70 loss: 1.0201561778783799\n  batch 80 loss: 0.725390151143074\n  batch 90 loss: 0.7270918250083923\n  batch 100 loss: 1.021941876411438\n  batch 110 loss: 0.733399075269699\n  batch 120 loss: 0.7476463079452514\n  batch 130 loss: 0.6169711410999298\n  batch 140 loss: 0.8115537464618683\n  batch 150 loss: 0.7889172047376632\nEPOCH  2 val loss:  0.7039893410272069\nsave model concat-tile-pooling-384-effv2-fold4.pth\nEPOCH 3:\n  batch 10 loss: 0.7558011174201965\n  batch 20 loss: 0.6557415783405304\n  batch 30 loss: 0.5829327523708343\n  batch 40 loss: 0.8979571014642715\n  batch 50 loss: 0.6221306562423706\n  batch 60 loss: 0.6463375449180603\n  batch 70 loss: 0.7395451188087463\n  batch 80 loss: 0.6558496445417404\n  batch 90 loss: 0.6916313543915749\n  batch 100 loss: 0.7213007360696793\n  batch 110 loss: 0.7156357139348983\n  batch 120 loss: 0.6933015421032905\n  batch 130 loss: 0.7602933406829834\n  batch 140 loss: 0.541277039051056\n  batch 150 loss: 0.5902114391326905\nEPOCH  3 val loss:  6.513607743920551\nEPOCH 4:\n  batch 10 loss: 0.65945705473423\n  batch 20 loss: 0.8174331516027451\n  batch 30 loss: 0.5887694031000137\n  batch 40 loss: 0.6070417374372482\n  batch 50 loss: 0.6300537794828415\n  batch 60 loss: 0.6478604912757874\n  batch 70 loss: 0.8288367390632629\n  batch 80 loss: 0.8471863612532615\n  batch 90 loss: 0.6188979744911194\n  batch 100 loss: 0.5611442893743515\n  batch 110 loss: 0.5154323175549507\n  batch 120 loss: 0.6085633307695388\n  batch 130 loss: 0.455584192276001\n  batch 140 loss: 0.5523881763219833\n  batch 150 loss: 0.7521347999572754\nEPOCH  4 val loss:  1.0266233428070943\nEPOCH 5:\n  batch 10 loss: 0.7524990797042846\n  batch 20 loss: 0.6546142876148224\n  batch 30 loss: 0.7231990993022919\n  batch 40 loss: 0.5364775121212005\n  batch 50 loss: 0.705011123418808\n  batch 60 loss: 0.5369152396917343\n  batch 70 loss: 0.5843887120485306\n  batch 80 loss: 0.5933282673358917\n  batch 90 loss: 0.7228568106889725\n  batch 100 loss: 0.7895067989826202\n  batch 110 loss: 0.5701173409819603\n  batch 120 loss: 0.6392798960208893\n  batch 130 loss: 0.6379745066165924\n  batch 140 loss: 0.7503209233283996\n  batch 150 loss: 0.7125335812568665\nEPOCH  5 val loss:  2.780098198188676\nEPOCH 6:\n  batch 10 loss: 0.67474664747715\n  batch 20 loss: 0.6657732665538788\n  batch 30 loss: 0.8658116698265076\n  batch 40 loss: 0.6225085884332657\n  batch 50 loss: 0.8290651202201843\n  batch 60 loss: 0.7508853137493133\n  batch 70 loss: 0.7851234585046768\n  batch 80 loss: 0.6660753786563873\n  batch 90 loss: 0.7398007214069366\n  batch 100 loss: 0.6378250420093536\n  batch 110 loss: 0.6428227990865707\n  batch 120 loss: 0.7807805091142654\n  batch 130 loss: 0.7330340966582298\n  batch 140 loss: 0.465577095746994\n  batch 150 loss: 0.6730020105838775\nEPOCH  6 val loss:  3.7957247156235905\nEPOCH 7:\n  batch 10 loss: 0.4954648450016975\n  batch 20 loss: 0.6922115623950958\n  batch 30 loss: 0.7736589804291725\n  batch 40 loss: 0.8282858014106751\n  batch 50 loss: 0.7830479294061661\n  batch 60 loss: 0.5360048025846481\n  batch 70 loss: 0.6684593379497528\n  batch 80 loss: 0.5201552480459213\n  batch 90 loss: 0.5679045081138611\n  batch 100 loss: 0.6744756937026978\n  batch 110 loss: 0.5201525062322616\n  batch 120 loss: 0.5074390530586242\n  batch 130 loss: 1.2504442870616912\n  batch 140 loss: 0.7632704734802246\n  batch 150 loss: 0.8764172554016113\nEPOCH  7 val loss:  0.7572021736866899\nEPOCH 8:\n  batch 10 loss: 0.5820069581270217\n  batch 20 loss: 0.6440314918756485\n  batch 30 loss: 0.6386456400156021\n  batch 40 loss: 0.6473677843809128\n  batch 50 loss: 0.6273968517780304\n  batch 60 loss: 0.6087708920240402\n  batch 70 loss: 0.62511205971241\n  batch 80 loss: 0.6399880722165108\n  batch 90 loss: 0.9378433972597122\n  batch 100 loss: 0.656480211019516\n  batch 110 loss: 0.6050853997468948\n  batch 120 loss: 0.6435927599668503\n  batch 130 loss: 0.6674213200807572\n  batch 140 loss: 0.7203455835580825\n  batch 150 loss: 0.7567423790693283\nEPOCH  8 val loss:  1.1136940204434924\nEPOCH 9:\n  batch 10 loss: 0.4776506572961807\n  batch 20 loss: 0.8395299822092056\n  batch 30 loss: 0.6639555901288986\n  batch 40 loss: 0.5235420674085617\n  batch 50 loss: 0.7660651594400406\n  batch 60 loss: 0.7612378180027009\n  batch 70 loss: 0.6917467415332794\n  batch 80 loss: 0.5712214827537536\n  batch 90 loss: 0.7080876886844635\n  batch 100 loss: 0.5558500111103057\n  batch 110 loss: 0.5842748671770096\n  batch 120 loss: 0.6272496104240417\n  batch 130 loss: 0.8015540182590485\n  batch 140 loss: 0.6005944162607193\n  batch 150 loss: 0.6936697512865067\nEPOCH  9 val loss:  2.810265079140663\nEPOCH 10:\n  batch 10 loss: 0.5204189106822014\n  batch 20 loss: 0.7798856779932976\n  batch 30 loss: 0.5513177752494812\n  batch 40 loss: 0.6209012657403946\n  batch 50 loss: 0.639155724644661\n  batch 60 loss: 0.5964416980743408\n  batch 70 loss: 0.6437805518507957\n  batch 80 loss: 0.7424193620681763\n  batch 90 loss: 0.6324858039617538\n  batch 100 loss: 0.5914859056472779\n  batch 110 loss: 0.6982011049985886\n  batch 120 loss: 0.80849789083004\n  batch 130 loss: 0.604424637556076\n  batch 140 loss: 0.5674979656934738\n  batch 150 loss: 0.7179816693067551\nEPOCH  10 val loss:  0.6891908546288809\nsave model concat-tile-pooling-384-effv2-fold4.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# for input, label in training_loader:\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-09-18T09:50:42.725367Z","iopub.execute_input":"2022-09-18T09:50:42.729151Z","iopub.status.idle":"2022-09-18T09:50:42.737281Z","shell.execute_reply.started":"2022-09-18T09:50:42.729105Z","shell.execute_reply":"2022-09-18T09:50:42.734261Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# label\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T09:50:42.738846Z","iopub.execute_input":"2022-09-18T09:50:42.739384Z","iopub.status.idle":"2022-09-18T09:50:42.744705Z","shell.execute_reply.started":"2022-09-18T09:50:42.739347Z","shell.execute_reply":"2022-09-18T09:50:42.743654Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# torch.save(model.state_dict(), \"concat-tile-pooling-model.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-09-18T09:50:42.746158Z","iopub.execute_input":"2022-09-18T09:50:42.746989Z","iopub.status.idle":"2022-09-18T09:50:42.753797Z","shell.execute_reply.started":"2022-09-18T09:50:42.746954Z","shell.execute_reply":"2022-09-18T09:50:42.752796Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/models\n!mv /kaggle/working/*.pth /kaggle/working/models/","metadata":{"execution":{"iopub.status.busy":"2022-09-18T09:50:42.755237Z","iopub.execute_input":"2022-09-18T09:50:42.756007Z","iopub.status.idle":"2022-09-18T09:50:45.066206Z","shell.execute_reply.started":"2022-09-18T09:50:42.755972Z","shell.execute_reply":"2022-09-18T09:50:45.064622Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -cvf /kaggle/working/models.tar /kaggle/working/models/","metadata":{"execution":{"iopub.status.busy":"2022-09-18T09:50:45.070559Z","iopub.execute_input":"2022-09-18T09:50:45.071543Z","iopub.status.idle":"2022-09-18T09:50:46.592289Z","shell.execute_reply.started":"2022-09-18T09:50:45.071499Z","shell.execute_reply":"2022-09-18T09:50:46.591080Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"tar: Removing leading `/' from member names\n/kaggle/working/models/\n/kaggle/working/models/concat-tile-pooling-384-effv2-fold3.pth\n/kaggle/working/models/concat-tile-pooling-384-effv2-fold0.pth\n/kaggle/working/models/concat-tile-pooling-384-effv2-fold2.pth\n/kaggle/working/models/concat-tile-pooling-384-effv2-fold4.pth\n/kaggle/working/models/concat-tile-pooling-384-effv2-fold1.pth\n","output_type":"stream"}]}]}