{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1466f6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:15.840646Z",
     "iopub.status.busy": "2022-09-26T12:55:15.839811Z",
     "iopub.status.idle": "2022-09-26T12:55:15.850273Z",
     "shell.execute_reply": "2022-09-26T12:55:15.849449Z"
    },
    "papermill": {
     "duration": 0.020052,
     "end_time": "2022-09-26T12:55:15.852299",
     "exception": false,
     "start_time": "2022-09-26T12:55:15.832247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sz = 384\n",
    "bs = 4\n",
    "nfolds = 10\n",
    "SEED = 2022\n",
    "N = 16 #number of tiles per image\n",
    "EPOCHS = 10\n",
    "N_tile = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124be1ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:15.863172Z",
     "iopub.status.busy": "2022-09-26T12:55:15.862349Z",
     "iopub.status.idle": "2022-09-26T12:55:15.867007Z",
     "shell.execute_reply": "2022-09-26T12:55:15.866197Z"
    },
    "papermill": {
     "duration": 0.011953,
     "end_time": "2022-09-26T12:55:15.869003",
     "exception": false,
     "start_time": "2022-09-26T12:55:15.857050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8565916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:15.879979Z",
     "iopub.status.busy": "2022-09-26T12:55:15.879197Z",
     "iopub.status.idle": "2022-09-26T12:55:20.559891Z",
     "shell.execute_reply": "2022-09-26T12:55:20.558556Z"
    },
    "papermill": {
     "duration": 4.688603,
     "end_time": "2022-09-26T12:55:20.562480",
     "exception": false,
     "start_time": "2022-09-26T12:55:15.873877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from fastai.vision import *\n",
    "from fastai.layers import AdaptiveConcatPool2d, Flatten, Mish\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbb21da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:20.588552Z",
     "iopub.status.busy": "2022-09-26T12:55:20.587155Z",
     "iopub.status.idle": "2022-09-26T12:55:20.664216Z",
     "shell.execute_reply": "2022-09-26T12:55:20.663331Z"
    },
    "papermill": {
     "duration": 0.091076,
     "end_time": "2022-09-26T12:55:20.666869",
     "exception": false,
     "start_time": "2022-09-26T12:55:20.575793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc53b93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:20.682433Z",
     "iopub.status.busy": "2022-09-26T12:55:20.681969Z",
     "iopub.status.idle": "2022-09-26T12:55:20.697388Z",
     "shell.execute_reply": "2022-09-26T12:55:20.696558Z"
    },
    "papermill": {
     "duration": 0.026047,
     "end_time": "2022-09-26T12:55:20.699953",
     "exception": false,
     "start_time": "2022-09-26T12:55:20.673906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionSoftMax(torch.nn.Module):\n",
    "    def __init__(self, in_features=3, out_features = None):\n",
    "        super(AttentionSoftMax, self).__init__()\n",
    "        self.otherdim = 'b'\n",
    "        if out_features is None:\n",
    "            out_features = in_features\n",
    "\n",
    "        self.layer_linear_tr = nn.Linear(in_features, out_features) \n",
    "        self.activation = nn.LeakyReLU() \n",
    "        self.layer_linear_query= nn.Linear(out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.layer_linear_tr(x)\n",
    "        keys = self.activation(keys)\n",
    "\n",
    "        attention_map_raw = self.layer_linear_query(keys)[...,0]\n",
    "        attention_map = nn.Softmax(dim=-1)(attention_map_raw)\n",
    "\n",
    "        result = torch.einsum(f'{self.otherdim}i, {self.otherdim}ij->{self.otherdim}j', attention_map, x)\n",
    "        return result\n",
    "\n",
    "class Model (nn.Module):\n",
    "    def __init__(self, arch='tf_efficientnetv2_s', n=2, pre=True, enc_out_feat=1280): \n",
    "        super().__init__()\n",
    "        m = timm.create_model(model_name=arch, pretrained = pre, num_classes = 0)\n",
    "        self.enc = nn.Sequential(*list(m.children())[:-1])\n",
    "        self.enc_out_feat = enc_out_feat\n",
    "        self.head = nn.Sequential(AttentionSoftMax(enc_out_feat), nn.Dropout(0.5), nn.Linear(enc_out_feat,n))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        bs, instance_num, c, w, h = x.shape \n",
    "        x = x.view(bs*instance_num,c,w,h)\n",
    "\n",
    "        # print(x.shape)\n",
    "        #x: bs*instance_num x cx wxh\n",
    "\n",
    "        x = self.enc(x)\n",
    "        #x: bs instance_num x enc_out_feat\n",
    "\n",
    "        x = x.view(bs,instance_num, self.enc_out_feat).contiguous()\n",
    "\n",
    "        #x: bs x instance_num x enc_out_feat\n",
    "        x = self.head(x)\n",
    "        #x: bs x n\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc2f081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:20.715450Z",
     "iopub.status.busy": "2022-09-26T12:55:20.715115Z",
     "iopub.status.idle": "2022-09-26T12:55:20.720373Z",
     "shell.execute_reply": "2022-09-26T12:55:20.719561Z"
    },
    "papermill": {
     "duration": 0.017087,
     "end_time": "2022-09-26T12:55:20.724433",
     "exception": false,
     "start_time": "2022-09-26T12:55:20.707346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #PyTorch\n",
    "# class DiceBCELoss(nn.Module):\n",
    "#     def __init__(self, weight=None, size_average=True):\n",
    "#         super(DiceBCELoss, self).__init__()\n",
    "        \n",
    "#     def onehot_onezero(y_true):\n",
    "#         res = []\n",
    "#         for i in y_true:\n",
    "#             if i == 0:\n",
    "#                 res.append(torch.tensor([1., 0.]))\n",
    "#             else:\n",
    "#                 res.append(torch.tensor([0., 1.]))\n",
    "\n",
    "#         return torch.stack(res)\n",
    "\n",
    "#     def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "#         #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "#         inputs = F.sigmoid(inputs)  \n",
    "        \n",
    "#         targets = DiceBCELoss.onehot_onezero(targets).to(device)\n",
    "#         #flatten label and prediction tensors\n",
    "#         inputs = inputs.view(-1)\n",
    "#         targets = targets.view(-1)\n",
    "        \n",
    "#         intersection = (inputs * targets).sum()                            \n",
    "#         dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "#         BCE = torch.nn.BCELoss()(inputs, targets)\n",
    "#         Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "#         return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2334a47d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:20.739553Z",
     "iopub.status.busy": "2022-09-26T12:55:20.739214Z",
     "iopub.status.idle": "2022-09-26T12:55:20.748973Z",
     "shell.execute_reply": "2022-09-26T12:55:20.748175Z"
    },
    "papermill": {
     "duration": 0.019963,
     "end_time": "2022-09-26T12:55:20.751434",
     "exception": false,
     "start_time": "2022-09-26T12:55:20.731471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLossWithOneHot(nn.Module):\n",
    "    def __init__(self, gamma=0, eps=1e-7):\n",
    "        super(FocalLossWithOneHot, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        y = torch.nn.functional.one_hot(target.to(torch.int64), num_classes=2)\n",
    "\n",
    "        logit = torch.nn.functional.softmax(input, dim=-1)\n",
    "        logit = logit.clamp(self.eps, 1. - self.eps)\n",
    "\n",
    "        loss = -1 * y * torch.log(logit) # cross entropy\n",
    "        loss = loss * (1 - logit) ** self.gamma # focal loss\n",
    "\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c033e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:20.766486Z",
     "iopub.status.busy": "2022-09-26T12:55:20.766160Z",
     "iopub.status.idle": "2022-09-26T12:55:39.860947Z",
     "shell.execute_reply": "2022-09-26T12:55:39.859694Z"
    },
    "papermill": {
     "duration": 19.105067,
     "end_time": "2022-09-26T12:55:39.863401",
     "exception": false,
     "start_time": "2022-09-26T12:55:20.758334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s-eb54923e.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnetv2_s-eb54923e.pth\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model = model.to(device)\n",
    "\n",
    "# クロスエントロピー損失関数使用\n",
    "# loss_fn = nn.BCEWithLogitsLoss().cuda()\n",
    "loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "# loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
    "\n",
    "# loss_fn = FocalLossWithOneHot(gamma=2).cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# optimizer = SGD(model, 0.1)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "# 前処理\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((sz, sz)),\n",
    "#     torchvision.transforms.RandAugment(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(45),\n",
    "#     torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "transform_val = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((sz, sz)),\n",
    "#     torchvision.transforms.RandAugment(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900b29b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:39.879722Z",
     "iopub.status.busy": "2022-09-26T12:55:39.879356Z",
     "iopub.status.idle": "2022-09-26T12:55:39.884590Z",
     "shell.execute_reply": "2022-09-26T12:55:39.883447Z"
    },
    "papermill": {
     "duration": 0.016594,
     "end_time": "2022-09-26T12:55:39.887801",
     "exception": false,
     "start_time": "2022-09-26T12:55:39.871207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423e7529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:39.903332Z",
     "iopub.status.busy": "2022-09-26T12:55:39.902740Z",
     "iopub.status.idle": "2022-09-26T12:55:39.907162Z",
     "shell.execute_reply": "2022-09-26T12:55:39.906168Z"
    },
    "papermill": {
     "duration": 0.014911,
     "end_time": "2022-09-26T12:55:39.909839",
     "exception": false,
     "start_time": "2022-09-26T12:55:39.894928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary(model=model, input_size=(bs, N, 3, sz, sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719e788e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:39.925376Z",
     "iopub.status.busy": "2022-09-26T12:55:39.925093Z",
     "iopub.status.idle": "2022-09-26T12:55:39.977260Z",
     "shell.execute_reply": "2022-09-26T12:55:39.975012Z"
    },
    "papermill": {
     "duration": 0.063045,
     "end_time": "2022-09-26T12:55:39.980033",
     "exception": false,
     "start_time": "2022-09-26T12:55:39.916988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# This block is reffered from https://www.kaggle.com/code/yasufuminakama/mayo-train-images-size-1024-n-16-1/notebook\n",
    "train = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/train.csv')\n",
    "# train = train[train[\"image_id\"] != \"2c3c06_0\"]\n",
    "\n",
    "train['image_dir'] = ''\n",
    "\n",
    "train.loc[:100,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_1/'\n",
    "train.loc[100:200,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_2/'\n",
    "train.loc[200:300,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_3/'\n",
    "train.loc[300:400,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_4/'\n",
    "train.loc[400:500,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_5/'\n",
    "train.loc[500:600,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_6/'\n",
    "train.loc[600:700,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_7/'\n",
    "train.loc[700:,'image_dir'] = '../input/mayo-tiled-16-384x384/train_images_noscale/train_images/train_images_8/'\n",
    "# train.loc[:100,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_1/'\n",
    "# train.loc[100:200,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_2/'\n",
    "# train.loc[200:300,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_3/'\n",
    "# train.loc[300:400,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_4/'\n",
    "# train.loc[400:500,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_5/'\n",
    "# train.loc[500:600,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_6/'\n",
    "# train.loc[600:700,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_7/'\n",
    "# train.loc[700:,'image_dir'] = '/kaggle/input/mayo-tiled-16-384x384/train_images/train_images/train_images_8/'\n",
    "\n",
    "target_mapper = {\"CE\": 0, \"LAA\": 1}\n",
    "\n",
    "train[\"target\"] = train[\"label\"].map(lambda x: target_mapper[x])\n",
    "\n",
    "splits = StratifiedKFold(n_splits=nfolds, random_state=SEED, shuffle=True)\n",
    "splits = list(splits.split(train,train.center_id))\n",
    "folds_splits = np.zeros(len(train)).astype(np.int)\n",
    "for i in range(nfolds): folds_splits[splits[i][1]] = i\n",
    "train['split'] = folds_splits\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df, transform=None, aug=True):\n",
    "        self.cfg = cfg\n",
    "        self.image_ids = df['image_id'].values\n",
    "        self.image_dirs = df['image_dir'].values\n",
    "#         self.image_path = df[\"path\"].values\n",
    "        self.labels = df['target'].values\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_dir = self.image_dirs[idx]\n",
    "        images = []\n",
    "#         img_indexes = random.sample(list(range(0, N_tile)), N)\n",
    "        img_indexes = list(range(0, N))\n",
    "\n",
    "        for i in img_indexes:\n",
    "            path = image_dir + image_id + f'_{i}.jpg'\n",
    "            image = cv2.imread(path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = self.transform(image)\n",
    "            images.append(image)\n",
    "        images = torch.stack(images, dim=0)\n",
    "        \n",
    "#         if self.aug:\n",
    "#             images = torchvision.transforms.RandAugment()(images)\n",
    "            \n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return images, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7750fe69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:39.995509Z",
     "iopub.status.busy": "2022-09-26T12:55:39.995215Z",
     "iopub.status.idle": "2022-09-26T12:55:40.008710Z",
     "shell.execute_reply": "2022-09-26T12:55:40.007575Z"
    },
    "papermill": {
     "duration": 0.023839,
     "end_time": "2022-09-26T12:55:40.010978",
     "exception": false,
     "start_time": "2022-09-26T12:55:39.987139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, train_loader, val_loader, optimizer, scheduler, epoch, loss_fn):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    val_loss = 0.\n",
    "    model.train(True)\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data.to(device), target.to(device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             loss = loss_fn(torch.squeeze(outputs), labels.float())\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         if len(outputs) != bs:\n",
    "#             print(outputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch * len(train_loader) + i + 1\n",
    "            running_loss = 0.\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, (data, target) in enumerate(val_loader):\n",
    "            inputs, labels = data.to(device), target.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "#                 loss = loss_fn(torch.squeeze(outputs), labels.float())\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "132c9af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:40.028279Z",
     "iopub.status.busy": "2022-09-26T12:55:40.027648Z",
     "iopub.status.idle": "2022-09-26T12:55:40.032664Z",
     "shell.execute_reply": "2022-09-26T12:55:40.031208Z"
    },
    "papermill": {
     "duration": 0.016245,
     "end_time": "2022-09-26T12:55:40.035688",
     "exception": false,
     "start_time": "2022-09-26T12:55:40.019443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, one_batch in enumerate(training_loader):\n",
    "#     x = one_batch[0]\n",
    "#     print(x.shape)\n",
    "#     y = one_batch[1]\n",
    "#     x = [x for x in x]\n",
    "#     shape = x[0].shape\n",
    "#     print(torch.stack(x,1).view(-1,shape[1],shape[2],shape[3]).shape)\n",
    "#     if i==5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0398d3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:40.051324Z",
     "iopub.status.busy": "2022-09-26T12:55:40.050872Z",
     "iopub.status.idle": "2022-09-26T12:55:40.055473Z",
     "shell.execute_reply": "2022-09-26T12:55:40.054384Z"
    },
    "papermill": {
     "duration": 0.01562,
     "end_time": "2022-09-26T12:55:40.058495",
     "exception": false,
     "start_time": "2022-09-26T12:55:40.042875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e482c7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:55:40.073926Z",
     "iopub.status.busy": "2022-09-26T12:55:40.073481Z",
     "iopub.status.idle": "2022-09-26T18:47:57.877131Z",
     "shell.execute_reply": "2022-09-26T18:47:57.875260Z"
    },
    "papermill": {
     "duration": 21137.903862,
     "end_time": "2022-09-26T18:47:57.969173",
     "exception": false,
     "start_time": "2022-09-26T12:55:40.065311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.7263397216796875\n",
      "  batch 20 loss: 0.8704562902450561\n",
      "  batch 30 loss: 0.6384628295898438\n",
      "  batch 40 loss: 0.5520227432250977\n",
      "  batch 50 loss: 0.6139557838439942\n",
      "  batch 60 loss: 0.6144134521484375\n",
      "  batch 70 loss: 0.5399444580078125\n",
      "  batch 80 loss: 0.6169998168945312\n",
      "  batch 90 loss: 0.6946197509765625\n",
      "  batch 100 loss: 0.576275634765625\n",
      "  batch 110 loss: 0.575006103515625\n",
      "  batch 120 loss: 0.724993896484375\n",
      "  batch 130 loss: 0.586480712890625\n",
      "  batch 140 loss: 0.510113525390625\n",
      "  batch 150 loss: 0.676507568359375\n",
      "  batch 160 loss: 0.62576904296875\n",
      "EPOCH  1 val loss:  0.6890462239583334\n",
      "save model concat-tile-pooling-384-effv2-fold0.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.573504638671875\n",
      "  batch 20 loss: 0.63614501953125\n",
      "  batch 30 loss: 0.4742156982421875\n",
      "  batch 40 loss: 0.63292236328125\n",
      "  batch 50 loss: 0.614495849609375\n",
      "  batch 60 loss: 0.7197418212890625\n",
      "  batch 70 loss: 0.5985107421875\n",
      "  batch 80 loss: 0.48173828125\n",
      "  batch 90 loss: 0.5878524780273438\n",
      "  batch 100 loss: 0.559185791015625\n",
      "  batch 110 loss: 0.63199462890625\n",
      "  batch 120 loss: 0.6209716796875\n",
      "  batch 130 loss: 0.5407440185546875\n",
      "  batch 140 loss: 0.65372314453125\n",
      "  batch 150 loss: 0.579827880859375\n",
      "  batch 160 loss: 0.6710693359375\n",
      "EPOCH  2 val loss:  0.6545867919921875\n",
      "save model concat-tile-pooling-384-effv2-fold0.pth\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.4792449951171875\n",
      "  batch 20 loss: 0.6242156982421875\n",
      "  batch 30 loss: 0.5876953125\n",
      "  batch 40 loss: 0.581976318359375\n",
      "  batch 50 loss: 0.63116455078125\n",
      "  batch 60 loss: 0.543218994140625\n",
      "  batch 70 loss: 0.5557708740234375\n",
      "  batch 80 loss: 0.6420623779296875\n",
      "  batch 90 loss: 0.6174774169921875\n",
      "  batch 100 loss: 0.5406463623046875\n",
      "  batch 110 loss: 0.631243896484375\n",
      "  batch 120 loss: 0.52110595703125\n",
      "  batch 130 loss: 0.650970458984375\n",
      "  batch 140 loss: 0.56527099609375\n",
      "  batch 150 loss: 0.5924468994140625\n",
      "  batch 160 loss: 0.5411224365234375\n",
      "EPOCH  3 val loss:  0.6961008707682291\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.5788970947265625\n",
      "  batch 20 loss: 0.599346923828125\n",
      "  batch 30 loss: 0.6026611328125\n",
      "  batch 40 loss: 0.743084716796875\n",
      "  batch 50 loss: 0.532623291015625\n",
      "  batch 60 loss: 0.656866455078125\n",
      "  batch 70 loss: 0.5871185302734375\n",
      "  batch 80 loss: 0.6570587158203125\n",
      "  batch 90 loss: 0.53692626953125\n",
      "  batch 100 loss: 0.456097412109375\n",
      "  batch 110 loss: 0.6122467041015625\n",
      "  batch 120 loss: 0.5066253662109375\n",
      "  batch 130 loss: 0.561114501953125\n",
      "  batch 140 loss: 0.6046234130859375\n",
      "  batch 150 loss: 0.5608154296875\n",
      "  batch 160 loss: 0.515753173828125\n",
      "EPOCH  4 val loss:  0.6875508626302084\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.603485107421875\n",
      "  batch 20 loss: 0.5897247314453125\n",
      "  batch 30 loss: 0.681951904296875\n",
      "  batch 40 loss: 0.49361572265625\n",
      "  batch 50 loss: 0.6298187255859375\n",
      "  batch 60 loss: 0.506182861328125\n",
      "  batch 70 loss: 0.6317108154296875\n",
      "  batch 80 loss: 0.52633056640625\n",
      "  batch 90 loss: 0.615618896484375\n",
      "  batch 100 loss: 0.649853515625\n",
      "  batch 110 loss: 0.612091064453125\n",
      "  batch 120 loss: 0.658673095703125\n",
      "  batch 130 loss: 0.631121826171875\n",
      "  batch 140 loss: 0.5429779052734375\n",
      "  batch 150 loss: 0.4945404052734375\n",
      "  batch 160 loss: 0.371246337890625\n",
      "EPOCH  5 val loss:  0.7370639377170138\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.6698486328125\n",
      "  batch 20 loss: 0.6641326904296875\n",
      "  batch 30 loss: 0.51912841796875\n",
      "  batch 40 loss: 0.5472747802734375\n",
      "  batch 50 loss: 0.631884765625\n",
      "  batch 60 loss: 0.49549102783203125\n",
      "  batch 70 loss: 0.714776611328125\n",
      "  batch 80 loss: 0.609814453125\n",
      "  batch 90 loss: 0.653436279296875\n",
      "  batch 100 loss: 0.5474456787109375\n",
      "  batch 110 loss: 0.5609222412109375\n",
      "  batch 120 loss: 0.4668548583984375\n",
      "  batch 130 loss: 0.614422607421875\n",
      "  batch 140 loss: 0.4389678955078125\n",
      "  batch 150 loss: 0.548114013671875\n",
      "  batch 160 loss: 0.7566162109375\n",
      "EPOCH  6 val loss:  0.6774868435329862\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.6091400146484375\n",
      "  batch 20 loss: 0.5778228759765625\n",
      "  batch 30 loss: 0.66353759765625\n",
      "  batch 40 loss: 0.655096435546875\n",
      "  batch 50 loss: 0.5569488525390625\n",
      "  batch 60 loss: 0.5697998046875\n",
      "  batch 70 loss: 0.403076171875\n",
      "  batch 80 loss: 0.45971527099609377\n",
      "  batch 90 loss: 0.5722747802734375\n",
      "  batch 100 loss: 0.7253631591796875\n",
      "  batch 110 loss: 0.5674102783203125\n",
      "  batch 120 loss: 0.498199462890625\n",
      "  batch 130 loss: 0.6689178466796875\n",
      "  batch 140 loss: 0.64757080078125\n",
      "  batch 150 loss: 0.449603271484375\n",
      "  batch 160 loss: 0.5355743408203125\n",
      "EPOCH  7 val loss:  0.6752048068576388\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.5970703125\n",
      "  batch 20 loss: 0.481072998046875\n",
      "  batch 30 loss: 0.704150390625\n",
      "  batch 40 loss: 0.5928009033203125\n",
      "  batch 50 loss: 0.5714019775390625\n",
      "  batch 60 loss: 0.61737060546875\n",
      "  batch 70 loss: 0.662530517578125\n",
      "  batch 80 loss: 0.6148406982421875\n",
      "  batch 90 loss: 0.64049072265625\n",
      "  batch 100 loss: 0.4944610595703125\n",
      "  batch 110 loss: 0.5526702880859375\n",
      "  batch 120 loss: 0.6470245361328125\n",
      "  batch 130 loss: 0.6065093994140625\n",
      "  batch 140 loss: 0.5484649658203125\n",
      "  batch 150 loss: 0.6334381103515625\n",
      "  batch 160 loss: 0.5890350341796875\n",
      "EPOCH  8 val loss:  0.6611599392361112\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.470416259765625\n",
      "  batch 20 loss: 0.5380584716796875\n",
      "  batch 30 loss: 0.5725234985351563\n",
      "  batch 40 loss: 0.5958847045898438\n",
      "  batch 50 loss: 0.4648834228515625\n",
      "  batch 60 loss: 0.661541748046875\n",
      "  batch 70 loss: 0.450933837890625\n",
      "  batch 80 loss: 0.56417236328125\n",
      "  batch 90 loss: 0.7598419189453125\n",
      "  batch 100 loss: 0.655828857421875\n",
      "  batch 110 loss: 0.681890869140625\n",
      "  batch 120 loss: 0.62918701171875\n",
      "  batch 130 loss: 0.586993408203125\n",
      "  batch 140 loss: 0.6577362060546875\n",
      "  batch 150 loss: 0.5816650390625\n",
      "  batch 160 loss: 0.547296142578125\n",
      "EPOCH  9 val loss:  0.8531324598524306\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.4521331787109375\n",
      "  batch 20 loss: 0.6812774658203125\n",
      "  batch 30 loss: 0.569659423828125\n",
      "  batch 40 loss: 0.5608154296875\n",
      "  batch 50 loss: 0.5104034423828125\n",
      "  batch 60 loss: 0.6326629638671875\n",
      "  batch 70 loss: 0.560528564453125\n",
      "  batch 80 loss: 0.59730224609375\n",
      "  batch 90 loss: 0.573065185546875\n",
      "  batch 100 loss: 0.4786346435546875\n",
      "  batch 110 loss: 0.639849853515625\n",
      "  batch 120 loss: 0.538037109375\n",
      "  batch 130 loss: 0.619415283203125\n",
      "  batch 140 loss: 0.520098876953125\n",
      "  batch 150 loss: 0.602215576171875\n",
      "  batch 160 loss: 0.7082855224609375\n",
      "EPOCH  10 val loss:  0.6753913031684028\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.72764892578125\n",
      "  batch 20 loss: 0.63447265625\n",
      "  batch 30 loss: 0.56087646484375\n",
      "  batch 40 loss: 0.5272216796875\n",
      "  batch 50 loss: 0.491326904296875\n",
      "  batch 60 loss: 0.661822509765625\n",
      "  batch 70 loss: 0.6647247314453125\n",
      "  batch 80 loss: 0.5745574951171875\n",
      "  batch 90 loss: 0.5354095458984375\n",
      "  batch 100 loss: 0.5260986328125\n",
      "  batch 110 loss: 0.615093994140625\n",
      "  batch 120 loss: 0.543963623046875\n",
      "  batch 130 loss: 0.576336669921875\n",
      "  batch 140 loss: 0.46077880859375\n",
      "  batch 150 loss: 0.5644744873046875\n",
      "  batch 160 loss: 0.6465850830078125\n",
      "EPOCH  1 val loss:  0.6436191134982638\n",
      "save model concat-tile-pooling-384-effv2-fold1.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.669091796875\n",
      "  batch 20 loss: 0.569366455078125\n",
      "  batch 30 loss: 0.5337310791015625\n",
      "  batch 40 loss: 0.691265869140625\n",
      "  batch 50 loss: 0.573291015625\n",
      "  batch 60 loss: 0.5908050537109375\n",
      "  batch 70 loss: 0.48980712890625\n",
      "  batch 80 loss: 0.4489288330078125\n",
      "  batch 90 loss: 0.5938446044921875\n",
      "  batch 100 loss: 0.492547607421875\n",
      "  batch 110 loss: 0.7676300048828125\n",
      "  batch 120 loss: 0.617694091796875\n",
      "  batch 130 loss: 0.6182861328125\n",
      "  batch 140 loss: 0.503228759765625\n",
      "  batch 150 loss: 0.697137451171875\n",
      "  batch 160 loss: 0.6373443603515625\n",
      "EPOCH  2 val loss:  0.6358150906032987\n",
      "save model concat-tile-pooling-384-effv2-fold1.pth\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.6475616455078125\n",
      "  batch 20 loss: 0.6286956787109375\n",
      "  batch 30 loss: 0.5521514892578125\n",
      "  batch 40 loss: 0.4283477783203125\n",
      "  batch 50 loss: 0.57532958984375\n",
      "  batch 60 loss: 0.599676513671875\n",
      "  batch 70 loss: 0.5727447509765625\n",
      "  batch 80 loss: 0.587030029296875\n",
      "  batch 90 loss: 0.47664794921875\n",
      "  batch 100 loss: 0.5858245849609375\n",
      "  batch 110 loss: 0.7010498046875\n",
      "  batch 120 loss: 0.61734619140625\n",
      "  batch 130 loss: 0.67198486328125\n",
      "  batch 140 loss: 0.547857666015625\n",
      "  batch 150 loss: 0.6896636962890625\n",
      "  batch 160 loss: 0.5659637451171875\n",
      "EPOCH  3 val loss:  0.6238725450303819\n",
      "save model concat-tile-pooling-384-effv2-fold1.pth\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.551336669921875\n",
      "  batch 20 loss: 0.653271484375\n",
      "  batch 30 loss: 0.67530517578125\n",
      "  batch 40 loss: 0.6479522705078125\n",
      "  batch 50 loss: 0.497210693359375\n",
      "  batch 60 loss: 0.5541259765625\n",
      "  batch 70 loss: 0.538616943359375\n",
      "  batch 80 loss: 0.752484130859375\n",
      "  batch 90 loss: 0.5074249267578125\n",
      "  batch 100 loss: 0.662261962890625\n",
      "  batch 110 loss: 0.5463043212890625\n",
      "  batch 120 loss: 0.6418426513671875\n",
      "  batch 130 loss: 0.5782958984375\n",
      "  batch 140 loss: 0.630633544921875\n",
      "  batch 150 loss: 0.5552703857421875\n",
      "  batch 160 loss: 0.5319854736328125\n",
      "EPOCH  4 val loss:  0.6396484375\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.7158203125\n",
      "  batch 20 loss: 0.5086334228515625\n",
      "  batch 30 loss: 0.6716796875\n",
      "  batch 40 loss: 0.63717041015625\n",
      "  batch 50 loss: 0.576849365234375\n",
      "  batch 60 loss: 0.5136993408203125\n",
      "  batch 70 loss: 0.60745849609375\n",
      "  batch 80 loss: 0.5761260986328125\n",
      "  batch 90 loss: 0.7209442138671875\n",
      "  batch 100 loss: 0.587103271484375\n",
      "  batch 110 loss: 0.54901123046875\n",
      "  batch 120 loss: 0.647216796875\n",
      "  batch 130 loss: 0.62552490234375\n",
      "  batch 140 loss: 0.5594024658203125\n",
      "  batch 150 loss: 0.5239959716796875\n",
      "  batch 160 loss: 0.5403228759765625\n",
      "EPOCH  5 val loss:  0.6111551920572916\n",
      "save model concat-tile-pooling-384-effv2-fold1.pth\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.6004623413085938\n",
      "  batch 20 loss: 0.44502410888671873\n",
      "  batch 30 loss: 0.5072265625\n",
      "  batch 40 loss: 0.539190673828125\n",
      "  batch 50 loss: 0.50133056640625\n",
      "  batch 60 loss: 0.6027496337890625\n",
      "  batch 70 loss: 0.5717315673828125\n",
      "  batch 80 loss: 0.628765869140625\n",
      "  batch 90 loss: 0.5772003173828125\n",
      "  batch 100 loss: 0.5224945068359375\n",
      "  batch 110 loss: 0.540716552734375\n",
      "  batch 120 loss: 0.54302978515625\n",
      "  batch 130 loss: 0.7408233642578125\n",
      "  batch 140 loss: 0.5691375732421875\n",
      "  batch 150 loss: 0.706219482421875\n",
      "  batch 160 loss: 0.61544189453125\n",
      "EPOCH  6 val loss:  0.6189744737413194\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.6074798583984375\n",
      "  batch 20 loss: 0.59700927734375\n",
      "  batch 30 loss: 0.5546905517578125\n",
      "  batch 40 loss: 0.5116455078125\n",
      "  batch 50 loss: 0.627752685546875\n",
      "  batch 60 loss: 0.5614990234375\n",
      "  batch 70 loss: 0.7883941650390625\n",
      "  batch 80 loss: 0.56497802734375\n",
      "  batch 90 loss: 0.5268585205078125\n",
      "  batch 100 loss: 0.6029754638671875\n",
      "  batch 110 loss: 0.502117919921875\n",
      "  batch 120 loss: 0.5572296142578125\n",
      "  batch 130 loss: 0.557763671875\n",
      "  batch 140 loss: 0.58873291015625\n",
      "  batch 150 loss: 0.62403564453125\n",
      "  batch 160 loss: 0.5893646240234375\n",
      "EPOCH  7 val loss:  0.6173044840494791\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.604296875\n",
      "  batch 20 loss: 0.620965576171875\n",
      "  batch 30 loss: 0.54486083984375\n",
      "  batch 40 loss: 0.62239990234375\n",
      "  batch 50 loss: 0.4860443115234375\n",
      "  batch 60 loss: 0.5473785400390625\n",
      "  batch 70 loss: 0.7086883544921875\n",
      "  batch 80 loss: 0.56298828125\n",
      "  batch 90 loss: 0.547100830078125\n",
      "  batch 100 loss: 0.5921112060546875\n",
      "  batch 110 loss: 0.593328857421875\n",
      "  batch 120 loss: 0.5194610595703125\n",
      "  batch 130 loss: 0.5348297119140625\n",
      "  batch 140 loss: 0.7043243408203125\n",
      "  batch 150 loss: 0.687481689453125\n",
      "  batch 160 loss: 0.5687744140625\n",
      "EPOCH  8 val loss:  0.6285824245876737\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.58892822265625\n",
      "  batch 20 loss: 0.683502197265625\n",
      "  batch 30 loss: 0.444110107421875\n",
      "  batch 40 loss: 0.68857421875\n",
      "  batch 50 loss: 0.456976318359375\n",
      "  batch 60 loss: 0.6174652099609375\n",
      "  batch 70 loss: 0.5337677001953125\n",
      "  batch 80 loss: 0.5465850830078125\n",
      "  batch 90 loss: 0.618121337890625\n",
      "  batch 100 loss: 0.7358642578125\n",
      "  batch 110 loss: 0.540179443359375\n",
      "  batch 120 loss: 0.63465576171875\n",
      "  batch 130 loss: 0.4934539794921875\n",
      "  batch 140 loss: 0.586553955078125\n",
      "  batch 150 loss: 0.66492919921875\n",
      "  batch 160 loss: 0.6366851806640625\n",
      "EPOCH  9 val loss:  0.6725328233506944\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.71275634765625\n",
      "  batch 20 loss: 0.526434326171875\n",
      "  batch 30 loss: 0.6158477783203125\n",
      "  batch 40 loss: 0.6097076416015625\n",
      "  batch 50 loss: 0.50576171875\n",
      "  batch 60 loss: 0.5246917724609375\n",
      "  batch 70 loss: 0.5321014404296875\n",
      "  batch 80 loss: 0.5587158203125\n",
      "  batch 90 loss: 0.4946868896484375\n",
      "  batch 100 loss: 0.7238677978515625\n",
      "  batch 110 loss: 0.656207275390625\n",
      "  batch 120 loss: 0.6542236328125\n",
      "  batch 130 loss: 0.521295166015625\n",
      "  batch 140 loss: 0.7194915771484375\n",
      "  batch 150 loss: 0.5475677490234375\n",
      "  batch 160 loss: 0.4423370361328125\n",
      "EPOCH  10 val loss:  0.664520263671875\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.6313568115234375\n",
      "  batch 20 loss: 0.5329086303710937\n",
      "  batch 30 loss: 0.5764633178710937\n",
      "  batch 40 loss: 0.5026031494140625\n",
      "  batch 50 loss: 0.642578125\n",
      "  batch 60 loss: 0.651605224609375\n",
      "  batch 70 loss: 0.504150390625\n",
      "  batch 80 loss: 0.5282012939453125\n",
      "  batch 90 loss: 0.5988189697265625\n",
      "  batch 100 loss: 0.68341064453125\n",
      "  batch 110 loss: 0.769097900390625\n",
      "  batch 120 loss: 0.6147705078125\n",
      "  batch 130 loss: 0.61575927734375\n",
      "  batch 140 loss: 0.5394744873046875\n",
      "  batch 150 loss: 0.53133544921875\n",
      "  batch 160 loss: 0.566064453125\n",
      "EPOCH  1 val loss:  0.6271464029947916\n",
      "save model concat-tile-pooling-384-effv2-fold2.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.5296630859375\n",
      "  batch 20 loss: 0.5893280029296875\n",
      "  batch 30 loss: 0.5097259521484375\n",
      "  batch 40 loss: 0.6438690185546875\n",
      "  batch 50 loss: 0.6545166015625\n",
      "  batch 60 loss: 0.5166229248046875\n",
      "  batch 70 loss: 0.4700775146484375\n",
      "  batch 80 loss: 0.750311279296875\n",
      "  batch 90 loss: 0.687530517578125\n",
      "  batch 100 loss: 0.6996185302734375\n",
      "  batch 110 loss: 0.6926177978515625\n",
      "  batch 120 loss: 0.8493080139160156\n",
      "  batch 130 loss: 0.7148447513580323\n",
      "  batch 140 loss: 0.5883804321289062\n",
      "  batch 150 loss: 0.5709585189819336\n",
      "  batch 160 loss: 0.6302278518676758\n",
      "EPOCH  2 val loss:  0.8173918194240994\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.5750503540039062\n",
      "  batch 20 loss: 0.6391693115234375\n",
      "  batch 30 loss: 0.6399627685546875\n",
      "  batch 40 loss: 0.6812347412109375\n",
      "  batch 50 loss: 0.642742919921875\n",
      "  batch 60 loss: 0.587567138671875\n",
      "  batch 70 loss: 0.612921142578125\n",
      "  batch 80 loss: 0.580902099609375\n",
      "  batch 90 loss: 0.6850341796875\n",
      "  batch 100 loss: 0.5502655029296875\n",
      "  batch 110 loss: 0.5398208618164062\n",
      "  batch 120 loss: 0.5923797607421875\n",
      "  batch 130 loss: 0.589520263671875\n",
      "  batch 140 loss: 0.5157562255859375\n",
      "  batch 150 loss: 0.5640289306640625\n",
      "  batch 160 loss: 0.7214202880859375\n",
      "EPOCH  3 val loss:  0.6426374647352431\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.58199462890625\n",
      "  batch 20 loss: 0.5203521728515625\n",
      "  batch 30 loss: 0.66259765625\n",
      "  batch 40 loss: 0.5798446655273437\n",
      "  batch 50 loss: 0.5598495483398438\n",
      "  batch 60 loss: 0.7884071350097657\n",
      "  batch 70 loss: 0.7020980834960937\n",
      "  batch 80 loss: 0.6060867309570312\n",
      "  batch 90 loss: 0.7863037109375\n",
      "  batch 100 loss: 0.5749565124511719\n",
      "  batch 110 loss: 0.7420654296875\n",
      "  batch 120 loss: 0.709185791015625\n",
      "  batch 130 loss: 0.6161590576171875\n",
      "  batch 140 loss: 0.5480010986328125\n",
      "  batch 150 loss: 0.5832275390625\n",
      "  batch 160 loss: 0.6128646850585937\n",
      "EPOCH  4 val loss:  0.7065836588541666\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.6392913818359375\n",
      "  batch 20 loss: 0.58475341796875\n",
      "  batch 30 loss: 0.5893768310546875\n",
      "  batch 40 loss: 0.6343063354492188\n",
      "  batch 50 loss: 0.6278228759765625\n",
      "  batch 60 loss: 0.43840789794921875\n",
      "  batch 70 loss: 0.79229736328125\n",
      "  batch 80 loss: 0.6522186279296875\n",
      "  batch 90 loss: 0.647625732421875\n",
      "  batch 100 loss: 0.70291748046875\n",
      "  batch 110 loss: 0.639105224609375\n",
      "  batch 120 loss: 0.6002365112304687\n",
      "  batch 130 loss: 0.6015106201171875\n",
      "  batch 140 loss: 0.691656494140625\n",
      "  batch 150 loss: 0.65211181640625\n",
      "  batch 160 loss: 0.6969696044921875\n",
      "EPOCH  5 val loss:  0.6190219455295138\n",
      "save model concat-tile-pooling-384-effv2-fold2.pth\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.7210784912109375\n",
      "  batch 20 loss: 0.5543365478515625\n",
      "  batch 30 loss: 0.5158218383789063\n",
      "  batch 40 loss: 0.5716049194335937\n",
      "  batch 50 loss: 0.551629638671875\n",
      "  batch 60 loss: 0.5770599365234375\n",
      "  batch 70 loss: 0.53131103515625\n",
      "  batch 80 loss: 0.48627777099609376\n",
      "  batch 90 loss: 0.711920166015625\n",
      "  batch 100 loss: 0.7466644287109375\n",
      "  batch 110 loss: 0.611968994140625\n",
      "  batch 120 loss: 0.6474334716796875\n",
      "  batch 130 loss: 0.6100006103515625\n",
      "  batch 140 loss: 0.656903076171875\n",
      "  batch 150 loss: 0.6556427001953125\n",
      "  batch 160 loss: 0.568731689453125\n",
      "EPOCH  6 val loss:  0.6324683295355903\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.6472198486328125\n",
      "  batch 20 loss: 0.499609375\n",
      "  batch 30 loss: 0.6752914428710938\n",
      "  batch 40 loss: 0.5933746337890625\n",
      "  batch 50 loss: 0.45815277099609375\n",
      "  batch 60 loss: 0.61563720703125\n",
      "  batch 70 loss: 0.64417724609375\n",
      "  batch 80 loss: 0.607269287109375\n",
      "  batch 90 loss: 0.62264404296875\n",
      "  batch 100 loss: 0.658551025390625\n",
      "  batch 110 loss: 0.5396881103515625\n",
      "  batch 120 loss: 0.6416259765625\n",
      "  batch 130 loss: 0.6846282958984375\n",
      "  batch 140 loss: 0.6435577392578125\n",
      "  batch 150 loss: 0.588519287109375\n",
      "  batch 160 loss: 0.646484375\n",
      "EPOCH  7 val loss:  0.625823974609375\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.6347503662109375\n",
      "  batch 20 loss: 0.48577880859375\n",
      "  batch 30 loss: 0.640216064453125\n",
      "  batch 40 loss: 0.75640869140625\n",
      "  batch 50 loss: 0.6466766357421875\n",
      "  batch 60 loss: 0.56114501953125\n",
      "  batch 70 loss: 0.6559967041015625\n",
      "  batch 80 loss: 0.6509017944335938\n",
      "  batch 90 loss: 0.5182907104492187\n",
      "  batch 100 loss: 0.5445297241210938\n",
      "  batch 110 loss: 0.684210205078125\n",
      "  batch 120 loss: 0.6849868774414063\n",
      "  batch 130 loss: 0.700177001953125\n",
      "  batch 140 loss: 0.7005096435546875\n",
      "  batch 150 loss: 0.565887451171875\n",
      "  batch 160 loss: 0.62523193359375\n",
      "EPOCH  8 val loss:  1.0580800374348958\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.6483108520507812\n",
      "  batch 20 loss: 0.7603424072265625\n",
      "  batch 30 loss: 0.6149383544921875\n",
      "  batch 40 loss: 0.642193603515625\n",
      "  batch 50 loss: 0.4512420654296875\n",
      "  batch 60 loss: 0.455047607421875\n",
      "  batch 70 loss: 0.5241569519042969\n",
      "  batch 80 loss: 0.8220062255859375\n",
      "  batch 90 loss: 0.492694091796875\n",
      "  batch 100 loss: 0.624920654296875\n",
      "  batch 110 loss: 0.6115203857421875\n",
      "  batch 120 loss: 0.596478271484375\n",
      "  batch 130 loss: 0.539984130859375\n",
      "  batch 140 loss: 0.6203643798828125\n",
      "  batch 150 loss: 0.7202545166015625\n",
      "  batch 160 loss: 0.6568359375\n",
      "EPOCH  9 val loss:  0.6194241378042433\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.543536376953125\n",
      "  batch 20 loss: 0.684228515625\n",
      "  batch 30 loss: 0.496148681640625\n",
      "  batch 40 loss: 0.5474395751953125\n",
      "  batch 50 loss: 0.67779541015625\n",
      "  batch 60 loss: 0.661846923828125\n",
      "  batch 70 loss: 0.5934188842773438\n",
      "  batch 80 loss: 0.547149658203125\n",
      "  batch 90 loss: 0.6224090576171875\n",
      "  batch 100 loss: 0.652276611328125\n",
      "  batch 110 loss: 0.5933319091796875\n",
      "  batch 120 loss: 0.618768310546875\n",
      "  batch 130 loss: 0.578424072265625\n",
      "  batch 140 loss: 0.5974151611328125\n",
      "  batch 150 loss: 0.599945068359375\n",
      "  batch 160 loss: 0.655419921875\n",
      "EPOCH  10 val loss:  0.655029296875\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.70562744140625\n",
      "  batch 20 loss: 0.530914306640625\n",
      "  batch 30 loss: 0.5615325927734375\n",
      "  batch 40 loss: 0.5523910522460938\n",
      "  batch 50 loss: 0.680010986328125\n",
      "  batch 60 loss: 0.57554931640625\n",
      "  batch 70 loss: 0.658892822265625\n",
      "  batch 80 loss: 0.7541748046875\n",
      "  batch 90 loss: 0.659893798828125\n",
      "  batch 100 loss: 0.552667236328125\n",
      "  batch 110 loss: 0.58123779296875\n",
      "  batch 120 loss: 0.497412109375\n",
      "  batch 130 loss: 0.633599853515625\n",
      "  batch 140 loss: 0.687811279296875\n",
      "  batch 150 loss: 0.642352294921875\n",
      "  batch 160 loss: 0.536651611328125\n",
      "EPOCH  1 val loss:  0.5963524712456597\n",
      "save model concat-tile-pooling-384-effv2-fold3.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.6814056396484375\n",
      "  batch 20 loss: 0.577825927734375\n",
      "  batch 30 loss: 0.608349609375\n",
      "  batch 40 loss: 0.5130706787109375\n",
      "  batch 50 loss: 0.58865966796875\n",
      "  batch 60 loss: 0.6328125\n",
      "  batch 70 loss: 0.7246185302734375\n",
      "  batch 80 loss: 0.6237701416015625\n",
      "  batch 90 loss: 0.570758056640625\n",
      "  batch 100 loss: 0.574517822265625\n",
      "  batch 110 loss: 0.666705322265625\n",
      "  batch 120 loss: 0.652850341796875\n",
      "  batch 130 loss: 0.5347747802734375\n",
      "  batch 140 loss: 0.5690460205078125\n",
      "  batch 150 loss: 0.6371231079101562\n",
      "  batch 160 loss: 0.523028564453125\n",
      "EPOCH  2 val loss:  0.6076388888888888\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.76376953125\n",
      "  batch 20 loss: 0.6121307373046875\n",
      "  batch 30 loss: 0.5119384765625\n",
      "  batch 40 loss: 0.549737548828125\n",
      "  batch 50 loss: 0.709527587890625\n",
      "  batch 60 loss: 0.5290740966796875\n",
      "  batch 70 loss: 0.5906280517578125\n",
      "  batch 80 loss: 0.6323638916015625\n",
      "  batch 90 loss: 0.5671600341796875\n",
      "  batch 100 loss: 0.590472412109375\n",
      "  batch 110 loss: 0.46730194091796873\n",
      "  batch 120 loss: 0.719171142578125\n",
      "  batch 130 loss: 0.61951904296875\n",
      "  batch 140 loss: 0.6243438720703125\n",
      "  batch 150 loss: 0.5463104248046875\n",
      "  batch 160 loss: 0.5761138916015625\n",
      "EPOCH  3 val loss:  0.5969899495442709\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.65506591796875\n",
      "  batch 20 loss: 0.6576934814453125\n",
      "  batch 30 loss: 0.58819580078125\n",
      "  batch 40 loss: 0.588812255859375\n",
      "  batch 50 loss: 0.584295654296875\n",
      "  batch 60 loss: 0.6069976806640625\n",
      "  batch 70 loss: 0.5488189697265625\n",
      "  batch 80 loss: 0.68070068359375\n",
      "  batch 90 loss: 0.5023284912109375\n",
      "  batch 100 loss: 0.6853271484375\n",
      "  batch 110 loss: 0.707989501953125\n",
      "  batch 120 loss: 0.57213134765625\n",
      "  batch 130 loss: 0.541461181640625\n",
      "  batch 140 loss: 0.64639892578125\n",
      "  batch 150 loss: 0.510546875\n",
      "  batch 160 loss: 0.609759521484375\n",
      "EPOCH  4 val loss:  0.5997060139973959\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.4623382568359375\n",
      "  batch 20 loss: 0.535418701171875\n",
      "  batch 30 loss: 0.6706024169921875\n",
      "  batch 40 loss: 0.7415985107421875\n",
      "  batch 50 loss: 0.63204345703125\n",
      "  batch 60 loss: 0.65501708984375\n",
      "  batch 70 loss: 0.5973968505859375\n",
      "  batch 80 loss: 0.6817535400390625\n",
      "  batch 90 loss: 0.6461700439453125\n",
      "  batch 100 loss: 0.5797119140625\n",
      "  batch 110 loss: 0.5367340087890625\n",
      "  batch 120 loss: 0.5410247802734375\n",
      "  batch 130 loss: 0.5073562622070312\n",
      "  batch 140 loss: 0.689398193359375\n",
      "  batch 150 loss: 0.610064697265625\n",
      "  batch 160 loss: 0.624664306640625\n",
      "EPOCH  5 val loss:  0.5897996690538194\n",
      "save model concat-tile-pooling-384-effv2-fold3.pth\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.5958099365234375\n",
      "  batch 20 loss: 0.5824462890625\n",
      "  batch 30 loss: 0.68487548828125\n",
      "  batch 40 loss: 0.645989990234375\n",
      "  batch 50 loss: 0.5302947998046875\n",
      "  batch 60 loss: 0.5063690185546875\n",
      "  batch 70 loss: 0.629180908203125\n",
      "  batch 80 loss: 0.5538543701171875\n",
      "  batch 90 loss: 0.4853057861328125\n",
      "  batch 100 loss: 0.6279632568359375\n",
      "  batch 110 loss: 0.55731201171875\n",
      "  batch 120 loss: 0.6583343505859375\n",
      "  batch 130 loss: 0.6316162109375\n",
      "  batch 140 loss: 0.5615234375\n",
      "  batch 150 loss: 0.653363037109375\n",
      "  batch 160 loss: 0.6011550903320313\n",
      "EPOCH  6 val loss:  0.6117824978298612\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.600677490234375\n",
      "  batch 20 loss: 0.5773681640625\n",
      "  batch 30 loss: 0.6109375\n",
      "  batch 40 loss: 0.538482666015625\n",
      "  batch 50 loss: 0.6585784912109375\n",
      "  batch 60 loss: 0.626995849609375\n",
      "  batch 70 loss: 0.666064453125\n",
      "  batch 80 loss: 0.563714599609375\n",
      "  batch 90 loss: 0.5385772705078125\n",
      "  batch 100 loss: 0.4829681396484375\n",
      "  batch 110 loss: 0.5857208251953125\n",
      "  batch 120 loss: 0.6307525634765625\n",
      "  batch 130 loss: 0.6591766357421875\n",
      "  batch 140 loss: 0.637713623046875\n",
      "  batch 150 loss: 0.645892333984375\n",
      "  batch 160 loss: 0.582232666015625\n",
      "EPOCH  7 val loss:  0.5914543999565972\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.545648193359375\n",
      "  batch 20 loss: 0.4594268798828125\n",
      "  batch 30 loss: 0.6667999267578125\n",
      "  batch 40 loss: 0.6973297119140625\n",
      "  batch 50 loss: 0.55504150390625\n",
      "  batch 60 loss: 0.628765869140625\n",
      "  batch 70 loss: 0.5526824951171875\n",
      "  batch 80 loss: 0.63695068359375\n",
      "  batch 90 loss: 0.62437744140625\n",
      "  batch 100 loss: 0.5728668212890625\n",
      "  batch 110 loss: 0.668560791015625\n",
      "  batch 120 loss: 0.731817626953125\n",
      "  batch 130 loss: 0.6446868896484375\n",
      "  batch 140 loss: 0.637603759765625\n",
      "  batch 150 loss: 0.6000152587890625\n",
      "  batch 160 loss: 0.612811279296875\n",
      "EPOCH  8 val loss:  0.596099853515625\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.715618896484375\n",
      "  batch 20 loss: 0.62879638671875\n",
      "  batch 30 loss: 0.610931396484375\n",
      "  batch 40 loss: 0.5822906494140625\n",
      "  batch 50 loss: 0.5463043212890625\n",
      "  batch 60 loss: 0.5628662109375\n",
      "  batch 70 loss: 0.5508270263671875\n",
      "  batch 80 loss: 0.4518463134765625\n",
      "  batch 90 loss: 0.6998916625976562\n",
      "  batch 100 loss: 0.562884521484375\n",
      "  batch 110 loss: 0.4418670654296875\n",
      "  batch 120 loss: 0.7316253662109375\n",
      "  batch 130 loss: 0.6275634765625\n",
      "  batch 140 loss: 0.60435791015625\n",
      "  batch 150 loss: 0.6378082275390625\n",
      "  batch 160 loss: 0.4719879150390625\n",
      "EPOCH  9 val loss:  0.5883568657769097\n",
      "save model concat-tile-pooling-384-effv2-fold3.pth\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.5245391845703125\n",
      "  batch 20 loss: 0.6797210693359375\n",
      "  batch 30 loss: 0.6408538818359375\n",
      "  batch 40 loss: 0.58455810546875\n",
      "  batch 50 loss: 0.59388427734375\n",
      "  batch 60 loss: 0.601123046875\n",
      "  batch 70 loss: 0.534661865234375\n",
      "  batch 80 loss: 0.6674163818359375\n",
      "  batch 90 loss: 0.621868896484375\n",
      "  batch 100 loss: 0.6896484375\n",
      "  batch 110 loss: 0.610797119140625\n",
      "  batch 120 loss: 0.595013427734375\n",
      "  batch 130 loss: 0.5589599609375\n",
      "  batch 140 loss: 0.6040771484375\n",
      "  batch 150 loss: 0.4717529296875\n",
      "  batch 160 loss: 0.469573974609375\n",
      "EPOCH  10 val loss:  0.5978156195746528\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.5769256591796875\n",
      "  batch 20 loss: 0.540948486328125\n",
      "  batch 30 loss: 0.648663330078125\n",
      "  batch 40 loss: 0.49219970703125\n",
      "  batch 50 loss: 0.6936126708984375\n",
      "  batch 60 loss: 0.56982421875\n",
      "  batch 70 loss: 0.6418792724609375\n",
      "  batch 80 loss: 0.624652099609375\n",
      "  batch 90 loss: 0.627392578125\n",
      "  batch 100 loss: 0.600030517578125\n",
      "  batch 110 loss: 0.640704345703125\n",
      "  batch 120 loss: 0.587078857421875\n",
      "  batch 130 loss: 0.709454345703125\n",
      "  batch 140 loss: 0.56627197265625\n",
      "  batch 150 loss: 0.5159698486328125\n",
      "  batch 160 loss: 0.559033203125\n",
      "EPOCH  1 val loss:  0.5940013212316176\n",
      "save model concat-tile-pooling-384-effv2-fold4.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.7631927490234375\n",
      "  batch 20 loss: 0.562286376953125\n",
      "  batch 30 loss: 0.55279541015625\n",
      "  batch 40 loss: 0.5575439453125\n",
      "  batch 50 loss: 0.722845458984375\n",
      "  batch 60 loss: 0.614837646484375\n",
      "  batch 70 loss: 0.6535430908203125\n",
      "  batch 80 loss: 0.5645721435546875\n",
      "  batch 90 loss: 0.590435791015625\n",
      "  batch 100 loss: 0.5420654296875\n",
      "  batch 110 loss: 0.6003326416015625\n",
      "  batch 120 loss: 0.56153564453125\n",
      "  batch 130 loss: 0.451806640625\n",
      "  batch 140 loss: 0.707855224609375\n",
      "  batch 150 loss: 0.5176513671875\n",
      "  batch 160 loss: 0.604107666015625\n",
      "EPOCH  2 val loss:  0.6071669634650735\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.70665283203125\n",
      "  batch 20 loss: 0.545758056640625\n",
      "  batch 30 loss: 0.639019775390625\n",
      "  batch 40 loss: 0.64727783203125\n",
      "  batch 50 loss: 0.5294219970703125\n",
      "  batch 60 loss: 0.58743896484375\n",
      "  batch 70 loss: 0.5757293701171875\n",
      "  batch 80 loss: 0.488348388671875\n",
      "  batch 90 loss: 0.588958740234375\n",
      "  batch 100 loss: 0.682421875\n",
      "  batch 110 loss: 0.687982177734375\n",
      "  batch 120 loss: 0.55079345703125\n",
      "  batch 130 loss: 0.6221405029296875\n",
      "  batch 140 loss: 0.5583648681640625\n",
      "  batch 150 loss: 0.583721923828125\n",
      "  batch 160 loss: 0.678656005859375\n",
      "EPOCH  3 val loss:  0.5923856847426471\n",
      "save model concat-tile-pooling-384-effv2-fold4.pth\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.545550537109375\n",
      "  batch 20 loss: 0.6257720947265625\n",
      "  batch 30 loss: 0.6761688232421875\n",
      "  batch 40 loss: 0.7275146484375\n",
      "  batch 50 loss: 0.53145751953125\n",
      "  batch 60 loss: 0.5243865966796875\n",
      "  batch 70 loss: 0.6003570556640625\n",
      "  batch 80 loss: 0.5415557861328125\n",
      "  batch 90 loss: 0.782525634765625\n",
      "  batch 100 loss: 0.52933349609375\n",
      "  batch 110 loss: 0.49598236083984376\n",
      "  batch 120 loss: 0.7104415893554688\n",
      "  batch 130 loss: 0.455987548828125\n",
      "  batch 140 loss: 0.6104248046875\n",
      "  batch 150 loss: 0.5962310791015625\n",
      "  batch 160 loss: 0.63060302734375\n",
      "EPOCH  4 val loss:  0.6116368910845589\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.6453094482421875\n",
      "  batch 20 loss: 0.589617919921875\n",
      "  batch 30 loss: 0.50142822265625\n",
      "  batch 40 loss: 0.606781005859375\n",
      "  batch 50 loss: 0.517889404296875\n",
      "  batch 60 loss: 0.6327972412109375\n",
      "  batch 70 loss: 0.6430419921875\n",
      "  batch 80 loss: 0.7308349609375\n",
      "  batch 90 loss: 0.539837646484375\n",
      "  batch 100 loss: 0.618487548828125\n",
      "  batch 110 loss: 0.561541748046875\n",
      "  batch 120 loss: 0.429278564453125\n",
      "  batch 130 loss: 0.5954010009765625\n",
      "  batch 140 loss: 0.584649658203125\n",
      "  batch 150 loss: 0.6573516845703125\n",
      "  batch 160 loss: 0.67913818359375\n",
      "EPOCH  5 val loss:  0.6145953010110294\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.576019287109375\n",
      "  batch 20 loss: 0.631195068359375\n",
      "  batch 30 loss: 0.6913909912109375\n",
      "  batch 40 loss: 0.6101806640625\n",
      "  batch 50 loss: 0.659783935546875\n",
      "  batch 60 loss: 0.5156524658203125\n",
      "  batch 70 loss: 0.67890625\n",
      "  batch 80 loss: 0.6023681640625\n",
      "  batch 90 loss: 0.52847900390625\n",
      "  batch 100 loss: 0.5671295166015625\n",
      "  batch 110 loss: 0.6054443359375\n",
      "  batch 120 loss: 0.6885498046875\n",
      "  batch 130 loss: 0.569207763671875\n",
      "  batch 140 loss: 0.554931640625\n",
      "  batch 150 loss: 0.55921630859375\n",
      "  batch 160 loss: 0.5278228759765625\n",
      "EPOCH  6 val loss:  0.6435457117417279\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.5161529541015625\n",
      "  batch 20 loss: 0.5513015747070312\n",
      "  batch 30 loss: 0.629156494140625\n",
      "  batch 40 loss: 0.7093643188476563\n",
      "  batch 50 loss: 0.5272705078125\n",
      "  batch 60 loss: 0.53377685546875\n",
      "  batch 70 loss: 0.535821533203125\n",
      "  batch 80 loss: 0.52801513671875\n",
      "  batch 90 loss: 0.7273101806640625\n",
      "  batch 100 loss: 0.58355712890625\n",
      "  batch 110 loss: 0.694683837890625\n",
      "  batch 120 loss: 0.5827606201171875\n",
      "  batch 130 loss: 0.7405059814453125\n",
      "  batch 140 loss: 0.641082763671875\n",
      "  batch 150 loss: 0.55146484375\n",
      "  batch 160 loss: 0.6160552978515625\n",
      "EPOCH  7 val loss:  0.6162755629595589\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.61328125\n",
      "  batch 20 loss: 0.5645782470703125\n",
      "  batch 30 loss: 0.4632598876953125\n",
      "  batch 40 loss: 0.5866744995117188\n",
      "  batch 50 loss: 0.84786376953125\n",
      "  batch 60 loss: 0.572332763671875\n",
      "  batch 70 loss: 0.4734649658203125\n",
      "  batch 80 loss: 0.6331085205078125\n",
      "  batch 90 loss: 0.554107666015625\n",
      "  batch 100 loss: 0.5465087890625\n",
      "  batch 110 loss: 0.7120376586914062\n",
      "  batch 120 loss: 0.7069091796875\n",
      "  batch 130 loss: 0.65257568359375\n",
      "  batch 140 loss: 0.603656005859375\n",
      "  batch 150 loss: 0.579266357421875\n",
      "  batch 160 loss: 0.511590576171875\n",
      "EPOCH  8 val loss:  0.5857095157398897\n",
      "save model concat-tile-pooling-384-effv2-fold4.pth\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.6082000732421875\n",
      "  batch 20 loss: 0.493133544921875\n",
      "  batch 30 loss: 0.6376495361328125\n",
      "  batch 40 loss: 0.519244384765625\n",
      "  batch 50 loss: 0.74088134765625\n",
      "  batch 60 loss: 0.617236328125\n",
      "  batch 70 loss: 0.491802978515625\n",
      "  batch 80 loss: 0.812396240234375\n",
      "  batch 90 loss: 0.59876708984375\n",
      "  batch 100 loss: 0.607781982421875\n",
      "  batch 110 loss: 0.563946533203125\n",
      "  batch 120 loss: 0.6374786376953125\n",
      "  batch 130 loss: 0.6449493408203125\n",
      "  batch 140 loss: 0.55177001953125\n",
      "  batch 150 loss: 0.544158935546875\n",
      "  batch 160 loss: 0.5216888427734375\n",
      "EPOCH  9 val loss:  0.6064417221966911\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.6095458984375\n",
      "  batch 20 loss: 0.6771636962890625\n",
      "  batch 30 loss: 0.56240234375\n",
      "  batch 40 loss: 0.548736572265625\n",
      "  batch 50 loss: 0.58310546875\n",
      "  batch 60 loss: 0.510467529296875\n",
      "  batch 70 loss: 0.7157501220703125\n",
      "  batch 80 loss: 0.66063232421875\n",
      "  batch 90 loss: 0.413702392578125\n",
      "  batch 100 loss: 0.7218109130859375\n",
      "  batch 110 loss: 0.658697509765625\n",
      "  batch 120 loss: 0.573712158203125\n",
      "  batch 130 loss: 0.494439697265625\n",
      "  batch 140 loss: 0.735260009765625\n",
      "  batch 150 loss: 0.563134765625\n",
      "  batch 160 loss: 0.614404296875\n",
      "EPOCH  10 val loss:  0.5993795955882353\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.5473297119140625\n",
      "  batch 20 loss: 0.4416900634765625\n",
      "  batch 30 loss: 0.49712371826171875\n",
      "  batch 40 loss: 0.608807373046875\n",
      "  batch 50 loss: 0.589239501953125\n",
      "  batch 60 loss: 0.587689208984375\n",
      "  batch 70 loss: 0.657452392578125\n",
      "  batch 80 loss: 0.69259033203125\n",
      "  batch 90 loss: 0.55355224609375\n",
      "  batch 100 loss: 0.56715087890625\n",
      "  batch 110 loss: 0.7478271484375\n",
      "  batch 120 loss: 0.606121826171875\n",
      "  batch 130 loss: 0.6334075927734375\n",
      "  batch 140 loss: 0.65135498046875\n",
      "  batch 150 loss: 0.46981201171875\n",
      "  batch 160 loss: 0.602423095703125\n",
      "EPOCH  1 val loss:  0.6717996036305147\n",
      "save model concat-tile-pooling-384-effv2-fold5.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.6717315673828125\n",
      "  batch 20 loss: 0.5817626953125\n",
      "  batch 30 loss: 0.54649658203125\n",
      "  batch 40 loss: 0.6846435546875\n",
      "  batch 50 loss: 0.5117401123046875\n",
      "  batch 60 loss: 0.6098602294921875\n",
      "  batch 70 loss: 0.48585205078125\n",
      "  batch 80 loss: 0.574969482421875\n",
      "  batch 90 loss: 0.624822998046875\n",
      "  batch 100 loss: 0.5433624267578125\n",
      "  batch 110 loss: 0.5136505126953125\n",
      "  batch 120 loss: 0.679498291015625\n",
      "  batch 130 loss: 0.5959228515625\n",
      "  batch 140 loss: 0.572808837890625\n",
      "  batch 150 loss: 0.6211669921875\n",
      "  batch 160 loss: 0.56732177734375\n",
      "EPOCH  2 val loss:  0.6404490751378676\n",
      "save model concat-tile-pooling-384-effv2-fold5.pth\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.5671142578125\n",
      "  batch 20 loss: 0.5187469482421875\n",
      "  batch 30 loss: 0.5744720458984375\n",
      "  batch 40 loss: 0.568804931640625\n",
      "  batch 50 loss: 0.538787841796875\n",
      "  batch 60 loss: 0.6431976318359375\n",
      "  batch 70 loss: 0.6584259033203125\n",
      "  batch 80 loss: 0.61163330078125\n",
      "  batch 90 loss: 0.588983154296875\n",
      "  batch 100 loss: 0.54173583984375\n",
      "  batch 110 loss: 0.561651611328125\n",
      "  batch 120 loss: 0.682415771484375\n",
      "  batch 130 loss: 0.6361297607421875\n",
      "  batch 140 loss: 0.613897705078125\n",
      "  batch 150 loss: 0.653118896484375\n",
      "  batch 160 loss: 0.59146728515625\n",
      "EPOCH  3 val loss:  0.6425888959099265\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.563775634765625\n",
      "  batch 20 loss: 0.4491058349609375\n",
      "  batch 30 loss: 0.596026611328125\n",
      "  batch 40 loss: 0.66455078125\n",
      "  batch 50 loss: 0.54754638671875\n",
      "  batch 60 loss: 0.6826873779296875\n",
      "  batch 70 loss: 0.5862823486328125\n",
      "  batch 80 loss: 0.504022216796875\n",
      "  batch 90 loss: 0.67333984375\n",
      "  batch 100 loss: 0.49124755859375\n",
      "  batch 110 loss: 0.6052459716796875\n",
      "  batch 120 loss: 0.50938720703125\n",
      "  batch 130 loss: 0.585467529296875\n",
      "  batch 140 loss: 0.7343536376953125\n",
      "  batch 150 loss: 0.587078857421875\n",
      "  batch 160 loss: 0.7011474609375\n",
      "EPOCH  4 val loss:  0.6501895680147058\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.495977783203125\n",
      "  batch 20 loss: 0.656268310546875\n",
      "  batch 30 loss: 0.707159423828125\n",
      "  batch 40 loss: 0.5836669921875\n",
      "  batch 50 loss: 0.753167724609375\n",
      "  batch 60 loss: 0.59384765625\n",
      "  batch 70 loss: 0.5625732421875\n",
      "  batch 80 loss: 0.501171875\n",
      "  batch 90 loss: 0.6693572998046875\n",
      "  batch 100 loss: 0.52835693359375\n",
      "  batch 110 loss: 0.5135406494140625\n",
      "  batch 120 loss: 0.63795166015625\n",
      "  batch 130 loss: 0.5247894287109375\n",
      "  batch 140 loss: 0.582867431640625\n",
      "  batch 150 loss: 0.487579345703125\n",
      "  batch 160 loss: 0.711981201171875\n",
      "EPOCH  5 val loss:  0.6751493566176471\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.48922119140625\n",
      "  batch 20 loss: 0.6443817138671875\n",
      "  batch 30 loss: 0.45977783203125\n",
      "  batch 40 loss: 0.4821075439453125\n",
      "  batch 50 loss: 0.6659088134765625\n",
      "  batch 60 loss: 0.6625152587890625\n",
      "  batch 70 loss: 0.5887939453125\n",
      "  batch 80 loss: 0.5748611450195312\n",
      "  batch 90 loss: 0.698974609375\n",
      "  batch 100 loss: 0.59869384765625\n",
      "  batch 110 loss: 0.629144287109375\n",
      "  batch 120 loss: 0.6387115478515625\n",
      "  batch 130 loss: 0.51817626953125\n",
      "  batch 140 loss: 0.68177490234375\n",
      "  batch 150 loss: 0.6193359375\n",
      "  batch 160 loss: 0.655780029296875\n",
      "EPOCH  6 val loss:  0.6387939453125\n",
      "save model concat-tile-pooling-384-effv2-fold5.pth\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.642608642578125\n",
      "  batch 20 loss: 0.679046630859375\n",
      "  batch 30 loss: 0.6290283203125\n",
      "  batch 40 loss: 0.4961669921875\n",
      "  batch 50 loss: 0.5247344970703125\n",
      "  batch 60 loss: 0.5012664794921875\n",
      "  batch 70 loss: 0.6151336669921875\n",
      "  batch 80 loss: 0.541790771484375\n",
      "  batch 90 loss: 0.60577392578125\n",
      "  batch 100 loss: 0.5800262451171875\n",
      "  batch 110 loss: 0.736029052734375\n",
      "  batch 120 loss: 0.67108154296875\n",
      "  batch 130 loss: 0.59163818359375\n",
      "  batch 140 loss: 0.517315673828125\n",
      "  batch 150 loss: 0.433258056640625\n",
      "  batch 160 loss: 0.6288177490234375\n",
      "EPOCH  7 val loss:  0.6695736155790442\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.64903564453125\n",
      "  batch 20 loss: 0.569036865234375\n",
      "  batch 30 loss: 0.62352294921875\n",
      "  batch 40 loss: 0.652264404296875\n",
      "  batch 50 loss: 0.53988037109375\n",
      "  batch 60 loss: 0.5709503173828125\n",
      "  batch 70 loss: 0.6207611083984375\n",
      "  batch 80 loss: 0.581756591796875\n",
      "  batch 90 loss: 0.532342529296875\n",
      "  batch 100 loss: 0.4890380859375\n",
      "  batch 110 loss: 0.4836029052734375\n",
      "  batch 120 loss: 0.5996795654296875\n",
      "  batch 130 loss: 0.6237579345703125\n",
      "  batch 140 loss: 0.5745208740234375\n",
      "  batch 150 loss: 0.579150390625\n",
      "  batch 160 loss: 0.7216064453125\n",
      "EPOCH  8 val loss:  0.6389303768382353\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.6293701171875\n",
      "  batch 20 loss: 0.637908935546875\n",
      "  batch 30 loss: 0.464263916015625\n",
      "  batch 40 loss: 0.52498779296875\n",
      "  batch 50 loss: 0.60526123046875\n",
      "  batch 60 loss: 0.693756103515625\n",
      "  batch 70 loss: 0.539654541015625\n",
      "  batch 80 loss: 0.648638916015625\n",
      "  batch 90 loss: 0.582232666015625\n",
      "  batch 100 loss: 0.699407958984375\n",
      "  batch 110 loss: 0.622259521484375\n",
      "  batch 120 loss: 0.600592041015625\n",
      "  batch 130 loss: 0.559423828125\n",
      "  batch 140 loss: 0.595623779296875\n",
      "  batch 150 loss: 0.4322174072265625\n",
      "  batch 160 loss: 0.5880523681640625\n",
      "EPOCH  9 val loss:  0.6638327205882353\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.6410125732421875\n",
      "  batch 20 loss: 0.5065338134765625\n",
      "  batch 30 loss: 0.6895660400390625\n",
      "  batch 40 loss: 0.5049896240234375\n",
      "  batch 50 loss: 0.5756866455078125\n",
      "  batch 60 loss: 0.52811279296875\n",
      "  batch 70 loss: 0.5352020263671875\n",
      "  batch 80 loss: 0.753265380859375\n",
      "  batch 90 loss: 0.536419677734375\n",
      "  batch 100 loss: 0.56953125\n",
      "  batch 110 loss: 0.6120941162109375\n",
      "  batch 120 loss: 0.5047698974609375\n",
      "  batch 130 loss: 0.6595977783203125\n",
      "  batch 140 loss: 0.56119384765625\n",
      "  batch 150 loss: 0.6236541748046875\n",
      "  batch 160 loss: 0.57177734375\n",
      "EPOCH  10 val loss:  0.6482256721047794\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.59615478515625\n",
      "  batch 20 loss: 0.569854736328125\n",
      "  batch 30 loss: 0.657623291015625\n",
      "  batch 40 loss: 0.6315673828125\n",
      "  batch 50 loss: 0.5776123046875\n",
      "  batch 60 loss: 0.5107421875\n",
      "  batch 70 loss: 0.590826416015625\n",
      "  batch 80 loss: 0.7552490234375\n",
      "  batch 90 loss: 0.646759033203125\n",
      "  batch 100 loss: 0.528594970703125\n",
      "  batch 110 loss: 0.49127197265625\n",
      "  batch 120 loss: 0.7542266845703125\n",
      "  batch 130 loss: 0.619427490234375\n",
      "  batch 140 loss: 0.611065673828125\n",
      "  batch 150 loss: 0.539434814453125\n",
      "  batch 160 loss: 0.684686279296875\n",
      "EPOCH  1 val loss:  0.5363266888786765\n",
      "save model concat-tile-pooling-384-effv2-fold6.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.61409912109375\n",
      "  batch 20 loss: 0.5200439453125\n",
      "  batch 30 loss: 0.695062255859375\n",
      "  batch 40 loss: 0.54969482421875\n",
      "  batch 50 loss: 0.61719970703125\n",
      "  batch 60 loss: 0.588226318359375\n",
      "  batch 70 loss: 0.611480712890625\n",
      "  batch 80 loss: 0.666290283203125\n",
      "  batch 90 loss: 0.641943359375\n",
      "  batch 100 loss: 0.603961181640625\n",
      "  batch 110 loss: 0.614593505859375\n",
      "  batch 120 loss: 0.68812255859375\n",
      "  batch 130 loss: 0.549188232421875\n",
      "  batch 140 loss: 0.4572540283203125\n",
      "  batch 150 loss: 0.5853851318359375\n",
      "  batch 160 loss: 0.415777587890625\n",
      "EPOCH  2 val loss:  0.5412382238051471\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.665765380859375\n",
      "  batch 20 loss: 0.4686370849609375\n",
      "  batch 30 loss: 0.6910858154296875\n",
      "  batch 40 loss: 0.684844970703125\n",
      "  batch 50 loss: 0.59942626953125\n",
      "  batch 60 loss: 0.611962890625\n",
      "  batch 70 loss: 0.4886749267578125\n",
      "  batch 80 loss: 0.519342041015625\n",
      "  batch 90 loss: 0.724932861328125\n",
      "  batch 100 loss: 0.6878143310546875\n",
      "  batch 110 loss: 0.64814453125\n",
      "  batch 120 loss: 0.520111083984375\n",
      "  batch 130 loss: 0.621783447265625\n",
      "  batch 140 loss: 0.5166168212890625\n",
      "  batch 150 loss: 0.5912628173828125\n",
      "  batch 160 loss: 0.596856689453125\n",
      "EPOCH  3 val loss:  0.5572222541360294\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.621014404296875\n",
      "  batch 20 loss: 0.699493408203125\n",
      "  batch 30 loss: 0.61097412109375\n",
      "  batch 40 loss: 0.630145263671875\n",
      "  batch 50 loss: 0.60233154296875\n",
      "  batch 60 loss: 0.512506103515625\n",
      "  batch 70 loss: 0.585894775390625\n",
      "  batch 80 loss: 0.6089813232421875\n",
      "  batch 90 loss: 0.644378662109375\n",
      "  batch 100 loss: 0.598431396484375\n",
      "  batch 110 loss: 0.569488525390625\n",
      "  batch 120 loss: 0.5903564453125\n",
      "  batch 130 loss: 0.621905517578125\n",
      "  batch 140 loss: 0.4860076904296875\n",
      "  batch 150 loss: 0.5330474853515625\n",
      "  batch 160 loss: 0.61854248046875\n",
      "EPOCH  4 val loss:  0.5329966825597426\n",
      "save model concat-tile-pooling-384-effv2-fold6.pth\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.616314697265625\n",
      "  batch 20 loss: 0.5186187744140625\n",
      "  batch 30 loss: 0.71005859375\n",
      "  batch 40 loss: 0.6961181640625\n",
      "  batch 50 loss: 0.605938720703125\n",
      "  batch 60 loss: 0.518450927734375\n",
      "  batch 70 loss: 0.5927978515625\n",
      "  batch 80 loss: 0.6624786376953125\n",
      "  batch 90 loss: 0.68887939453125\n",
      "  batch 100 loss: 0.639080810546875\n",
      "  batch 110 loss: 0.588665771484375\n",
      "  batch 120 loss: 0.495367431640625\n",
      "  batch 130 loss: 0.5385101318359375\n",
      "  batch 140 loss: 0.6444183349609375\n",
      "  batch 150 loss: 0.5248046875\n",
      "  batch 160 loss: 0.5426277160644531\n",
      "EPOCH  5 val loss:  0.5469180836397058\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.684112548828125\n",
      "  batch 20 loss: 0.585565185546875\n",
      "  batch 30 loss: 0.51650390625\n",
      "  batch 40 loss: 0.5960418701171875\n",
      "  batch 50 loss: 0.5819976806640625\n",
      "  batch 60 loss: 0.61358642578125\n",
      "  batch 70 loss: 0.6036865234375\n",
      "  batch 80 loss: 0.6929901123046875\n",
      "  batch 90 loss: 0.545550537109375\n",
      "  batch 100 loss: 0.650250244140625\n",
      "  batch 110 loss: 0.584747314453125\n",
      "  batch 120 loss: 0.548046875\n",
      "  batch 130 loss: 0.5668914794921875\n",
      "  batch 140 loss: 0.5404876708984375\n",
      "  batch 150 loss: 0.67169189453125\n",
      "  batch 160 loss: 0.6639617919921875\n",
      "EPOCH  6 val loss:  0.5865263097426471\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.5517669677734375\n",
      "  batch 20 loss: 0.6903289794921875\n",
      "  batch 30 loss: 0.772802734375\n",
      "  batch 40 loss: 0.52828369140625\n",
      "  batch 50 loss: 0.5460479736328125\n",
      "  batch 60 loss: 0.4074798583984375\n",
      "  batch 70 loss: 0.6967620849609375\n",
      "  batch 80 loss: 0.6954925537109375\n",
      "  batch 90 loss: 0.6637939453125\n",
      "  batch 100 loss: 0.70340576171875\n",
      "  batch 110 loss: 0.605096435546875\n",
      "  batch 120 loss: 0.587127685546875\n",
      "  batch 130 loss: 0.62047119140625\n",
      "  batch 140 loss: 0.5945281982421875\n",
      "  batch 150 loss: 0.57886962890625\n",
      "  batch 160 loss: 0.6000579833984375\n",
      "EPOCH  7 val loss:  0.5849215458421146\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.582379150390625\n",
      "  batch 20 loss: 0.691204833984375\n",
      "  batch 30 loss: 0.5726409912109375\n",
      "  batch 40 loss: 0.553448486328125\n",
      "  batch 50 loss: 0.5059967041015625\n",
      "  batch 60 loss: 0.4871368408203125\n",
      "  batch 70 loss: 0.711737060546875\n",
      "  batch 80 loss: 0.585736083984375\n",
      "  batch 90 loss: 0.55994873046875\n",
      "  batch 100 loss: 0.482318115234375\n",
      "  batch 110 loss: 0.702532958984375\n",
      "  batch 120 loss: 0.7677001953125\n",
      "  batch 130 loss: 0.610589599609375\n",
      "  batch 140 loss: 0.593658447265625\n",
      "  batch 150 loss: 0.685382080078125\n",
      "  batch 160 loss: 0.673175048828125\n",
      "EPOCH  8 val loss:  0.6283425723805147\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.55279541015625\n",
      "  batch 20 loss: 0.65103759765625\n",
      "  batch 30 loss: 0.635321044921875\n",
      "  batch 40 loss: 0.716680908203125\n",
      "  batch 50 loss: 0.62047119140625\n",
      "  batch 60 loss: 0.642236328125\n",
      "  batch 70 loss: 0.609161376953125\n",
      "  batch 80 loss: 0.527130126953125\n",
      "  batch 90 loss: 0.5272369384765625\n",
      "  batch 100 loss: 0.7209686279296875\n",
      "  batch 110 loss: 0.614801025390625\n",
      "  batch 120 loss: 0.637298583984375\n",
      "  batch 130 loss: 0.611016845703125\n",
      "  batch 140 loss: 0.62506103515625\n",
      "  batch 150 loss: 0.51640625\n",
      "  batch 160 loss: 0.5490890502929687\n",
      "EPOCH  9 val loss:  0.8679271024816176\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.5387161254882813\n",
      "  batch 20 loss: 0.651788330078125\n",
      "  batch 30 loss: 0.6083938598632812\n",
      "  batch 40 loss: 0.38394012451171877\n",
      "  batch 50 loss: 0.892041015625\n",
      "  batch 60 loss: 0.635028076171875\n",
      "  batch 70 loss: 0.636041259765625\n",
      "  batch 80 loss: 0.61788330078125\n",
      "  batch 90 loss: 0.655889892578125\n",
      "  batch 100 loss: 0.6033935546875\n",
      "  batch 110 loss: 0.6395751953125\n",
      "  batch 120 loss: 0.6054443359375\n",
      "  batch 130 loss: 0.51478271484375\n",
      "  batch 140 loss: 0.6354705810546875\n",
      "  batch 150 loss: 0.598822021484375\n",
      "  batch 160 loss: 0.573956298828125\n",
      "EPOCH  10 val loss:  0.8055689194623161\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.60421142578125\n",
      "  batch 20 loss: 0.589959716796875\n",
      "  batch 30 loss: 0.5114837646484375\n",
      "  batch 40 loss: 0.5695343017578125\n",
      "  batch 50 loss: 0.446734619140625\n",
      "  batch 60 loss: 0.5725128173828125\n",
      "  batch 70 loss: 0.5837646484375\n",
      "  batch 80 loss: 0.51883544921875\n",
      "  batch 90 loss: 0.395538330078125\n",
      "  batch 100 loss: 0.870587158203125\n",
      "  batch 110 loss: 0.652996826171875\n",
      "  batch 120 loss: 0.626934814453125\n",
      "  batch 130 loss: 0.62431640625\n",
      "  batch 140 loss: 0.647137451171875\n",
      "  batch 150 loss: 0.632647705078125\n",
      "  batch 160 loss: 0.609686279296875\n",
      "EPOCH  1 val loss:  0.5986579446231618\n",
      "save model concat-tile-pooling-384-effv2-fold7.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.638037109375\n",
      "  batch 20 loss: 0.582867431640625\n",
      "  batch 30 loss: 0.606622314453125\n",
      "  batch 40 loss: 0.6937469482421875\n",
      "  batch 50 loss: 0.558056640625\n",
      "  batch 60 loss: 0.651434326171875\n",
      "  batch 70 loss: 0.531451416015625\n",
      "  batch 80 loss: 0.612310791015625\n",
      "  batch 90 loss: 0.63402099609375\n",
      "  batch 100 loss: 0.57725830078125\n",
      "  batch 110 loss: 0.6548095703125\n",
      "  batch 120 loss: 0.5416046142578125\n",
      "  batch 130 loss: 0.545501708984375\n",
      "  batch 140 loss: 0.5986785888671875\n",
      "  batch 150 loss: 0.482366943359375\n",
      "  batch 160 loss: 0.7826904296875\n",
      "EPOCH  2 val loss:  0.5883824965533089\n",
      "save model concat-tile-pooling-384-effv2-fold7.pth\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.5285552978515625\n",
      "  batch 20 loss: 0.51494140625\n",
      "  batch 30 loss: 0.6687042236328125\n",
      "  batch 40 loss: 0.6353912353515625\n",
      "  batch 50 loss: 0.593511962890625\n",
      "  batch 60 loss: 0.6004241943359375\n",
      "  batch 70 loss: 0.563677978515625\n",
      "  batch 80 loss: 0.590155029296875\n",
      "  batch 90 loss: 0.7101837158203125\n",
      "  batch 100 loss: 0.513623046875\n",
      "  batch 110 loss: 0.460272216796875\n",
      "  batch 120 loss: 0.6736419677734375\n",
      "  batch 130 loss: 0.4465911865234375\n",
      "  batch 140 loss: 0.7350738525390625\n",
      "  batch 150 loss: 0.690570068359375\n",
      "  batch 160 loss: 0.746539306640625\n",
      "EPOCH  3 val loss:  0.6290821748621324\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.6367431640625\n",
      "  batch 20 loss: 0.65579833984375\n",
      "  batch 30 loss: 0.52384033203125\n",
      "  batch 40 loss: 0.612969970703125\n",
      "  batch 50 loss: 0.73724365234375\n",
      "  batch 60 loss: 0.580987548828125\n",
      "  batch 70 loss: 0.475762939453125\n",
      "  batch 80 loss: 0.5420806884765625\n",
      "  batch 90 loss: 0.4374664306640625\n",
      "  batch 100 loss: 0.5136566162109375\n",
      "  batch 110 loss: 0.696746826171875\n",
      "  batch 120 loss: 0.5528045654296875\n",
      "  batch 130 loss: 0.639373779296875\n",
      "  batch 140 loss: 0.5875579833984375\n",
      "  batch 150 loss: 0.6470458984375\n",
      "  batch 160 loss: 0.682611083984375\n",
      "EPOCH  4 val loss:  0.5798546356313369\n",
      "save model concat-tile-pooling-384-effv2-fold7.pth\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.57762451171875\n",
      "  batch 20 loss: 0.517022705078125\n",
      "  batch 30 loss: 0.486383056640625\n",
      "  batch 40 loss: 0.626849365234375\n",
      "  batch 50 loss: 0.647119140625\n",
      "  batch 60 loss: 0.670538330078125\n",
      "  batch 70 loss: 0.69716796875\n",
      "  batch 80 loss: 0.6300537109375\n",
      "  batch 90 loss: 0.564508056640625\n",
      "  batch 100 loss: 0.66275634765625\n",
      "  batch 110 loss: 0.594464111328125\n",
      "  batch 120 loss: 0.554437255859375\n",
      "  batch 130 loss: 0.591290283203125\n",
      "  batch 140 loss: 0.715191650390625\n",
      "  batch 150 loss: 0.564642333984375\n",
      "  batch 160 loss: 0.583489990234375\n",
      "EPOCH  5 val loss:  0.5850955738740808\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.59061279296875\n",
      "  batch 20 loss: 0.648883056640625\n",
      "  batch 30 loss: 0.559185791015625\n",
      "  batch 40 loss: 0.702398681640625\n",
      "  batch 50 loss: 0.699774169921875\n",
      "  batch 60 loss: 0.5594970703125\n",
      "  batch 70 loss: 0.50426025390625\n",
      "  batch 80 loss: 0.4852203369140625\n",
      "  batch 90 loss: 0.666033935546875\n",
      "  batch 100 loss: 0.5623138427734375\n",
      "  batch 110 loss: 0.65594482421875\n",
      "  batch 120 loss: 0.655242919921875\n",
      "  batch 130 loss: 0.525775146484375\n",
      "  batch 140 loss: 0.5571258544921875\n",
      "  batch 150 loss: 0.5869537353515625\n",
      "  batch 160 loss: 0.6393524169921875\n",
      "EPOCH  6 val loss:  0.5671664967256433\n",
      "save model concat-tile-pooling-384-effv2-fold7.pth\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.5224822998046875\n",
      "  batch 20 loss: 0.5641510009765625\n",
      "  batch 30 loss: 0.59091796875\n",
      "  batch 40 loss: 0.569793701171875\n",
      "  batch 50 loss: 0.4394134521484375\n",
      "  batch 60 loss: 0.71199951171875\n",
      "  batch 70 loss: 0.7232177734375\n",
      "  batch 80 loss: 0.62117919921875\n",
      "  batch 90 loss: 0.578289794921875\n",
      "  batch 100 loss: 0.5746337890625\n",
      "  batch 110 loss: 0.594708251953125\n",
      "  batch 120 loss: 0.6390380859375\n",
      "  batch 130 loss: 0.63551025390625\n",
      "  batch 140 loss: 0.639862060546875\n",
      "  batch 150 loss: 0.602569580078125\n",
      "  batch 160 loss: 0.574981689453125\n",
      "EPOCH  7 val loss:  0.5794282801011029\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.5537689208984375\n",
      "  batch 20 loss: 0.604180908203125\n",
      "  batch 30 loss: 0.695465087890625\n",
      "  batch 40 loss: 0.621002197265625\n",
      "  batch 50 loss: 0.566583251953125\n",
      "  batch 60 loss: 0.699456787109375\n",
      "  batch 70 loss: 0.65513916015625\n",
      "  batch 80 loss: 0.567034912109375\n",
      "  batch 90 loss: 0.606768798828125\n",
      "  batch 100 loss: 0.53360595703125\n",
      "  batch 110 loss: 0.511407470703125\n",
      "  batch 120 loss: 0.597705078125\n",
      "  batch 130 loss: 0.6811767578125\n",
      "  batch 140 loss: 0.52364501953125\n",
      "  batch 150 loss: 0.5306488037109375\n",
      "  batch 160 loss: 0.48980712890625\n",
      "EPOCH  8 val loss:  0.5832156013039982\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.6352569580078125\n",
      "  batch 20 loss: 0.534234619140625\n",
      "  batch 30 loss: 0.664630126953125\n",
      "  batch 40 loss: 0.625445556640625\n",
      "  batch 50 loss: 0.654156494140625\n",
      "  batch 60 loss: 0.606976318359375\n",
      "  batch 70 loss: 0.507098388671875\n",
      "  batch 80 loss: 0.5391448974609375\n",
      "  batch 90 loss: 0.4786041259765625\n",
      "  batch 100 loss: 0.717791748046875\n",
      "  batch 110 loss: 0.530242919921875\n",
      "  batch 120 loss: 0.4858367919921875\n",
      "  batch 130 loss: 0.742425537109375\n",
      "  batch 140 loss: 0.675213623046875\n",
      "  batch 150 loss: 0.678759765625\n",
      "  batch 160 loss: 0.516943359375\n",
      "EPOCH  9 val loss:  0.57598876953125\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.660296630859375\n",
      "  batch 20 loss: 0.5684600830078125\n",
      "  batch 30 loss: 0.58465576171875\n",
      "  batch 40 loss: 0.732354736328125\n",
      "  batch 50 loss: 0.570562744140625\n",
      "  batch 60 loss: 0.5634521484375\n",
      "  batch 70 loss: 0.683148193359375\n",
      "  batch 80 loss: 0.5923828125\n",
      "  batch 90 loss: 0.54686279296875\n",
      "  batch 100 loss: 0.5912353515625\n",
      "  batch 110 loss: 0.657000732421875\n",
      "  batch 120 loss: 0.4423553466796875\n",
      "  batch 130 loss: 0.55194091796875\n",
      "  batch 140 loss: 0.635205078125\n",
      "  batch 150 loss: 0.666009521484375\n",
      "  batch 160 loss: 0.6432159423828125\n",
      "EPOCH  10 val loss:  0.6017545812270221\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.5488975524902344\n",
      "  batch 20 loss: 0.5722137451171875\n",
      "  batch 30 loss: 0.56561279296875\n",
      "  batch 40 loss: 0.5083480834960937\n",
      "  batch 50 loss: 0.6094955444335938\n",
      "  batch 60 loss: 0.6578689575195312\n",
      "  batch 70 loss: 0.6705886840820312\n",
      "  batch 80 loss: 0.677587890625\n",
      "  batch 90 loss: 0.63028564453125\n",
      "  batch 100 loss: 0.5557373046875\n",
      "  batch 110 loss: 0.5332611083984375\n",
      "  batch 120 loss: 0.687030029296875\n",
      "  batch 130 loss: 0.6045654296875\n",
      "  batch 140 loss: 0.621636962890625\n",
      "  batch 150 loss: 0.54376220703125\n",
      "  batch 160 loss: 0.7152679443359375\n",
      "EPOCH  1 val loss:  0.6395514993106618\n",
      "save model concat-tile-pooling-384-effv2-fold8.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.519708251953125\n",
      "  batch 20 loss: 0.7332763671875\n",
      "  batch 30 loss: 0.636370849609375\n",
      "  batch 40 loss: 0.59403076171875\n",
      "  batch 50 loss: 0.59761962890625\n",
      "  batch 60 loss: 0.61641845703125\n",
      "  batch 70 loss: 0.484130859375\n",
      "  batch 80 loss: 0.77733154296875\n",
      "  batch 90 loss: 0.5169708251953125\n",
      "  batch 100 loss: 0.5865966796875\n",
      "  batch 110 loss: 0.694921875\n",
      "  batch 120 loss: 0.5550201416015625\n",
      "  batch 130 loss: 0.5550445556640625\n",
      "  batch 140 loss: 0.6747344970703125\n",
      "  batch 150 loss: 0.50341796875\n",
      "  batch 160 loss: 0.522064208984375\n",
      "EPOCH  2 val loss:  0.719419142779182\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.63170166015625\n",
      "  batch 20 loss: 0.628253173828125\n",
      "  batch 30 loss: 0.55289306640625\n",
      "  batch 40 loss: 0.5667999267578125\n",
      "  batch 50 loss: 0.6337890625\n",
      "  batch 60 loss: 0.5456512451171875\n",
      "  batch 70 loss: 0.6027587890625\n",
      "  batch 80 loss: 0.5691558837890625\n",
      "  batch 90 loss: 0.637042236328125\n",
      "  batch 100 loss: 0.636920166015625\n",
      "  batch 110 loss: 0.61231689453125\n",
      "  batch 120 loss: 0.5715606689453125\n",
      "  batch 130 loss: 0.517724609375\n",
      "  batch 140 loss: 0.580804443359375\n",
      "  batch 150 loss: 0.6528717041015625\n",
      "  batch 160 loss: 0.553155517578125\n",
      "EPOCH  3 val loss:  0.5971787396599265\n",
      "save model concat-tile-pooling-384-effv2-fold8.pth\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.641021728515625\n",
      "  batch 20 loss: 0.617999267578125\n",
      "  batch 30 loss: 0.5746368408203125\n",
      "  batch 40 loss: 0.678009033203125\n",
      "  batch 50 loss: 0.632965087890625\n",
      "  batch 60 loss: 0.548297119140625\n",
      "  batch 70 loss: 0.603656005859375\n",
      "  batch 80 loss: 0.5450820922851562\n",
      "  batch 90 loss: 0.61226806640625\n",
      "  batch 100 loss: 0.555902099609375\n",
      "  batch 110 loss: 0.5763397216796875\n",
      "  batch 120 loss: 0.5295501708984375\n",
      "  batch 130 loss: 0.5565093994140625\n",
      "  batch 140 loss: 0.5726531982421875\n",
      "  batch 150 loss: 0.610382080078125\n",
      "  batch 160 loss: 0.5765380859375\n",
      "EPOCH  4 val loss:  0.6590971105238971\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.554534912109375\n",
      "  batch 20 loss: 0.4963592529296875\n",
      "  batch 30 loss: 0.5090240478515625\n",
      "  batch 40 loss: 0.561761474609375\n",
      "  batch 50 loss: 0.5572967529296875\n",
      "  batch 60 loss: 0.69547119140625\n",
      "  batch 70 loss: 0.597967529296875\n",
      "  batch 80 loss: 0.5175750732421875\n",
      "  batch 90 loss: 0.5437347412109375\n",
      "  batch 100 loss: 0.5415252685546875\n",
      "  batch 110 loss: 0.785113525390625\n",
      "  batch 120 loss: 0.573272705078125\n",
      "  batch 130 loss: 0.6470458984375\n",
      "  batch 140 loss: 0.5529541015625\n",
      "  batch 150 loss: 0.6422119140625\n",
      "  batch 160 loss: 0.703826904296875\n",
      "EPOCH  5 val loss:  0.6195499195772058\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.569854736328125\n",
      "  batch 20 loss: 0.510858154296875\n",
      "  batch 30 loss: 0.6223114013671875\n",
      "  batch 40 loss: 0.507135009765625\n",
      "  batch 50 loss: 0.675244140625\n",
      "  batch 60 loss: 0.660784912109375\n",
      "  batch 70 loss: 0.532366943359375\n",
      "  batch 80 loss: 0.6112213134765625\n",
      "  batch 90 loss: 0.531011962890625\n",
      "  batch 100 loss: 0.6796600341796875\n",
      "  batch 110 loss: 0.5669967651367187\n",
      "  batch 120 loss: 0.554339599609375\n",
      "  batch 130 loss: 0.7572784423828125\n",
      "  batch 140 loss: 0.62144775390625\n",
      "  batch 150 loss: 0.58919677734375\n",
      "  batch 160 loss: 0.55579833984375\n",
      "EPOCH  6 val loss:  0.7036491842830882\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.566180419921875\n",
      "  batch 20 loss: 0.6677154541015625\n",
      "  batch 30 loss: 0.7112213134765625\n",
      "  batch 40 loss: 0.679425048828125\n",
      "  batch 50 loss: 0.562884521484375\n",
      "  batch 60 loss: 0.5962554931640625\n",
      "  batch 70 loss: 0.652716064453125\n",
      "  batch 80 loss: 0.5869384765625\n",
      "  batch 90 loss: 0.612322998046875\n",
      "  batch 100 loss: 0.611474609375\n",
      "  batch 110 loss: 0.620391845703125\n",
      "  batch 120 loss: 0.5951171875\n",
      "  batch 130 loss: 0.49688720703125\n",
      "  batch 140 loss: 0.6027557373046875\n",
      "  batch 150 loss: 0.518365478515625\n",
      "  batch 160 loss: 0.4971405029296875\n",
      "EPOCH  7 val loss:  0.705021577722886\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.6540374755859375\n",
      "  batch 20 loss: 0.585797119140625\n",
      "  batch 30 loss: 0.4600921630859375\n",
      "  batch 40 loss: 0.614776611328125\n",
      "  batch 50 loss: 0.611700439453125\n",
      "  batch 60 loss: 0.588720703125\n",
      "  batch 70 loss: 0.5972412109375\n",
      "  batch 80 loss: 0.68050537109375\n",
      "  batch 90 loss: 0.5597412109375\n",
      "  batch 100 loss: 0.50968017578125\n",
      "  batch 110 loss: 0.6433502197265625\n",
      "  batch 120 loss: 0.55875244140625\n",
      "  batch 130 loss: 0.6558563232421875\n",
      "  batch 140 loss: 0.550640869140625\n",
      "  batch 150 loss: 0.555352783203125\n",
      "  batch 160 loss: 0.649835205078125\n",
      "EPOCH  8 val loss:  0.6257180606617647\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.616046142578125\n",
      "  batch 20 loss: 0.599505615234375\n",
      "  batch 30 loss: 0.58074951171875\n",
      "  batch 40 loss: 0.601947021484375\n",
      "  batch 50 loss: 0.5794830322265625\n",
      "  batch 60 loss: 0.5766510009765625\n",
      "  batch 70 loss: 0.594903564453125\n",
      "  batch 80 loss: 0.612591552734375\n",
      "  batch 90 loss: 0.5647308349609375\n",
      "  batch 100 loss: 0.51160888671875\n",
      "  batch 110 loss: 0.567144775390625\n",
      "  batch 120 loss: 0.6102447509765625\n",
      "  batch 130 loss: 0.4622650146484375\n",
      "  batch 140 loss: 0.7430389404296875\n",
      "  batch 150 loss: 0.669830322265625\n",
      "  batch 160 loss: 0.60096435546875\n",
      "EPOCH  9 val loss:  0.6447511560776654\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.559991455078125\n",
      "  batch 20 loss: 0.59017333984375\n",
      "  batch 30 loss: 0.6329559326171875\n",
      "  batch 40 loss: 0.77679443359375\n",
      "  batch 50 loss: 0.5595947265625\n",
      "  batch 60 loss: 0.707135009765625\n",
      "  batch 70 loss: 0.584814453125\n",
      "  batch 80 loss: 0.4608123779296875\n",
      "  batch 90 loss: 0.6280014038085937\n",
      "  batch 100 loss: 0.46902923583984374\n",
      "  batch 110 loss: 0.6205718994140625\n",
      "  batch 120 loss: 0.55966796875\n",
      "  batch 130 loss: 0.5921722412109375\n",
      "  batch 140 loss: 0.5249359130859375\n",
      "  batch 150 loss: 0.684490966796875\n",
      "  batch 160 loss: 0.558544921875\n",
      "EPOCH  10 val loss:  0.6580954159007353\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 0.6178375244140625\n",
      "  batch 20 loss: 0.62496337890625\n",
      "  batch 30 loss: 0.6992355346679687\n",
      "  batch 40 loss: 0.49805908203125\n",
      "  batch 50 loss: 0.566632080078125\n",
      "  batch 60 loss: 0.5463623046875\n",
      "  batch 70 loss: 0.4412506103515625\n",
      "  batch 80 loss: 0.5786895751953125\n",
      "  batch 90 loss: 0.6817047119140625\n",
      "  batch 100 loss: 0.558026123046875\n",
      "  batch 110 loss: 0.55052490234375\n",
      "  batch 120 loss: 0.603826904296875\n",
      "  batch 130 loss: 0.66341552734375\n",
      "  batch 140 loss: 0.62806396484375\n",
      "  batch 150 loss: 0.597601318359375\n",
      "  batch 160 loss: 0.624114990234375\n",
      "EPOCH  1 val loss:  0.6915355009191176\n",
      "save model concat-tile-pooling-384-effv2-fold9.pth\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.68150634765625\n",
      "  batch 20 loss: 0.5118438720703125\n",
      "  batch 30 loss: 0.532745361328125\n",
      "  batch 40 loss: 0.6299560546875\n",
      "  batch 50 loss: 0.6130035400390625\n",
      "  batch 60 loss: 0.6123046875\n",
      "  batch 70 loss: 0.506475830078125\n",
      "  batch 80 loss: 0.55870361328125\n",
      "  batch 90 loss: 0.7423431396484375\n",
      "  batch 100 loss: 0.5553955078125\n",
      "  batch 110 loss: 0.578912353515625\n",
      "  batch 120 loss: 0.6221221923828125\n",
      "  batch 130 loss: 0.613818359375\n",
      "  batch 140 loss: 0.5321380615234375\n",
      "  batch 150 loss: 0.586383056640625\n",
      "  batch 160 loss: 0.630242919921875\n",
      "EPOCH  2 val loss:  0.6880349551930147\n",
      "save model concat-tile-pooling-384-effv2-fold9.pth\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.5075286865234375\n",
      "  batch 20 loss: 0.713690185546875\n",
      "  batch 30 loss: 0.6804443359375\n",
      "  batch 40 loss: 0.656622314453125\n",
      "  batch 50 loss: 0.577264404296875\n",
      "  batch 60 loss: 0.61995849609375\n",
      "  batch 70 loss: 0.624871826171875\n",
      "  batch 80 loss: 0.606573486328125\n",
      "  batch 90 loss: 0.6383056640625\n",
      "  batch 100 loss: 0.5465576171875\n",
      "  batch 110 loss: 0.62529296875\n",
      "  batch 120 loss: 0.4594573974609375\n",
      "  batch 130 loss: 0.57119140625\n",
      "  batch 140 loss: 0.54735107421875\n",
      "  batch 150 loss: 0.60604248046875\n",
      "  batch 160 loss: 0.5353515625\n",
      "EPOCH  3 val loss:  0.7034912109375\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.614434814453125\n",
      "  batch 20 loss: 0.6002227783203125\n",
      "  batch 30 loss: 0.502996826171875\n",
      "  batch 40 loss: 0.4444091796875\n",
      "  batch 50 loss: 0.535748291015625\n",
      "  batch 60 loss: 0.648468017578125\n",
      "  batch 70 loss: 0.7354339599609375\n",
      "  batch 80 loss: 0.393017578125\n",
      "  batch 90 loss: 0.7328857421875\n",
      "  batch 100 loss: 0.592181396484375\n",
      "  batch 110 loss: 0.658099365234375\n",
      "  batch 120 loss: 0.65447998046875\n",
      "  batch 130 loss: 0.63397216796875\n",
      "  batch 140 loss: 0.564239501953125\n",
      "  batch 150 loss: 0.524114990234375\n",
      "  batch 160 loss: 0.5327362060546875\n",
      "EPOCH  4 val loss:  0.6931224149816176\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.6800079345703125\n",
      "  batch 20 loss: 0.5445343017578125\n",
      "  batch 30 loss: 0.630572509765625\n",
      "  batch 40 loss: 0.646539306640625\n",
      "  batch 50 loss: 0.59117431640625\n",
      "  batch 60 loss: 0.552197265625\n",
      "  batch 70 loss: 0.549285888671875\n",
      "  batch 80 loss: 0.7\n",
      "  batch 90 loss: 0.508624267578125\n",
      "  batch 100 loss: 0.5696044921875\n",
      "  batch 110 loss: 0.5749664306640625\n",
      "  batch 120 loss: 0.5054107666015625\n",
      "  batch 130 loss: 0.4762298583984375\n",
      "  batch 140 loss: 0.712615966796875\n",
      "  batch 150 loss: 0.581085205078125\n",
      "  batch 160 loss: 0.594183349609375\n",
      "EPOCH  5 val loss:  0.6818093692555147\n",
      "save model concat-tile-pooling-384-effv2-fold9.pth\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.64207763671875\n",
      "  batch 20 loss: 0.52215576171875\n",
      "  batch 30 loss: 0.6026641845703125\n",
      "  batch 40 loss: 0.5576416015625\n",
      "  batch 50 loss: 0.6268157958984375\n",
      "  batch 60 loss: 0.5039703369140625\n",
      "  batch 70 loss: 0.4426666259765625\n",
      "  batch 80 loss: 0.7342559814453125\n",
      "  batch 90 loss: 0.6034210205078125\n",
      "  batch 100 loss: 0.591839599609375\n",
      "  batch 110 loss: 0.5880462646484375\n",
      "  batch 120 loss: 0.448870849609375\n",
      "  batch 130 loss: 0.5326568603515625\n",
      "  batch 140 loss: 0.6216217041015625\n",
      "  batch 150 loss: 0.6866973876953125\n",
      "  batch 160 loss: 0.66839599609375\n",
      "EPOCH  6 val loss:  0.6751601275275735\n",
      "save model concat-tile-pooling-384-effv2-fold9.pth\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.56558837890625\n",
      "  batch 20 loss: 0.635601806640625\n",
      "  batch 30 loss: 0.598687744140625\n",
      "  batch 40 loss: 0.6547760009765625\n",
      "  batch 50 loss: 0.56204833984375\n",
      "  batch 60 loss: 0.51175537109375\n",
      "  batch 70 loss: 0.63714599609375\n",
      "  batch 80 loss: 0.594061279296875\n",
      "  batch 90 loss: 0.5340484619140625\n",
      "  batch 100 loss: 0.583392333984375\n",
      "  batch 110 loss: 0.6072174072265625\n",
      "  batch 120 loss: 0.5777191162109375\n",
      "  batch 130 loss: 0.563946533203125\n",
      "  batch 140 loss: 0.54246826171875\n",
      "  batch 150 loss: 0.5733367919921875\n",
      "  batch 160 loss: 0.5103546142578125\n",
      "EPOCH  7 val loss:  0.6678538602941176\n",
      "save model concat-tile-pooling-384-effv2-fold9.pth\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.57076416015625\n",
      "  batch 20 loss: 0.5505706787109375\n",
      "  batch 30 loss: 0.676214599609375\n",
      "  batch 40 loss: 0.6104827880859375\n",
      "  batch 50 loss: 0.56356201171875\n",
      "  batch 60 loss: 0.619427490234375\n",
      "  batch 70 loss: 0.5544097900390625\n",
      "  batch 80 loss: 0.5457489013671875\n",
      "  batch 90 loss: 0.6161407470703125\n",
      "  batch 100 loss: 0.6013916015625\n",
      "  batch 110 loss: 0.6059234619140625\n",
      "  batch 120 loss: 0.47174072265625\n",
      "  batch 130 loss: 0.5353607177734375\n",
      "  batch 140 loss: 0.6860748291015625\n",
      "  batch 150 loss: 0.5670318603515625\n",
      "  batch 160 loss: 0.52371826171875\n",
      "EPOCH  8 val loss:  0.6740884219898897\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.4903472900390625\n",
      "  batch 20 loss: 0.6097381591796875\n",
      "  batch 30 loss: 0.61917724609375\n",
      "  batch 40 loss: 0.6984222412109375\n",
      "  batch 50 loss: 0.572808837890625\n",
      "  batch 60 loss: 0.5263275146484375\n",
      "  batch 70 loss: 0.595135498046875\n",
      "  batch 80 loss: 0.66343994140625\n",
      "  batch 90 loss: 0.525762939453125\n",
      "  batch 100 loss: 0.5202301025390625\n",
      "  batch 110 loss: 0.5334197998046875\n",
      "  batch 120 loss: 0.609576416015625\n",
      "  batch 130 loss: 0.5423980712890625\n",
      "  batch 140 loss: 0.4577178955078125\n",
      "  batch 150 loss: 0.73277587890625\n",
      "  batch 160 loss: 0.6238311767578125\n",
      "EPOCH  9 val loss:  0.6744133444393382\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.62899169921875\n",
      "  batch 20 loss: 0.62784423828125\n",
      "  batch 30 loss: 0.64541015625\n",
      "  batch 40 loss: 0.5401824951171875\n",
      "  batch 50 loss: 0.506951904296875\n",
      "  batch 60 loss: 0.6524139404296875\n",
      "  batch 70 loss: 0.583209228515625\n",
      "  batch 80 loss: 0.5906829833984375\n",
      "  batch 90 loss: 0.50762939453125\n",
      "  batch 100 loss: 0.5847900390625\n",
      "  batch 110 loss: 0.54573974609375\n",
      "  batch 120 loss: 0.4905487060546875\n",
      "  batch 130 loss: 0.6417022705078125\n",
      "  batch 140 loss: 0.557916259765625\n",
      "  batch 150 loss: 0.6137542724609375\n",
      "  batch 160 loss: 0.748480224609375\n",
      "EPOCH  10 val loss:  0.6915247300091911\n"
     ]
    }
   ],
   "source": [
    "for fold in range(nfolds):\n",
    "    training_set = TrainDataset(None,train[train[\"split\"]!=fold], transform=transform, aug=True)\n",
    "    training_loader = torch.utils.data.DataLoader(training_set, shuffle=True, num_workers=2, batch_size=bs, drop_last=True)\n",
    "    val_set = TrainDataset(None,train[train[\"split\"]==fold], transform=transform_val, aug=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, shuffle=True, num_workers=2, batch_size=bs, drop_last=True)\n",
    "\n",
    "    epoch_number=0\n",
    "    avg_loss = [999,]\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        epoch_loss = train_one_epoch(model=model, device=device, train_loader=training_loader, val_loader=val_loader, optimizer=optimizer, scheduler=scheduler, epoch=epoch_number, loss_fn=loss_fn)\n",
    "        epoch_number += 1\n",
    "        print(\"EPOCH \", str(epoch+1), \"val loss: \", epoch_loss)\n",
    "        \n",
    "        if min(avg_loss) > epoch_loss:\n",
    "            print(\"save model concat-tile-pooling-384-effv2-fold{}.pth\".format(str(fold)))\n",
    "            torch.save(model.state_dict(), \"concat-tile-pooling-384-effv2-fold{}.pth\".format(str(fold)))\n",
    "        \n",
    "        avg_loss.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d1125a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T18:47:58.147312Z",
     "iopub.status.busy": "2022-09-26T18:47:58.146945Z",
     "iopub.status.idle": "2022-09-26T18:47:58.151315Z",
     "shell.execute_reply": "2022-09-26T18:47:58.150332Z"
    },
    "papermill": {
     "duration": 0.096268,
     "end_time": "2022-09-26T18:47:58.153429",
     "exception": false,
     "start_time": "2022-09-26T18:47:58.057161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for input, label in training_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b9feeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T18:47:58.331316Z",
     "iopub.status.busy": "2022-09-26T18:47:58.330991Z",
     "iopub.status.idle": "2022-09-26T18:47:58.336265Z",
     "shell.execute_reply": "2022-09-26T18:47:58.335442Z"
    },
    "papermill": {
     "duration": 0.096221,
     "end_time": "2022-09-26T18:47:58.338389",
     "exception": false,
     "start_time": "2022-09-26T18:47:58.242168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bcef4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T18:47:58.517787Z",
     "iopub.status.busy": "2022-09-26T18:47:58.517395Z",
     "iopub.status.idle": "2022-09-26T18:47:58.521756Z",
     "shell.execute_reply": "2022-09-26T18:47:58.520674Z"
    },
    "papermill": {
     "duration": 0.098905,
     "end_time": "2022-09-26T18:47:58.524375",
     "exception": false,
     "start_time": "2022-09-26T18:47:58.425470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"concat-tile-pooling-model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8acbe6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T18:47:58.719493Z",
     "iopub.status.busy": "2022-09-26T18:47:58.718497Z",
     "iopub.status.idle": "2022-09-26T18:48:00.702540Z",
     "shell.execute_reply": "2022-09-26T18:48:00.701219Z"
    },
    "papermill": {
     "duration": 2.086444,
     "end_time": "2022-09-26T18:48:00.705094",
     "exception": false,
     "start_time": "2022-09-26T18:47:58.618650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/models\n",
    "!mv /kaggle/working/*.pth /kaggle/working/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68644d9",
   "metadata": {
    "papermill": {
     "duration": 0.086751,
     "end_time": "2022-09-26T18:48:00.881843",
     "exception": false,
     "start_time": "2022-09-26T18:48:00.795092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a5ed3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T18:48:01.058352Z",
     "iopub.status.busy": "2022-09-26T18:48:01.057147Z",
     "iopub.status.idle": "2022-09-26T18:48:03.032312Z",
     "shell.execute_reply": "2022-09-26T18:48:03.031160Z"
    },
    "papermill": {
     "duration": 2.066254,
     "end_time": "2022-09-26T18:48:03.035231",
     "exception": false,
     "start_time": "2022-09-26T18:48:00.968977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\r\n",
      "/kaggle/working/models/\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold9.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold8.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold5.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold4.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold3.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold6.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold1.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold0.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold7.pth\r\n",
      "/kaggle/working/models/concat-tile-pooling-384-effv2-fold2.pth\r\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf /kaggle/working/models.tar /kaggle/working/models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21176.81738,
   "end_time": "2022-09-26T18:48:04.985444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-26T12:55:08.168064",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
