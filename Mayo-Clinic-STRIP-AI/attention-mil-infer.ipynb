{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e7ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:11.412079Z",
     "iopub.status.busy": "2022-09-27T10:01:11.411005Z",
     "iopub.status.idle": "2022-09-27T10:01:11.421845Z",
     "shell.execute_reply": "2022-09-27T10:01:11.421002Z"
    },
    "papermill": {
     "duration": 0.01904,
     "end_time": "2022-09-27T10:01:11.423947",
     "exception": false,
     "start_time": "2022-09-27T10:01:11.404907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a612dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:11.432403Z",
     "iopub.status.busy": "2022-09-27T10:01:11.432142Z",
     "iopub.status.idle": "2022-09-27T10:01:46.976275Z",
     "shell.execute_reply": "2022-09-27T10:01:46.974970Z"
    },
    "papermill": {
     "duration": 35.551142,
     "end_time": "2022-09-27T10:01:46.978917",
     "exception": false,
     "start_time": "2022-09-27T10:01:11.427775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n"
     ]
    }
   ],
   "source": [
    "# !conda install -y --channel conda-forge pyvips\n",
    "!conda install ../input/pyvips-install/pyvips/*.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a84a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:46.996491Z",
     "iopub.status.busy": "2022-09-27T10:01:46.996128Z",
     "iopub.status.idle": "2022-09-27T10:01:47.333318Z",
     "shell.execute_reply": "2022-09-27T10:01:47.332318Z"
    },
    "papermill": {
     "duration": 0.348403,
     "end_time": "2022-09-27T10:01:47.335548",
     "exception": false,
     "start_time": "2022-09-27T10:01:46.987145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from openslide import OpenSlide\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a921118a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:47.352832Z",
     "iopub.status.busy": "2022-09-27T10:01:47.352185Z",
     "iopub.status.idle": "2022-09-27T10:01:47.360232Z",
     "shell.execute_reply": "2022-09-27T10:01:47.359383Z"
    },
    "papermill": {
     "duration": 0.018439,
     "end_time": "2022-09-27T10:01:47.362138",
     "exception": false,
     "start_time": "2022-09-27T10:01:47.343699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# test_image_paths = glob.glob(\"../input/mayo-clinic-strip-ai/train/*.tif\")\n",
    "test_image_paths = glob.glob(\"../input/mayo-clinic-strip-ai/test/*.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b932991a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:47.378812Z",
     "iopub.status.busy": "2022-09-27T10:01:47.378221Z",
     "iopub.status.idle": "2022-09-27T10:01:47.383138Z",
     "shell.execute_reply": "2022-09-27T10:01:47.382329Z"
    },
    "papermill": {
     "duration": 0.015318,
     "end_time": "2022-09-27T10:01:47.385058",
     "exception": false,
     "start_time": "2022-09-27T10:01:47.369740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale = 4\n",
    "# output_dir = \"/kaggle/working/\"\n",
    "# too_big_for_process = []\n",
    "# # for path in test_image_paths[180:200]:\n",
    "# for path in test_image_paths:\n",
    "\n",
    "#     print(path)\n",
    "#     slide = OpenSlide(path)\n",
    "\n",
    "#     if slide.dimensions[0]*slide.dimensions[1] < 4131662535:\n",
    "#         image_id = os.path.splitext(os.path.basename(path))[0]\n",
    "#         image = tiff.imread(path)\n",
    "#         print(f\"{image.shape}\")\n",
    "#         cv2.imwrite(os.path.join(output_dir, f\"{image_id}.jpg\"), image[::scale,::scale,::-1])\n",
    "#         del image\n",
    "#         gc.collect()\n",
    "#     else:\n",
    "#         print(\"Skip process for avoiding OOM possibility.\")\n",
    "#         too_big_for_process.append(path)\n",
    "\n",
    "# test_image_paths = glob.glob(\"/kaggle/working/*.jpg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23eb793b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:47.401704Z",
     "iopub.status.busy": "2022-09-27T10:01:47.401432Z",
     "iopub.status.idle": "2022-09-27T10:01:51.398410Z",
     "shell.execute_reply": "2022-09-27T10:01:51.397124Z"
    },
    "papermill": {
     "duration": 4.008029,
     "end_time": "2022-09-27T10:01:51.400851",
     "exception": false,
     "start_time": "2022-09-27T10:01:47.392822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torchvision\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import pyvips\n",
    "import scipy.stats\n",
    "import random\n",
    "from fastai.vision import *\n",
    "from fastai.layers import AdaptiveConcatPool2d, Flatten, Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe42c9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:51.417968Z",
     "iopub.status.busy": "2022-09-27T10:01:51.417455Z",
     "iopub.status.idle": "2022-09-27T10:01:51.429599Z",
     "shell.execute_reply": "2022-09-27T10:01:51.428665Z"
    },
    "papermill": {
     "duration": 0.022915,
     "end_time": "2022-09-27T10:01:51.431581",
     "exception": false,
     "start_time": "2022-09-27T10:01:51.408666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionSoftMax(torch.nn.Module):\n",
    "    def __init__(self, in_features=3, out_features = None):\n",
    "        super(AttentionSoftMax, self).__init__()\n",
    "        self.otherdim = 'b'\n",
    "        if out_features is None:\n",
    "            out_features = in_features\n",
    "\n",
    "        self.layer_linear_tr = nn.Linear(in_features, out_features) \n",
    "        self.activation = nn.LeakyReLU() \n",
    "        self.layer_linear_query= nn.Linear(out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.layer_linear_tr(x)\n",
    "        keys = self.activation(keys)\n",
    "\n",
    "        attention_map_raw = self.layer_linear_query(keys)[...,0]\n",
    "        attention_map = nn.Softmax(dim=-1)(attention_map_raw)\n",
    "\n",
    "        result = torch.einsum(f'{self.otherdim}i, {self.otherdim}ij->{self.otherdim}j', attention_map, x)\n",
    "        return result\n",
    "\n",
    "class Model (nn.Module):\n",
    "    def __init__(self, arch='tf_efficientnetv2_s', n=2, pre=False, enc_out_feat=1280): \n",
    "        super().__init__()\n",
    "        m = timm.create_model(model_name=arch, pretrained = pre, num_classes = 0)\n",
    "        self.enc = nn.Sequential(*list(m.children())[:-1])\n",
    "        self.enc_out_feat = enc_out_feat\n",
    "        self.head = nn.Sequential(AttentionSoftMax(enc_out_feat), nn.Dropout(0.5), nn.Linear(enc_out_feat,n))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        bs, instance_num, c, w, h = x.shape \n",
    "        x = x.view(bs*instance_num,c,w,h)\n",
    "\n",
    "        # print(x.shape)\n",
    "        #x: bs*instance_num x cx wxh\n",
    "\n",
    "        x = self.enc(x)\n",
    "        #x: bs instance_num x enc_out_feat\n",
    "\n",
    "        x = x.view(bs,instance_num, self.enc_out_feat).contiguous()\n",
    "\n",
    "        #x: bs x instance_num x enc_out_feat\n",
    "        x = self.head(x)\n",
    "        #x: bs x n\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac19211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:51.447888Z",
     "iopub.status.busy": "2022-09-27T10:01:51.447606Z",
     "iopub.status.idle": "2022-09-27T10:01:51.514595Z",
     "shell.execute_reply": "2022-09-27T10:01:51.513591Z"
    },
    "papermill": {
     "duration": 0.077958,
     "end_time": "2022-09-27T10:01:51.517089",
     "exception": false,
     "start_time": "2022-09-27T10:01:51.439131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tile_sz=384\n",
    "sz = 384\n",
    "max_size = 10000\n",
    "N=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33419f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:51.534579Z",
     "iopub.status.busy": "2022-09-27T10:01:51.534160Z",
     "iopub.status.idle": "2022-09-27T10:01:54.896290Z",
     "shell.execute_reply": "2022-09-27T10:01:54.895190Z"
    },
    "papermill": {
     "duration": 3.374091,
     "end_time": "2022-09-27T10:01:54.899367",
     "exception": false,
     "start_time": "2022-09-27T10:01:51.525276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold5.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold3.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold8.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold7.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold4.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold1.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold0.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold6.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold2.pth',\n",
       " '../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/concat-tile-pooling-384-effv2-fold9.pth']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_class = 2\n",
    "# backbone = timm.create_model(model_name, pretrained=False, num_classes=0).to(device)\n",
    "# model = nn.Sequential(\n",
    "#     backbone,\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(backbone.num_features, n_class)\n",
    "# ).to(device)\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "# model_path = \"../input/mayoinfermodelbfjctp/20220821_models/kaggle/working/models/*.pth\"\n",
    "model_path = \"../input/mayo-infer-model-bfj/models-attention/kaggle/working/models/*.pth\"\n",
    "model_paths = glob.glob(model_path)\n",
    "\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa326c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:54.925348Z",
     "iopub.status.busy": "2022-09-27T10:01:54.924826Z",
     "iopub.status.idle": "2022-09-27T10:01:54.936396Z",
     "shell.execute_reply": "2022-09-27T10:01:54.935345Z"
    },
    "papermill": {
     "duration": 0.025132,
     "end_time": "2022-09-27T10:01:54.938750",
     "exception": false,
     "start_time": "2022-09-27T10:01:54.913618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cutting_point(image, width_0, height_0, box_size=20000):\n",
    "    height, width, c = image.shape\n",
    "    multiplier_h = height/height_0\n",
    "    multiplier_w = width/width_0\n",
    "\n",
    "    box_h = int(box_size*multiplier_h)\n",
    "    box_w = int(box_size*multiplier_w)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(height//int(box_h)+1):\n",
    "        for j in range(width//int(box_w)+1):\n",
    "            start_y = i*box_h \n",
    "            end_y = (i+1)*box_h\n",
    "            start_x = j*box_w\n",
    "            end_x = (j+1)*box_w\n",
    "\n",
    "            if i == height//box_h:\n",
    "                start_y = height - box_h\n",
    "                end_y = height\n",
    "            if j == width//box_w:\n",
    "                start_x = width - box_w\n",
    "                end_x = width\n",
    "\n",
    "            if start_x < 0:\n",
    "                start_x = 0\n",
    "            if start_y < 0:\n",
    "                start_y = 0\n",
    "\n",
    "            scores.append((start_x, start_y, len(cv2.imencode(\".jpg\", image[start_y:end_y, start_x:end_x])[1])))\n",
    "\n",
    "    x_pos , y_pos, _ = pd.DataFrame(scores, columns=[\"start_x\", \"start_y\", \"score\"]).sort_values('score', ascending=False).iloc[0, :]\n",
    "    \n",
    "    return int((x_pos/width)*width_0), int((y_pos//height)*height_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a8d41f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:54.957814Z",
     "iopub.status.busy": "2022-09-27T10:01:54.957506Z",
     "iopub.status.idle": "2022-09-27T10:01:54.977825Z",
     "shell.execute_reply": "2022-09-27T10:01:54.976767Z"
    },
    "papermill": {
     "duration": 0.032428,
     "end_time": "2022-09-27T10:01:54.980112",
     "exception": false,
     "start_time": "2022-09-27T10:01:54.947684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tile(img, sz=128, N=16):\n",
    "    shape = img.shape\n",
    "    pad0,pad1 = (sz - shape[0]%sz)%sz, (sz - shape[1]%sz)%sz\n",
    "    img = np.pad(img,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],constant_values=255)\n",
    "    img = img.reshape(img.shape[0]//sz,sz,img.shape[1]//sz,sz,3)\n",
    "    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n",
    "    if len(img) < N:\n",
    "        img = np.pad(img,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n",
    "    scores = []\n",
    "    for im in img:\n",
    "        scores.append(len(cv2.imencode(\".jpg\", im)[1]))\n",
    "#     idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:N]\n",
    "#     img = img[idxs]\n",
    "    scores, img = zip(*sorted(zip(scores, img), reverse=True, key=lambda x: x[0]))\n",
    "#     high_info_ind = pd.Series(scores).where(pd.Series(scores) > 30000).idxmin()\n",
    "#     print(high_info_ind is not np.nan)\n",
    "    \n",
    "    bg_ind = pd.Series(scores).where(pd.Series(scores) > 10000).idxmin()\n",
    "    \n",
    "    try:\n",
    "        bg_cand = img[bg_ind:]\n",
    "        for bgi, bg in enumerate(bg_cand):\n",
    "            bg = bg.reshape(bg.shape[0] * bg.shape[1], bg.shape[2])\n",
    "            white, _ = scipy.stats.mode(bg, axis=0)\n",
    "            diff_to_white = (255,255,255) - white\n",
    "            if sum(diff_to_white[0]) < 128*3:\n",
    "                break\n",
    "            else:\n",
    "                diff_to_white = [[0,0,0]]\n",
    "    except TypeError:\n",
    "        print(\"cannot find bg\")\n",
    "        diff_to_white = [[0,0,0]]\n",
    "        bg_cand = [np.zeros((sz,sz,3))]\n",
    "        bgi = 0\n",
    "\n",
    "    return img[:N], bg_cand[bgi], diff_to_white\n",
    "\n",
    "def vips2numpy(vi):\n",
    "    format_to_dtype = {\n",
    "       'uchar': np.uint8,\n",
    "       'char': np.int8,\n",
    "       'ushort': np.uint16,\n",
    "       'short': np.int16,\n",
    "       'uint': np.uint32,\n",
    "       'int': np.int32,\n",
    "       'float': np.float32,\n",
    "       'double': np.float64,\n",
    "       'complex': np.complex64,\n",
    "       'dpcomplex': np.complex128,\n",
    "    }\n",
    "    return np.ndarray(buffer=vi.write_to_memory(),dtype=format_to_dtype[vi.format],shape=[vi.height, vi.width, vi.bands])\n",
    "\n",
    "def return_tiled_images(image_path, transform, N=16, max_size=20000, crop_size=384):\n",
    "    image = pyvips.Image.thumbnail(image_path, max_size)\n",
    "    image = vips2numpy(image)\n",
    "    width, height, c = image.shape\n",
    "    print(f\"Input width: {width} height: {height}\")\n",
    "    images, bg, diff_to_white = tile(image, sz=crop_size, N=N)\n",
    "    output_images = []\n",
    "    for idx, img in enumerate(images):\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "#         img = cv2.imencode(\".jpg\", img, [cv2.IMWRITE_JPEG_QUALITY, 100])[1]\n",
    "#         img = cv2.imdecode(img, flags=cv2.IMREAD_COLOR)\n",
    "        img = img + diff_to_white\n",
    "        img = img / img.max()\n",
    "        img = np.clip(img * 255, a_min = 0, a_max = 255).astype(np.uint8)\n",
    "        img = transform(img)\n",
    "        \n",
    "        output_images.append(img)\n",
    "    \n",
    "    img_indexes = random.sample(list(range(0, N)), N)\n",
    "    \n",
    "    batched_images = []\n",
    "    one_batch = []\n",
    "    for i, index in enumerate(img_indexes):\n",
    "        one_batch.append(output_images[index])\n",
    "        if i%N==(N-1):\n",
    "            batched_images.append(torch.stack(one_batch, dim=0))\n",
    "            one_batch = []\n",
    "\n",
    "    batched_images = torch.stack(batched_images, dim=0)\n",
    "#     del img, image, images, output_images; gc.collect()\n",
    "    return batched_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd0f382e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:01:55.000045Z",
     "iopub.status.busy": "2022-09-27T10:01:54.999205Z",
     "iopub.status.idle": "2022-09-27T10:04:42.973213Z",
     "shell.execute_reply": "2022-09-27T10:04:42.972249Z"
    },
    "papermill": {
     "duration": 167.986712,
     "end_time": "2022-09-27T10:04:42.975986",
     "exception": false,
     "start_time": "2022-09-27T10:01:54.989274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input width: 280 height: 500\n",
      "org width: 34007 height: 60797\n",
      "../input/mayo-clinic-strip-ai/test/006388_0.tif 19918 0\n",
      "Input width: 123 height: 500\n",
      "org width: 15255 height: 61801\n",
      "../input/mayo-clinic-strip-ai/test/00c058_0.tif 0 0\n",
      "Input width: 100 height: 500\n",
      "org width: 5946 height: 29694\n",
      "../input/mayo-clinic-strip-ai/test/008e5c_0.tif 0 0\n",
      "Input width: 500 height: 238\n",
      "org width: 55831 height: 26553\n",
      "../input/mayo-clinic-strip-ai/test/01adc5_0.tif 0 0\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((sz, sz)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "preds = []\n",
    "\n",
    "for path in test_image_paths:\n",
    "    thumb = pyvips.Image.thumbnail(path, 500)\n",
    "    thumb = vips2numpy(thumb)\n",
    "    slide = OpenSlide(path)\n",
    "    height, width, c = thumb.shape\n",
    "    width_0, height_0 = slide.dimensions\n",
    "\n",
    "    print(f\"Input width: {width} height: {height}\")\n",
    "    print(f\"org width: {width_0} height: {height_0}\")\n",
    "\n",
    "    x_pos, y_pos = get_cutting_point(thumb, width_0, height_0, max_size)\n",
    "    print(path, x_pos, y_pos)\n",
    "    cropped = slide.read_region((x_pos, y_pos), 0, (max_size, max_size))\n",
    "    images, bg, diff_to_white = tile(img=cv2.cvtColor(np.array(cropped), cv2.COLOR_RGB2BGR), sz=tile_sz, N=N)\n",
    "    \n",
    "    batched_image = []\n",
    "    for image in images:\n",
    "         batched_image.append(transform(image))\n",
    "    \n",
    "    batched_image = torch.stack(batched_image, dim=0).to(device)\n",
    "    \n",
    "    del images, image\n",
    "    gc.collect()\n",
    "    \n",
    "    for i, m_path in enumerate(model_paths):\n",
    "        model.load_state_dict(\n",
    "        torch.load(\n",
    "            m_path, map_location=device\n",
    "            )\n",
    "        )\n",
    "        model.train(False)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(torch.unsqueeze(batched_image, 0))\n",
    "#             pred = model(images)\n",
    "\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        pred = sm(pred).to('cpu').detach().numpy().copy()\n",
    "        \n",
    "        pred_ce = (pred[:, 0]).mean()\n",
    "        pred_laa = (pred[:, 1]).mean()\n",
    "        \n",
    "        preds.append((path, i, pred_ce, pred_laa))\n",
    "        \n",
    "    del batched_image\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bdc396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:04:42.995445Z",
     "iopub.status.busy": "2022-09-27T10:04:42.994015Z",
     "iopub.status.idle": "2022-09-27T10:04:42.999210Z",
     "shell.execute_reply": "2022-09-27T10:04:42.998283Z"
    },
    "papermill": {
     "duration": 0.01635,
     "end_time": "2022-09-27T10:04:43.001137",
     "exception": false,
     "start_time": "2022-09-27T10:04:42.984787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if len(too_big_for_process)>0:\n",
    "#     for i in too_big_for_process:\n",
    "#         preds.append((i, 0, 0.82, 0.28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fabf517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:04:43.019348Z",
     "iopub.status.busy": "2022-09-27T10:04:43.017885Z",
     "iopub.status.idle": "2022-09-27T10:04:43.040260Z",
     "shell.execute_reply": "2022-09-27T10:04:43.039427Z"
    },
    "papermill": {
     "duration": 0.03318,
     "end_time": "2022-09-27T10:04:43.042263",
     "exception": false,
     "start_time": "2022-09-27T10:04:43.009083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_patient_id(path):\n",
    "    return os.path.basename(path).split(\"_\")[0]\n",
    "\n",
    "df = pd.DataFrame(preds, columns=(\"path\", \"fold\", \"CE\", \"LAA\"))\n",
    "df[\"patient_id\"] = df[\"path\"].map(path_to_patient_id)\n",
    "df.groupby(\"patient_id\").mean().drop(\"fold\", axis=1).to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "428ebd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T10:04:43.059844Z",
     "iopub.status.busy": "2022-09-27T10:04:43.059573Z",
     "iopub.status.idle": "2022-09-27T10:04:43.075325Z",
     "shell.execute_reply": "2022-09-27T10:04:43.074497Z"
    },
    "papermill": {
     "duration": 0.027002,
     "end_time": "2022-09-27T10:04:43.077332",
     "exception": false,
     "start_time": "2022-09-27T10:04:43.050330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CE</th>\n",
       "      <th>LAA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>006388</th>\n",
       "      <td>0.727539</td>\n",
       "      <td>0.272461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008e5c</th>\n",
       "      <td>0.731445</td>\n",
       "      <td>0.268799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00c058</th>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.259521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01adc5</th>\n",
       "      <td>0.732910</td>\n",
       "      <td>0.266846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CE       LAA\n",
       "patient_id                    \n",
       "006388      0.727539  0.272461\n",
       "008e5c      0.731445  0.268799\n",
       "00c058      0.740234  0.259521\n",
       "01adc5      0.732910  0.266846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"patient_id\").mean().drop(\"fold\", axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 222.404187,
   "end_time": "2022-09-27T10:04:45.653341",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-27T10:01:03.249154",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
