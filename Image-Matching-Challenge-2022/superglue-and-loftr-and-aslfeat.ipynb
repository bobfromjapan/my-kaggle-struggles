{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"merge 3 matcher(SuperGlue, LoFTR, ASLFeat) and some of original idea.\n\n3 matcher referred from:\n\nSuperGlue: https://www.kaggle.com/code/losveria/superglue-baseline\nASLFeat: https://www.kaggle.com/code/rsmits/tensorflow-aslfeat-inference\nLoFTR: https://www.kaggle.com/code/mcwema/imc-2022-kornia-loftr-score-plateau-0-726\n\nThanks for these notebooks!","metadata":{}},{"cell_type":"code","source":"%%capture\n#dry_run = False\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:50:54.834862Z","iopub.execute_input":"2022-05-30T12:50:54.83555Z","iopub.status.idle":"2022-05-30T12:51:52.468411Z","shell.execute_reply.started":"2022-05-30T12:50:54.835455Z","shell.execute_reply":"2022-05-30T12:51:52.46746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Modules\nimport numpy as np \nimport pandas as pd\nimport csv\nimport cv2\nimport gc\nimport tensorflow as tf\nimport sys\nimport yaml\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# Import ASLFeat\nsys.path.append('../input/aslfeat')\nfrom models import get_model\n\n# Disable Eager Execution\ntf.compat.v1.disable_eager_execution()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:51:52.471296Z","iopub.execute_input":"2022-05-30T12:51:52.471499Z","iopub.status.idle":"2022-05-30T12:51:57.461503Z","shell.execute_reply.started":"2022-05-30T12:51:52.471474Z","shell.execute_reply":"2022-05-30T12:51:57.46075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:51:57.462763Z","iopub.execute_input":"2022-05-30T12:51:57.463027Z","iopub.status.idle":"2022-05-30T12:51:57.471363Z","shell.execute_reply.started":"2022-05-30T12:51:57.462991Z","shell.execute_reply":"2022-05-30T12:51:57.470537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Config\nwith open('../input/aslfeat/configs/matching_eval2.yaml', 'r') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n# There are 2 checkpoints in the pretrained folder. This one should be the best...\naslfeat_model_path = '../input/aslfeat/pretrained/aslfeatv2/model.ckpt-60000' \nconfig['model_path'] = aslfeat_model_path\nconfig['net']['config']['kpt_n'] = 8000 # Sames as original config ... just for convenience ;-)\n\n# Summary config\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:51:57.473775Z","iopub.execute_input":"2022-05-30T12:51:57.474056Z","iopub.status.idle":"2022-05-30T12:51:57.491597Z","shell.execute_reply.started":"2022-05-30T12:51:57.474018Z","shell.execute_reply":"2022-05-30T12:51:57.490854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Model\nmodel = get_model('feat_model')(aslfeat_model_path, **config['net'])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:51:57.492743Z","iopub.execute_input":"2022-05-30T12:51:57.493152Z","iopub.status.idle":"2022-05-30T12:52:03.25469Z","shell.execute_reply.started":"2022-05-30T12:51:57.493118Z","shell.execute_reply":"2022-05-30T12:52:03.253965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ASLFeat Functions    \ndef load_imgs(img_paths):\n    rgb_list = []\n    gray_list = []\n    \n    for img_path in img_paths:\n        img = cv2.imread(img_path)\n        scale = 840 / max(img.shape[0], img.shape[1]) \n        w = int(img.shape[1] * scale)\n        h = int(img.shape[0] * scale)\n        img = cv2.resize(img, (w, h))\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[..., np.newaxis]\n        img = img[..., ::-1]\n        rgb_list.append(img)\n        gray_list.append(gray)\n        \n    return rgb_list, gray_list\n\ndef extract_local_features(gray_list):    \n    descs = []\n    kpts = []\n    \n    for gray_img in gray_list:\n        desc, kpt = [], []\n        desc, kpt, _ = model.run_test_data(gray_img)\n        descs.append(desc)\n        kpts.append(kpt)\n        \n    return descs, kpts\n\nclass MatcherWrapper(object):\n    \"\"\"OpenCV matcher wrapper.\"\"\"\n\n    def __init__(self):\n        # Swapped BFMatcher to FlannBasedMatcher\n        # FLANN parameters\n        FLANN_INDEX_KDTREE = 0\n        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 6)\n        search_params = dict(checks = 125)   # or pass empty dictionary\n        self.matcher = cv2.FlannBasedMatcher(index_params, search_params)\n    \n    def get_matches(self, feat1, feat2, cv_kpts1, cv_kpts2, ratio = 0.8, cross_check = True, err_thld = 0.5):\n        \"\"\"Compute putative and inlier matches.\n        Args:\n            feat: (n_kpts, 128) Local features.\n            cv_kpts: A list of keypoints represented as cv2.KeyPoint.\n            ratio: The threshold to apply ratio test.\n            cross_check: (True by default) Whether to apply cross check.\n            err_thld: Epipolar error threshold.\n        Returns:\n            good_matches: Putative matches.\n            mask: The mask to distinguish inliers/outliers on putative matches.\n        \"\"\"\n        \n        init_matches1 = self.matcher.knnMatch(feat1, feat2, k = 2)\n        init_matches2 = self.matcher.knnMatch(feat2, feat1, k = 2)\n\n        good_matches = []\n\n        for i in range(len(init_matches1)):\n            cond = True\n            if cross_check:\n                cond1 = cross_check and init_matches2[init_matches1[i][0].trainIdx][0].trainIdx == i\n                cond *= cond1\n            if ratio is not None and ratio < 1:\n                cond2 = init_matches1[i][0].distance <= ratio * init_matches1[i][1].distance\n                cond *= cond2\n            if cond:\n                good_matches.append(init_matches1[i][0])\n\n        if type(cv_kpts1) is list and type(cv_kpts2) is list:\n            good_kpts1 = np.array([cv_kpts1[m.queryIdx].pt for m in good_matches])\n            good_kpts2 = np.array([cv_kpts2[m.trainIdx].pt for m in good_matches])\n        elif type(cv_kpts1) is np.ndarray and type(cv_kpts2) is np.ndarray:\n            good_kpts1 = np.array([cv_kpts1[m.queryIdx] for m in good_matches])\n            good_kpts2 = np.array([cv_kpts2[m.trainIdx] for m in good_matches])\n        else:\n            good_kpts1 = np.empty(0)\n            good_kpts2 = np.empty(0)\n            \n        # Calculate Fundamental Mask and inliers\n        F, mask = get_fundamental_matrix(good_kpts1, good_kpts2, err_thld)\n        return F, (good_kpts1, good_kpts2), mask\n            \n    def draw_matches(self, img1, cv_kpts1, img2, cv_kpts2, good_matches, mask, match_color = (0, 255, 0), pt_color = (0, 0, 255)):\n        \"\"\"Draw matches.\"\"\"\n        if type(cv_kpts1) is np.ndarray and type(cv_kpts2) is np.ndarray:\n            cv_kpts1 = [cv2.KeyPoint(cv_kpts1[i][0], cv_kpts1[i][1], 1) for i in range(cv_kpts1.shape[0])]\n            cv_kpts2 = [cv2.KeyPoint(cv_kpts2[i][0], cv_kpts2[i][1], 1) for i in range(cv_kpts2.shape[0])]\n            \n        display = cv2.drawMatches(img1, cv_kpts1, img2, cv_kpts2, good_matches,\n                                  None,\n                                  matchColor = match_color,\n                                  singlePointColor = pt_color,\n                                  matchesMask = mask.ravel().tolist(), flags=4)\n        return display","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:52:03.256077Z","iopub.execute_input":"2022-05-30T12:52:03.256456Z","iopub.status.idle":"2022-05-30T12:52:03.288892Z","shell.execute_reply.started":"2022-05-30T12:52:03.25642Z","shell.execute_reply":"2022-05-30T12:52:03.287985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def FlattenMatrix(M, num_digits = 8):\n    '''Convenience function to write CSV files.'''    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\ndef get_fundamental_matrix(kpts1, kpts2, err_thld):    \n    if len(kpts1) > 7:\n        F, inliers = cv2.findFundamentalMat(kpts1, \n                                            kpts2, \n                                            cv2.USAC_MAGSAC, \n                                            ransacReprojThreshold = err_thld, \n                                            confidence = 0.99999, \n                                            maxIters = 100000) # Lower maxIters to increase speed / lower accuracy\n        return F, inliers\n    else:\n        return np.random.rand(3, 3), None","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:52:03.29053Z","iopub.execute_input":"2022-05-30T12:52:03.291273Z","iopub.status.idle":"2022-05-30T12:52:03.30161Z","shell.execute_reply.started":"2022-05-30T12:52:03.291232Z","shell.execute_reply":"2022-05-30T12:52:03.300125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_aslfeat_fmatrix(batch_id, img_id1, img_id2, plot = False):\n    image_fpath_1 = f'{src}/test_images/{batch_id}/{img_id1}.png'\n    image_fpath_2 = f'{src}/test_images/{batch_id}/{img_id2}.png'\n    \n    # Load Test Image Pair\n    rgb_list, gray_list = load_imgs([image_fpath_1, image_fpath_2])    \n\n    # Extract Local Features\n    descs, kpts = extract_local_features(gray_list)\n        \n    # feature matching and draw matches.\n    matcher = MatcherWrapper()\n    fundamental_matrix, match, mask = matcher.get_matches(descs[0], descs[1], kpts[0], kpts[1],\n                                                          ratio = None, \n                                                          cross_check = True, # I'am only using the Cross Check...not the ratio test.\n                                                          err_thld = 0.2)\n\n    return match","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:52:03.304734Z","iopub.execute_input":"2022-05-30T12:52:03.306426Z","iopub.status.idle":"2022-05-30T12:52:03.314611Z","shell.execute_reply.started":"2022-05-30T12:52:03.306386Z","shell.execute_reply":"2022-05-30T12:52:03.313891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_dict_asl = {}\nfor i, row in tqdm(enumerate(test_samples)):\n    sample_id, batch_id, img_id1, img_id2 = row\n\n    # Set Plot\n    plot = False\n        \n    # Get Fundamental matrix with ASLFeat And FLANNBasedMatcher\n    match_dict_asl[sample_id] = get_aslfeat_fmatrix(batch_id, img_id1, img_id2)\n        \n    # Mem Cleanup\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:52:55.636981Z","iopub.execute_input":"2022-05-30T12:52:55.637671Z","iopub.status.idle":"2022-05-30T12:53:01.728533Z","shell.execute_reply.started":"2022-05-30T12:52:55.637621Z","shell.execute_reply":"2022-05-30T12:53:01.727839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del get_model\nsys.modules.pop('models')\nsys.path.remove('../input/aslfeat')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:01.73026Z","iopub.execute_input":"2022-05-30T12:53:01.730906Z","iopub.status.idle":"2022-05-30T12:53:01.735215Z","shell.execute_reply.started":"2022-05-30T12:53:01.730867Z","shell.execute_reply":"2022-05-30T12:53:01.734442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport random\nfrom glob import glob\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport torch\n\nimport sys\nsys.path.append(\"../input/super-glue-pretrained-network\")\nfrom models.matching import Matching\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:01.736667Z","iopub.execute_input":"2022-05-30T12:53:01.736923Z","iopub.status.idle":"2022-05-30T12:53:03.211237Z","shell.execute_reply.started":"2022-05-30T12:53:01.736889Z","shell.execute_reply":"2022-05-30T12:53:03.210455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\n\napply_eq = False\n\ndef CLAHE_Convert(origin_input):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    t = np.asarray(origin_input)\n    t = cv2.cvtColor(t, cv2.COLOR_BGR2HSV)\n    t[:,:,-1] = clahe.apply(t[:,:,-1])\n    t = cv2.cvtColor(t, cv2.COLOR_HSV2BGR)\n    # t = Img.fromarray(t)\n    return t\n\ndef CLAHE_Convert2(origin_input):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    img = origin_input.astype(np.uint8)\n    img = clahe.apply(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:03.213417Z","iopub.execute_input":"2022-05-30T12:53:03.213672Z","iopub.status.idle":"2022-05-30T12:53:03.466292Z","shell.execute_reply.started":"2022-05-30T12:53:03.213638Z","shell.execute_reply":"2022-05-30T12:53:03.46559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\nloftr_matcher = KF.LoFTR(pretrained=None)\nloftr_matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nloftr_matcher = loftr_matcher.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:03.46757Z","iopub.execute_input":"2022-05-30T12:53:03.467907Z","iopub.status.idle":"2022-05-30T12:53:07.689621Z","shell.execute_reply.started":"2022-05-30T12:53:03.467868Z","shell.execute_reply":"2022-05-30T12:53:07.688874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_torch_image(fname, device):\n    img = cv2.imread(fname)\n\n    scale = 840 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    \n    if apply_eq:\n        img = CLAHE_Convert(img)\n    \n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:07.690823Z","iopub.execute_input":"2022-05-30T12:53:07.691108Z","iopub.status.idle":"2022-05-30T12:53:07.697427Z","shell.execute_reply.started":"2022-05-30T12:53:07.691073Z","shell.execute_reply":"2022-05-30T12:53:07.696771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\ntest_samples_df = pd.DataFrame(test_samples, columns=[\"sample_id\", \"batch_id\", \"image_1_id\", \"image_2_id\"])\ntest_samples_df","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:07.698748Z","iopub.execute_input":"2022-05-30T12:53:07.699251Z","iopub.status.idle":"2022-05-30T12:53:07.728252Z","shell.execute_reply.started":"2022-05-30T12:53:07.699212Z","shell.execute_reply":"2022-05-30T12:53:07.727599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize = [840, ]\nresize_float = True\n\nconfig = {\n    \"superpoint\": {\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 1024\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 20,\n        \"match_threshold\": 0.2,\n    }\n}\nsg_matcher = Matching(config).eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:07.72958Z","iopub.execute_input":"2022-05-30T12:53:07.730054Z","iopub.status.idle":"2022-05-30T12:53:08.699186Z","shell.execute_reply.started":"2022-05-30T12:53:07.730017Z","shell.execute_reply":"2022-05-30T12:53:08.697744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_dict_sg = {}\nmatch_dict_loftr = {}\n\nfor i, row in tqdm(enumerate(test_samples)):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    \n    # SG match\n    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n    \n    image_1, inp_1, scales_1 = read_image(image_fpath_1, device, resize, 0, resize_float)\n    image_2, inp_2, scales_2 = read_image(image_fpath_2, device, resize, 0, resize_float)\n    \n    if apply_eq:\n        image_1 = CLAHE_Convert2(image_1)\n        image_2 = CLAHE_Convert2(image_2)\n    \n    sg_pred = sg_matcher({\"image0\": inp_1, \"image1\": inp_2})\n    sg_pred = {k: v[0].detach().cpu().numpy() for k, v in sg_pred.items()}\n    sg_kpts1, sg_kpts2 = sg_pred[\"keypoints0\"], sg_pred[\"keypoints1\"]\n    sg_matches, conf = sg_pred[\"matches0\"], sg_pred[\"matching_scores0\"]\n\n    valid = sg_matches > -1\n    sg_mkpts1 = sg_kpts1[valid]\n    sg_mkpts2 = sg_kpts2[sg_matches[valid]]\n    mconf = conf[valid]\n    \n    match_dict_sg[sample_id] = (sg_mkpts1, sg_mkpts2)\n    \n    #LoFTR match\n    image_1 = load_torch_image(f'{src}/test_images/{batch_id}/{image_1_id}.png', device)\n    image_2 = load_torch_image(f'{src}/test_images/{batch_id}/{image_2_id}.png', device)\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1), \n              \"image1\": K.color.rgb_to_grayscale(image_2)}\n\n    with torch.no_grad():\n        correspondences = loftr_matcher(input_dict)\n        \n    loftr_mkpts1 = correspondences['keypoints0'].cpu().numpy()\n    loftr_mkpts2 = correspondences['keypoints1'].cpu().numpy()\n    \n    match_dict_loftr[sample_id] = (loftr_mkpts1, loftr_mkpts2)\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:08.700674Z","iopub.execute_input":"2022-05-30T12:53:08.700927Z","iopub.status.idle":"2022-05-30T12:53:09.776559Z","shell.execute_reply.started":"2022-05-30T12:53:08.700893Z","shell.execute_reply":"2022-05-30T12:53:09.775849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_dict = {}\nfor i, row in tqdm(enumerate(test_samples)):\n    sample_id, batch_id, image_1_id, image_2_id = row\n\n    mkpts1 = np.concatenate([match_dict_sg[sample_id][0], match_dict_loftr[sample_id][0], match_dict_asl[sample_id][0]])\n    mkpts2 = np.concatenate([match_dict_sg[sample_id][1], match_dict_loftr[sample_id][1], match_dict_asl[sample_id][1]])\n    \n    print(len(mkpts1), len(mkpts2))\n\n    if len(mkpts1) > 8:\n#         F, inlier_mask = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.99999, maxIters=10000)\n        F, inlier_mask = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        \n    gc.collect()\n        \n    if (i < 3):\n#         print(\"Running time: \", nd - st, \" s\")\n        print(loftr_mkpts1.shape)\n        print(mkpts1.shape)\n        print(sg_mkpts1.shape)\n\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts2).view(1,-1, 2),\n                                    torch.ones(mkpts2.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts2.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts1.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inlier_mask,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:09.778741Z","iopub.execute_input":"2022-05-30T12:53:09.779777Z","iopub.status.idle":"2022-05-30T12:53:16.483391Z","shell.execute_reply.started":"2022-05-30T12:53:09.779745Z","shell.execute_reply":"2022-05-30T12:53:16.482583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T12:53:16.484771Z","iopub.execute_input":"2022-05-30T12:53:16.485062Z","iopub.status.idle":"2022-05-30T12:53:16.491447Z","shell.execute_reply.started":"2022-05-30T12:53:16.485024Z","shell.execute_reply":"2022-05-30T12:53:16.490686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}